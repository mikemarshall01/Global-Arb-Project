# -*- coding: utf-8 -*-
# *** Spyder Python Console History Log ***

## ---(Thu Nov 30 09:20:44 2017)---
import pandas as pd
import numpy as np
df_clipper= pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
df_reuters = pd.read_excel('L:\TRADING\ANALYSIS\FIXTURES\REUTERS.xlsx', sheetname=1, header=0)
df_clipper=df_clipper['Vessel IMO'].unique()
df_clipper=df_clipper['IMO'].unique()
df_clipper=df_clipper['imo'].unique()
df_clipper= pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
df_clipper=df_clipper[df_clipper['Vessel IMO'].unique()]
df_clipper=df_clipper[df_clipper['imo'].unique()]
df_clipper['imo'].unique()
dary=df_clipper['imo'].unique()
dfFiltered=df_clipper[dary]
%clear
ref_test_data = pd.read_excel('M:\test_datra_for_ref_resample.xlsx')
ref_test_data.head()
ref_test_data = pd.read_excel('M:\test_datra_for_ref_resample.xlsx', sheetname=0, header=0)
ref_test_data.head()
ref_test_data = pd.read_excel('M:\test_data_for_ref_resample.xlsx', sheetname=0, header=0)
ref_test_data.head()

## ---(Thu Nov 30 10:43:41 2017)---
import pandas as pd
import numpy as np

ref_data = pd.read_excel('M:\test_data_for_ref_resample.xlsx')
import pandas as pd
import numpy as np

ref_data = pd.read_excel('M:\test_data_for_ref_resample.xlsx')
runfile('C:/Users/mima/.spyder-py3/temp.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np

ref_data = pd.read_excel('M://test_data_for_ref_resample.xlsx', sheetname=0, header=0)
ref_dat
num_days = ref_data.iloc[0, 5]
num_days = ref_data.iloc[0, 5:7]
num_days = ref_data.iloc[0, 4:6]
num_days_diff = num_days[1] - num_days[0]
num_days_diff
start_date = ref_data.iloc[0,5]
end_date = ref_data.iloc[0,6]
start_date = ref_data.iloc[0,4]
end_date = ref_data.iloc[0,5]
cap_offline = ref_data.iloc[0,6]

num_days_offline = end_date - start_date
runfile('C:/Users/mima/.spyder-py3/temp.py', wdir='C:/Users/mima/.spyder-py3')
ref_data['Diff']=ref_data['end_date']-ref_data['start_date']
ref_data['Diff']=ref_data['END_DATE']-ref_data['Start_Date']
type(ref_data['Diff'])
days_list=[x for x range(start_Date,end_Date)]
days_list=[x for x in range(start_Date,end_Date)]
days_list=[x for x in range(start_date,end_date)]
days_list=[x for x in range(num_days_offline.days +1)]
days_list=[start_date + timedelta(days=i) for x in range(num_days_offline.days +1)]
days_list=[start_date + timedelta(days=x) for x in range(num_days_offline.days +1)]
pd_index_start_date = datetime(2005, 1, 1)
pd_index_start_date = datetime(2020, 12, 1)

days_list=[start_date + timedelta(days=x) for x in range(num_days_offline.days +1)]
pd_index_end_date = datetime(2020, 12, 1)
pd_index_start_date = datetime(2005, 1, 1)
total_time_period = pd_index_end_date - pd_index_start_date
days_list=[pd_index_start_date + timedelta(days=x) for x in range(total_time_period.days +1)]
pd_index_days_list=[pd_index_start_date + timedelta(days=x) for x in range(total_time_period.days +1)]
maintenance_framework = pd.DataFrame(0, index= pd_index_days_list)
maintenance_framework = pd.DataFrame(data=None, index= pd_index_days_list)
test_dataframe = pd.DataFrame(data=250, index= pd_index_days_list)
test_dataframe = pd.DataFrame(data=int(250), index= pd_index_days_list)
test_dataframe = pd.DataFrame(data=None, index= pd_index_days_list)
test_dataframe['REF'] = 250
num_days_offline = end_date - start_date
### then create time series index for duration period
ref_test_index=[start_date + timedelta(days=x) 
        for x in range(num_days_offline.days +1)]

### then create dataframe for this index
ref_test_dataframe  = pd.DataFrame(data=None, index=ref_test_index)
ref_test_dataframe['Refinery'] = ref_data.loc[0,'CAP_OFFLINE']

import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)


dtrange = pd.date_range(begin, end)
p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)
d = df.index.day - np.clip((df.index.day-1) // 10, 0, 2)*10 - 1
date = df.index.values - np.array(d, dtype="timedelta64[D]")
df.groupby(date).mean()
d = df.index.day
print(d)
print(d-1)
print((d-1) // 10)
print(((d-1) // 10), 0, 2)
print(np.clip((d-1) // 10), 0, 2)
d = df.index.day
print(d)
print(d-1)
print((d-1) // 10)
print(np.clip((d-1) // 10, 0, 2))
print(np.clip((d-1) // 10, 0, 2)*10)
print(np.clip((d-1) // 10, 0, 2)*10-1)
print(df.index.day - np.clip((d-1) // 10, 0, 2)*10-1)
print(df.index.day - np.clip((df.index.day-1) // 10, 0, 2)*10-1)
d = df.index.day - np.clip((df.index.day-1) // 10, 0, 2)*10 - 1
print(df.index.values)
print(np.array(d, dtype = "timedelta[D]"))
print(np.array(d, dtype = "timedelta64[D]"))
print(df.index.values - np.array(d, dtype = "timedelta64[D]"))
decades = df.index.values - np.array(d, dtype = "timedelta64[D]")
d = df.index.day - np.clip((df.index.day-1) // 10, 0, 2)*10 - 1
date = df.index.values - np.array(d, dtype="timedelta64[D]")
df.groupby(date).mean()
for index, row in ref_data.iterrows():
    print  row['COUNTRY']
for index, row in ref_data.iterrows():
    print row['COUNTRY']
for index, row in ref_data.iterrows():
    print(row['COUNTRY'])
OUTAGE_TYPE
for index, row in ref_data.iterrows():
    print(row['OUTAGE_TYPE'])
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    #num_days_offline = end_date - start_date
    ref_test_df = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df[str(row['REFINERY'])] = ref_test_df['CAP_OFFLINE']
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    #num_days_offline = end_date - start_date
    ref_test_df = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df[row['REFINERY']] = ref_test_df['CAP_OFFLINE']
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    #num_days_offline = end_date - start_date
    ref_test_df = pd.date_range(start=start_date, end=end_date, freq='D')
    name = row['REFINERY']
    ref_test_df[name] = ref_test_df['CAP_OFFLINE']
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    #num_days_offline = end_date - start_date
    ref_test_df = pd.date_range(start=start_date, end=end_date, freq='D')
    name = row['REFINERY']
    ref_test_df[name] = row['CAP_OFFLINE']
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework.add(ref_test_df, fill_value=0)
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
    maintenance_framework = maintenance_framework.sum(axis=1).to_frame()
    
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
   #maintenance_framework = maintenance_framework.sum(axis=1).to_frame()
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
    maintenance_framework = maintenance_framework.sum(axis=1)
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
    maintenance_framework = maintenance_framework.sum(axis=1).to_frame()
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
    maintenance_framework = maintenance_framework.sum(axis=1).to_frame(name)
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//NORTH AMERICA.xlsm",
                         sheetname=0, header=0, usecols = "A:M")
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
    maintenance_framework = maintenance_framework.sum(axis=1).to_frame()
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
    maintenance_framework = maintenance_framework.sum(axis=1).to_frame(name)
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)
    ref_test_df[name] = row['CAP_OFFLINE']
    maintenance_framework = pd.concat([ref_test_df, maintenance_framework], axis=1)
    #maintenance_framework = maintenance_framework.sum(axis=1).to_frame(name)
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
maintenance_framework = pd.DataFrame(data=None, index= pd_index_days_list)
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
maintenance_framework = pd.DataFrame(data=None, index= pd_index_days_list)
index=1
row=
#ref_data = ref_data[(ref_data['UNIT_NAME'] == 'CRUDE #1') & ((ref_data['REFINERY'] == 'ALLIANCE/BELLE CHASE')
ref_data['Start_Date'].apply(lambda x : x.datetime() in ref_data['Start_Date'])
ref_data['Start_Date'].apply(lambda x : x.date() in ref_data['Start_Date'])
ref_data = pd.read_excel("M://test_data_for_ref_resample.xlsx", sheetname=0, header=0,parse_dates=False)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

#ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//NORTH AMERICA.xlsm",
 #                        sheetname=0, header=0)

ref_data = pd.read_excel("M://test_data_for_ref_resample.xlsx", sheetname=0, header=0,parse_dates=False)
end_date = row['END_DATE']
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']

for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=None, index=date_range)

ref_list=ref_data['REFINERY'].unique()
ref_list=pd.Series(ref_data['REFINERY'].unique())
maintenance_framework = pd.DataFrame(data=None, index= pd_index_days_list, columns=ref_list)
pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

#ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//NORTH AMERICA.xlsm",
 #                        sheetname=0, header=0)

ref_data = pd.read_excel("M://test_data_for_ref_resample.xlsx", sheetname=0, header=0)
ref_data['Start_Date'].apply(lambda x : x.date() in ref_data['Start_Date'])
#ref_data = ref_data[(ref_data['UNIT_NAME'] == 'CRUDE #1') & ((ref_data['REFINERY'] == 'ALLIANCE/BELLE CHASE')
#                     | (ref_data['REFINERY'] == 'ARDMORE'))]

ref_data = ref_data[(ref_data['UNIT_NAME'] == 'CRUDE #1')]

### Set start and end dates for the entire dataframe
pd_index_start_date = datetime(2005, 1, 1)
pd_index_end_date = datetime(2020, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())
maintenance_framework = pd.DataFrame(data=None, index= pd_index_days_list, columns=ref_list)
maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(data=row['CAP_OFFLINE'], index=date_range, columns= row['REFINERY'])

ref_test_df = pd.DataFrame(0, index=date_range, columns= row['REFINERY'])
row
date_range
ref_test_df = pd.DataFrame(0, index=date_range, columns= row['REFINERY'])
ref_test_df = pd.DataFrame(0, index=date_range, columns= list(row['REFINERY']))
ref_test_df = pd.DataFrame(0, index=date_range, columns= [row['REFINERY']])
list(row['REFINERY'])
ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
maintenance_framework.loc[ref_test_df.index,row['REFINERY']]=ref_test_df[ref_test_df.index,row['REFINERY']]
maintenance_framework.loc[date_range,row['REFINERY']]=ref_test_df[date_range,row['REFINERY']]
maintenance_framework.loc[date_range,name]=ref_test_df[date_range,name]
maintenance_framework.loc[ref_test_df.index,name]=ref_test_df.loc[ref_test_df.index,name]
maintenance_framework.to_excel(test_path+'main.xlsx')
test_path='C:/Python Proj/'
maintenance_framework.to_excel(test_path+'main.xlsx')
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=ref_test_df.loc[ref_test_df.index,name]

maintenance_framework.to_excel(test_path+'main.xlsx')
for index, row in ref_data.iterrows():
    end_date = row['END_DATE']
    start_date = row['Start_Date']
    name = row['REFINERY']
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]

maintenance_framework.to_excel(test_path+'main.xlsx')
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
maintenance_framework.to_excel(test_path+'main.xlsx')
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
print(d)
print(date)
maintenance_framework.groupby(date).mean()  
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
print(maintenance_framework_week)
print(maintenance_framework_weekly)
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
end_date = [x for x in row['END_DATE'] if row['END_DATE'] > date(2005,1,1)]
runfile('C:/Users/mima/.spyder-py3/refinery_database_tester.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/refinery_database_tworking.py', wdir='C:/Users/mima/.spyder-py3')
file = pd.read_excel('L:/TRADING/ANALYSIS/PRICES/CXL Prices.xlsx', sheetname=0)
print(file.head())
import pandas as pd

file = pd.read_excel('L:/TRADING/ANALYSIS/PRICES/CXL Prices.xlsx', sheetname=0)
print(file.head())
file.tail(5)
file.tail(50)
unique_name_list = file['name'].unique()
unique_name_list = pd.Series(file['name'].unique())
filter = file[file['name'] == 'P Diesel 10ppm FOB RDAM FWD']
from datetime import date

filter = file[(file['name'] == 'P Diesel 10ppm FOB RDAM FWD') &
              (file['asof_date' == date(2016,1,1)])]
filter = file[(file['name'] == 'P Diesel 10ppm FOB RDAM FWD') &
              (file['asof_date' == date(2016,4,1)])]
filter = file[(file['name'] == 'P Diesel 10ppm FOB RDAM FWD') &
              (file['asof_date' == datetime(2016,4,1)])]
from datetime import datetime

filter = file[(file['name'] == 'P Diesel 10ppm FOB RDAM FWD') &
              (file['asof_date' == datetime(2016,4,1)])]
filter = file[(file['name'] == 'P Diesel 10ppm FOB RDAM FWD') &
              (file['asof_date'] == datetime(2016,4,1))]
runfile('C:/Users/mima/.spyder-py3/refinery_database_tworking.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/refinery_database_tworking.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/KALUNBORG.py', wdir='C:/Users/mima/.spyder-py3')
ref_data = ref_data[(ref_data['REFINERY'] == 'KALUNDBORG') & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['REFINERY'] == 'KALUNDBORG') & (ref_data[5] > date(2017,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
ref_data = ref_data[(ref_data['REFINERY'] == 'KALUNDBORG') & (ref_data['START_DATE'] > date(2017,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['REFINERY'] == 'KALUNDBORG') & (ref_data['START_DATE'] > datetime(2017,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]

## ---(Tue Dec  5 10:46:28 2017)---
file = pd.read_excel('L:/TRADING/ANALYSIS/PRICES/CXL Prices.xlsx', sheetname=0)
print(file.head())

unique_name_list = pd.Series(file['name'].unique())

from datetime import datetime

filter = file[(file['name'] == 'P Diesel 10ppm FOB RDAM FWD') &
              (file['asof_date'] == datetime(2016,4,1))]
import pandas as pd

file = pd.read_excel('L:/TRADING/ANALYSIS/PRICES/CXL Prices.xlsx', sheetname=0)
print(file.head())

unique_name_list = pd.Series(file['name'].unique())

from datetime import datetime

filter = file[(file['name'] == 'P Diesel 10ppm FOB RDAM FWD') &
              (file['asof_date'] == datetime(2016,4,1))]
runfile('C:/Users/mima/.spyder-py3/cxl experiment.py', wdir='C:/Users/mima/.spyder-py3')
import sqlite3
import pyodbc
cnxn = pyodbc.connect(
        r'Driver={SQL Server Native Client 11.0};'
        r'Server=.\STCHGS126;'
        r'UID=gami;'
        r'Database=myDB;'
        r'Trusted_Connection=yes;'
        r'WSID=STUKLW05;'
        r'DATABASE=STG_Price;'
        r'ServerSPN=STG_Price;')
cursor = cnxn.cursor()
cursor.execute("""SELECT STUK_Prices.curve_num, STUK_Prices.name, STUK_Prices.asof_date, STUK_Prices.maturity_date, STUK_Prices.maturity_type, STUK_Prices.price
FROM STG_Price.dbo.STUK_Prices STUK_Prices
WHERE STUK_Prices.asof_date >= '1-jan-2010'""")
while 1:
    row = cursor.fetchone()
    if not row:
        break
    print(row.LastName)

cnxn.close()


debugfile('C:/Users/mima/.spyder-py3/SQL Connection Attempt.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/SQL Connection Attempt.py', wdir='C:/Users/mima/.spyder-py3')
import pyodbc
cnxn = pyodbc.connect(
        '''Driver={SQL Server Native Client 11.0};
        Server=.\STCHGS126;
        UID=gami;
        Trusted_Connection=yes;
        APP=Microsoft Office 2010;
        WSID=STUKLW05;
        DATABASE=STG_Price;
        ServerSPN=STG_Price''')
cursor = cnxn.cursor()
import pyodbc

cnxn = pyodbc.connect(
        '''Driver={SQL Server Native Client 11.0};
        Server=.\STCHGS126;
        UID=gami;
        Trusted_Connection=yes;
        APP=Microsoft Office 2010;
        WSID=STUKLW05;
        DATABASE=STG_Price;
        ServerSPN=STG_Price''')
cursor = cnxn.cursor()
import pypyodbc
connection = pypyodbc.connect('Driver={SQL Server};'
 'Server=STCHGS126;'
 'Database=STG_Price;'
 'uid=gami')
connection.close()
import pyodbc
connection = pypyodbc.connect('Driver={SQL Server};'
 'Server=STCHGS126;'
 'Database=STG_Price;'
 'uid=gami')
connection.close()
import pyodbc
connection = pyodbc.connect('Driver={SQL Server};'
 'Server=STCHGS126;'
 'Database=STG_Price;'
 'uid=gami')
connection.close()
import pyodbc
cnxn = pyodbc.connect(
        '''Driver={SQL Server Native Client 11.0};
        Server=STCHGS126;
        UID=gami;
        Trusted_Connection=yes;
        WSID=STUKLW05;
        DATABASE=STG_Price;
        ServerSPN=STG_Price''')
cursor = cnxn.cursor()
cursor.execute("""SELECT STUK_Prices.curve_num, STUK_Prices.name, STUK_Prices.asof_date, STUK_Prices.maturity_date, STUK_Prices.maturity_type, STUK_Prices.price
FROM STG_Price.dbo.STUK_Prices STUK_Prices
WHERE STUK_Prices.asof_date >= '1-jan-2010'""")
while 1:
    row = cursor.fetchone()
    if not row:
        break
    print(row.LastName)

cnxn.close()
import pyodbc
cnxn = pyodbc.connect(
        '''Driver={SQL Server Native Client 11.0};
        Server=STCHGS126;
        UID=gami;
        Trusted_Connection=yes;
        WSID=STUKLW05;
        DATABASE=STG_Price;
        ServerSPN=STG_Price''')
cursor = cnxn.cursor()
cursor.execute("""SELECT STUK_Prices.curve_num, STUK_Prices.name, STUK_Prices.asof_date, STUK_Prices.maturity_date, STUK_Prices.maturity_type, STUK_Prices.price
FROM STG_Price.dbo.STUK_Prices STUK_Prices
WHERE STUK_Prices.asof_date >= '1-jan-2010'""")
while 1:
    row = cursor.fetchone()
    if not row:
        break
    print(row.LastName)

cnxn.close()
while 1:
    row = cursor.fetchone()
    if not row:
        break
    print(row)

cnxn.close()

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 
ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] == datetime(2018,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]

pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)

for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = date(2005,1,1)
filtered_end_date = date(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]

#maintenance_framework.to_excel(test_path+'daily.xlsx')
#maintenance_framework_month.to_excel(test_path+'monthly.xlsx')
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] == datetime(2018,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)
for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = date(2005,1,1)
filtered_end_date = date(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]
for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]


filtered_start_date = date(2005,1,1)
filtered_end_date = date(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()
filtered_start_date = date(2005,1,1)
filtered_end_date = date(2020,12,1)    
filtered_start_date = date(2005,1,1)
for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]

filtered_start_date = date(2005,1,1)
filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)   
filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]
filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  
import pandas as pd
import numpy as np
from datetime import datetime, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] == datetime(2018,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)

for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]
filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1) 
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  
print(date)
filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
print(d)
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
print(date)
for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]

import pandas as pd
import numpy as np
from datetime import datetime, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)
ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] == datetime(2018,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] == datetime(2018,1,1,0,0,0)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] > datetime(2018,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)
ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] > datetime(2018,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)

for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]
"""
Created on Mon Dec  4 10:07:04 2017

@author: mima
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] > datetime(2018,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]

pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)

for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]

test_path='L:/TRADING/ANALYSIS/Python/MIMA/RefineryDatabaseChecks/'

maintenance_framework.to_excel(test_path+'daily.xlsx')
maintenance_framework_month.to_excel(test_path+'monthly.xlsx')
maintenance_framework_week.to_excel(test_path+'weekly.xlsx')
maintenance_framework_decade.to_excel(test_path+'decade.xlsx')
import pandas as pd
import numpy as np
from datetime import datetime, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] > datetime(2017,12,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]

pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)

for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]

test_path='L:/TRADING/ANALYSIS/Python/MIMA/RefineryDatabaseChecks/'

maintenance_framework.to_excel(test_path+'daily.xlsx')
maintenance_framework_month.to_excel(test_path+'monthly.xlsx')
maintenance_framework_week.to_excel(test_path+'weekly.xlsx')
maintenance_framework_decade.to_excel(test_path+'decade.xlsx')
import pandas as pd
import numpy as np
from datetime import datetime, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] > datetime(2017,12,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]

pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)

for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]

test_path='L:/TRADING/ANALYSIS/Python/MIMA/RefineryDatabaseChecks/'

maintenance_framework.to_excel(test_path+'daily.xlsx')
maintenance_framework_month.to_excel(test_path+'monthly.xlsx')
maintenance_framework_week.to_excel(test_path+'weekly.xlsx')
maintenance_framework_decade.to_excel(test_path+'decade.xlsx')
ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)
import pandas as pd
import numpy as np
from datetime import datetime, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] > datetime(2017,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]
import pandas as pd
import numpy as np
from datetime import datetime, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//REST of WORLD.xlsm",
                        sheetname=0, header=0)

ref_data = ref_data[(ref_data['COUNTRY'] == 'RUSSIAN FEDERATION') & (ref_data['START_DATE'] > datetime(2017,1,1)) & ((ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2'))]

pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')

ref_list=pd.Series(ref_data['REFINERY'].unique())

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)

for index, row in ref_data.iterrows():
    end_date = row[5]
    start_date = row[4]
    name = row[1]
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]



filtered_start_date = datetime(2005,1,1)
filtered_end_date = datetime(2020,12,1)    
d = maintenance_framework.index.day - np.clip((maintenance_framework.index.day-1) // 10, 0, 2)*10 - 1
date = maintenance_framework.index.values - np.array(d, dtype="timedelta64[D]")
maintenance_framework_decade = maintenance_framework.groupby(date).mean()  

maintenance_framework_month = maintenance_framework.resample('M', convention = 'start').mean()
maintenance_framework_week =  maintenance_framework.resample('W', convention = 'start').mean()

maintenance_framework_month = maintenance_framework_month.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_week = maintenance_framework_week.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework_decade = maintenance_framework_decade.loc[filtered_start_date:filtered_end_date,:]
maintenance_framework = maintenance_framework.loc[filtered_start_date:filtered_end_date,:]

test_path='L:/TRADING/ANALYSIS/Python/MIMA/RefineryDatabaseChecks/'

maintenance_framework.to_excel(test_path+'daily.xlsx')
maintenance_framework_month.to_excel(test_path+'monthly.xlsx')
maintenance_framework_week.to_excel(test_path+'weekly.xlsx')
maintenance_framework_decade.to_excel(test_path+'decade.xlsx')
runfile('C:/Users/mima/.spyder-py3/refinery_database_tworking.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd

latam1020 = pd.read_csv("C:/Users/mima/Desktop/latam 2010.csv")
import pandas as pd

latam1020 = pd.read_csv("C:/Users/mima/Desktop/latam 2010.csv")
latam1027 = pd.read_csv("C:/Users/mima/Desktop/latam 2710.csv")

## ---(Fri Dec  8 14:14:43 2017)---
conn = sqlite3.connect('L:/TRADING/ANALYSIS/Python/MIMA/TestAndTextbooks/tysql.sqlite') 
"""

import sqlite3

conn = sqlite3.connect('L:/TRADING/ANALYSIS/Python/MIMA/TestAndTextbooks/tysql.sqlite') 
import sqlite3

conn = sqlite3.connect('tysql.sqlite') 
cur = conn.cursor()
c.execute('''

CREATE TABLE Products
(
  prod_id    char(10)      NOT NULL ,
  vend_id    char(10)      NOT NULL ,
  prod_name  char(255)     NOT NULL ,
  prod_price decimal(8,2)  NOT NULL ,
  prod_desc  varchar(1000) NULL 
);

''')
c = conn.cursor()
c.execute('''

CREATE TABLE Products
(
  prod_id    char(10)      NOT NULL ,
  vend_id    char(10)      NOT NULL ,
  prod_name  char(255)     NOT NULL ,
  prod_price decimal(8,2)  NOT NULL ,
  prod_desc  varchar(1000) NULL 
);

''')
conn = sqlite3.connect('tysql.sqlite') 
c = conn.cursor()
c.execute('''

CREATE TABLE Products
(
  prod_id    char(10)      NOT NULL ,
  vend_id    char(10)      NOT NULL ,
  prod_name  char(255)     NOT NULL ,
  prod_price decimal(8,2)  NOT NULL ,
  prod_desc  varchar(1000) NULL 
);

''')
import pypdf2
"""
 import pypdf2
import pypdf2

## ---(Mon Dec 11 20:26:34 2017)---
import pypdf2

pip install pypdf2
import pandas as pd
import numpy as np
begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)
dtrange = pd.date_range(begin, end)
p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10
df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)
dfindexday = df.index.day
d = df.index.day - np.clip((df.index.day-1) // 10, 0, 2)*10 - 1
date = df.index.values - np.array(d, dtype="timedelta64[D]")
print(df.index.day)
print(df.index.day-1)
print((df.index.day-1) // 10)
print(np.clip((df.index.day-1) // 10, 0, 2))
print(np.clip((df.index.day-1) // 10, 0, 2))*10
%clear
import pandas as pd
import numpy as np
begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10
df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)
print(df.index.day)
print(df.index.day-1)
print((df.index.day-1) // 10)
print(np.clip((df.index.day-1) // 10, 0, 2))
print((np.clip((df.index.day-1) // 10, 0, 2))*10)
print((np.clip((df.index.day-1) // 10, 0, 2))*10-1)
print(df.index.values)
print(np.array(d, dtype="timedelta64[D]"))
d = df.index.day - np.clip((df.index.day-1) // 10, 0, 2)*10 - 1
print(np.array(d, dtype="timedelta64[D]"))
%clear
print(df.index.day)
print(df.index.day-1)
print((df.index.day-1) // 10)
print(df.index.day -1 -(np.clip((df.index.day-1) // 10, 0, 2))*10)
d = df.index.day -1 - np.clip((df.index.day-1) // 10, 0, 2)*10
print(df.index.values)
print(np.array(d, dtype="timedelta64[D]"))
print(df.index.values - np.array(d, dtype="timedelta64[D]"))
df.groupby(date).mean()

## ---(Wed Dec 20 11:46:02 2017)---
import pandas as pd
import numpy as np

file = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")

MappingData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Mapping")
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")

UniqueMappingDataLordPorts = MappingData['Load Port'].unique()
UniqueMappingDataLordPorts = MedFlowsData['Load Port'].unique()
print(UniqueMappingDataLordPorts)
UniqueMappingDataLordPorts = str(MedFlowsData['Load Port'].unique())
print(UniqueMappingDataLordPorts)
UniqueMappingDataLordPorts = list(MedFlowsData['Load Port'].unique())
print(UniqueMappingDataLordPorts)
UniqueMappingDataLordPorts = set(MedFlowsData['Load Port'])
print(UniqueMappingDataLordPorts)
UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))
UniqueMappingDataLordPorts = MedFlowsData['Load Port'].unique().sort()
UniqueMappingDataLordPorts = list(MedFlowsData['Load Port'].unique().sort())
UniqueMappingDataLordPorts = list(MedFlowsData['Load Port'].unique()).sort()
list(MedFlowsData['Load Port'].unique())
UniqueMappingDataLordPorts = list(MedFlowsData['Load Port'].unique())
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="Port Coords")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")
UniqueMappingDataLordPorts.iloc[1,1]
UniqueMappingDataLordPorts[1]
UniqueMedFlowsDatadPorts = list(MedFlowsData['Load Port'].unique())
x = MappingData.values.unique()
x = MappingData.columns.values.unique()
x = MappingData.columns.values
x = list(MappingData.columns.values)
UniqueMappingDataPorts = list(MappingData['TargoPortName'].unique())
import pandas as pd

file = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv",'r')
file = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
import pandas as pd

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
load_ports = list(all_clipper['load_port'].unique())
col_headers = all_clipper.columns.values
col_headers = list(all_clipper.columns.values)


load_ports = list(all_clipper['load_port'].unique())
discharge_ports = list(all_clipper['offtake_port'].unique())
port_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
col_headers = list(all_clipper.columns.values)


load_ports = list(all_clipper['load_port'].unique())
discharge_ports = list(all_clipper['offtake_port'].unique())


"""
Created on Thu Dec 21 18:08:55 2017

@author: mima
"""

import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
col_headers = list(all_clipper.columns.values)


load_ports = list(all_clipper['load_port'].unique())
discharge_ports = list(all_clipper['offtake_port'].unique())




clip_col_headers = list(all_clipper.columns.values)
targo_port_region_mappings_col_headers = list(targo_port_region_mappings.columns.values)


load_ports = list(all_clipper['load_port'].unique())
discharge_ports = list(all_clipper['offtake_port'].unique())




import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = list(all_clipper.columns.values)
targo_port_region_mappings_col_headers = list(targo_port_region_mappings.columns.values)


load_ports = list(all_clipper['load_port'].unique())
discharge_ports = list(all_clipper['offtake_port'].unique())


checked_list = pd.DataFrame()
checked_list = pd.DataFrame(targo_port_region_mappings_col_headers)
checked_list = pd.DataFrame(headers=targo_port_region_mappings_col_headers)
checked_list = pd.DataFrame(,targo_port_region_mappings_col_headers)
checked_list = pd.DataFrame(0,targo_port_region_mappings_col_headers)
checked_list = pd.DataFrame(columns=targo_port_region_mappings_col_headers)
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = list(all_clipper.columns.values)
targo_port_region_mappings_col_headers = list(targo_port_region_mappings.columns.values)

### get the unique port names that we need
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = list(all_clipper_load_ports + all_clipper_discharge_ports).unique()
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_port
all_clipper_ports = list(all_clipper_ports).unique()
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique(
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_port
all_clipper_ports = list(all_clipper_ports).unique()
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_ports = list(all_clipper_ports).unique()
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_ports = list(all_clipper_ports.unique())
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_ports = all_clipper_ports.unique()
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_ports = pd.Serie(all_clipper_ports)
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_ports = pd.Series(all_clipper_ports)
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_ports = pd.Series(all_clipper_ports).unique()
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
all_clipper_load_ports = list(all_clipper['load_port'].unique())
all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = all_clipper_load_ports + all_clipper_discharge_ports
all_clipper_ports = list(pd.Series(all_clipper_ports).unique())
targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = list(all_clipper.columns.values)
targo_port_region_mappings_col_headers = list(targo_port_region_mappings.columns.values)

### get the unique port names that we need
unique_all_clipper_load_ports = list(all_clipper['load_port'].unique())
unique_all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = unique_all_clipper_load_ports + unique_all_clipper_discharge_ports

### this means there are some load and discharge ports that are the same - investigate later
all_unique_clipper_ports = list(pd.Series(all_clipper_ports).unique())
unique_targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = list(all_clipper.columns.values)
targo_port_region_mappings_col_headers = list(targo_port_region_mappings.columns.values)

### get the unique port names that we need
unique_all_clipper_load_ports = list(all_clipper['load_port'].unique())
unique_all_clipper_discharge_ports = list(all_clipper['offtake_port'].unique())
all_clipper_ports = unique_all_clipper_load_ports + unique_all_clipper_discharge_ports

### this means there are some load and discharge ports that are the same - investigate later
all_unique_clipper_ports = list(pd.Series(all_clipper_ports).unique())
unique_targo_port_region_ports = list(targo_port_region_mappings['Clipper Port Name'].unique())
print(all_unique_clipper_ports)
clip_col_headers = all_clipper.columns.values
import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = all_clipper.columns.values
targo_port_region_mappings_col_headers = targo_port_region_mappings.columns.values

### get the unique port names that we need
unique_all_clipper_load_ports = all_clipper['load_port'].unique()
unique_all_clipper_discharge_ports = all_clipper['offtake_port'].unique()
all_clipper_ports = unique_all_clipper_load_ports + unique_all_clipper_discharge_ports

import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = all_clipper.columns.values
targo_port_region_mappings_col_headers = targo_port_region_mappings.columns.values

### get the unique port names that we need
unique_all_clipper_load_ports = all_clipper['load_port'].unique()
unique_all_clipper_discharge_ports = all_clipper['offtake_port'].unique()
all_clipper_ports = unique_all_clipper_load_ports + unique_all_clipper_discharge_ports


### get all thge unique 

### this means there are some load and discharge ports that are the same - investigate later
all_unique_clipper_ports = all_clipper_ports.unique()
unique_targo_port_region_ports = targo_port_region_mappings['Clipper Port Name'].unique()

x = all_unique_clipper_ports.tolist()
all_unique_clipper_ports = all_clipper_ports.unique()
unique_targo_port_region_ports = targo_port_region_mappings['Clipper Port Name'].unique()

import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = all_clipper.columns.values
targo_port_region_mappings_col_headers = targo_port_region_mappings.columns.values

### get the unique port names that we need
unique_all_clipper_load_ports = all_clipper['load_port'].unique()
unique_all_clipper_discharge_ports = all_clipper['offtake_port'].unique()
all_clipper_ports = unique_all_clipper_load_ports + unique_all_clipper_discharge_ports


### get all thge unique 

### this means there are some load and discharge ports that are the same - investigate later
all_unique_clipper_ports = all_clipper_ports.unique()
unique_targo_port_region_ports = targo_port_region_mappings['Clipper Port Name'].unique()
import pandas as pd
import numpy as np

all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = all_clipper.columns.values
targo_port_region_mappings_col_headers = targo_port_region_mappings.columns.values

### get the unique port names that we need
unique_all_clipper_load_ports = all_clipper['load_port'].unique()
unique_all_clipper_discharge_ports = all_clipper['offtake_port'].unique()
all_clipper_ports = unique_all_clipper_load_ports.tolist() + unique_all_clipper_discharge_ports.tolist()
import pandas as pd


all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = all_clipper.columns.values
targo_port_region_mappings_col_headers = targo_port_region_mappings.columns.values

### get the unique port names that we need
unique_all_clipper_load_ports = all_clipper['load_port'].unique()
unique_all_clipper_discharge_ports = all_clipper['offtake_port'].unique()
clip_col_headers = all_clipper.columns.values
targo_port_region_mappings_col_headers = targo_port_region_mappings.columns.values

### get the unique port names that we need
unique_all_clipper_load_ports = all_clipper['load_port'].unique
unique_all_clipper_discharge_ports = all_clipper['offtake_port'].unique
import pandas as pd


all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
targo_port_region_mappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
clip_col_headers = all_clipper.columns.values
targo_port_region_mappings_col_headers = targo_port_region_mappings.columns.values

### get the unique port names that we need
unique_all_clipper_load_ports = all_clipper['load_port'].unique
unique_all_clipper_discharge_ports = all_clipper['offtake_port'].unique
print(unique_all_clipper_load_ports)
import pandas as pd


all_clipper = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need
LoadPorts = ClipperRawData['load_port'].unique
DischargePorts = ClipperRawData['offtake_port'].unique
LoadPorts = list(ClipperRawData['load_port'].unique)
LoadPorts = list(ClipperRawData['load_port']).unique
LoadPorts = (list(ClipperRawData['load_port'])).unique
LoadPorts = ClipperRawData['load_port'].unique()
DischargePorts = ClipperRawData['offtake_port'].unique()
LoadPorts = list(ClipperRawData['load_port'].unique())
LoadPorts = list(ClipperRawData['load_port'].unique())
DischargePorts = list(ClipperRawData['offtake_port'].unique())
LoadPorts = list(ClipperRawData['load_port'].unique())
DischargePorts = list(ClipperRawData['offtake_port'].unique())

### which ports do we not have in our original list that we would want mapped
compared_port_lists = set(LoadPorts).symmetric_difference(set(DischargePorts))

print(compared_port_lists)
x = [i for i in LoadPorts not in DischargePorts]
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())
x = [i for i in LoadPorts not in DischargePorts]
LoadPorts = list(pd.Series(ClipperRawData['load_port'].unique()))
LoadPorts = ClipperRawData['load_port'].unique()
LoadPorts = LoadPorts.tolist()
DischargePorts = ClipperRawData['offtake_port'].unique()
LoadPorts = set(ClipperRawData['load_port'].unique())
LoadPorts = set(ClipperRawData['load_port'].unique())
DischargePorts = set(ClipperRawData['offtake_port'].unique())
x = [i for i in LoadPorts not in DischargePorts]
LoadPorts = list(set(ClipperRawData['load_port'].unique()))
DischargePorts = list(set(ClipperRawData['offtake_port'].unique()))
print(ClipperRawData.dtypes)
LoadPorts = ClipperRawData['load_port'].astype(str).unique()
LoadPorts = ClipperRawData['load_port'].astype(str)
LoadPorts = ClipperRawData['load_port'].unique().astype(str)
LoadPorts = ClipperRawData['load_port'].unique().astype(str)
DischargePorts = ClipperRawData['offtake_port'].unique().astype(str)
x = [i for i in LoadPorts not in DischargePorts]
x = []
x = [x.append(i) for i in LoadPorts not in DischargePorts]
LoadPorts = list(ClipperRawData['load_port'].unique().astype(str))
DischargePorts = list(ClipperRawData['offtake_port'].unique().astype(str))
LoadPorts = list(ClipperRawData['load_port'].unique())
LoadPorts.dtypes
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())
LoadPorts.dtypes
allunique=[list(LoadPorts)+list(DischargePorts)]
allunique=[LoadPorts+DischargePorts]
allunique=pd.Series(LoadPorts+DischargePorts)
allunique=LoadPorts.append(DischargePorts)
allunique=[LoadPorts.append(DischargePorts)].unique()
allunique=(LoadPorts.append(DischargePorts)).unique()
allunique=pd.Series((LoadPorts.append(DischargePorts)).unique())
dfNotIn= pd.DataFrame(PortMappings['Clipper Port Name'] not in pd.DataFrame(allunique))
x = []
x = [x.append(i) for i in PortMappings['Clipper Port Name'] not in allunique]
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique[0]
dfNotIn= allunique.loc[allunique[0] != PortMappings['Clipper Port Name']]
PortMappingsPorts = pd.DataFrame(PortMappings['Clipper Port Name'])
check = pd.merge(allunique,PortMappingsPort, indicator=True)
check = pd.merge(allunique,PortMappingsPorts, indicator=True)
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

print(ClipperRawData.dtypes)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())

PortMappingsPorts['Ports'] = pd.DataFrame(PortMappings['Clipper Port Name'])
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())

allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={'0':'Ports'})

PortMappingsPorts = pd.DataFrame(PortMappings['Clipper Port Name'])
PortMappingsPorts = PortMappingsPorts.rename(columns={'Clipper Port Name':'Ports'})

check = pd.merge(allunique,PortMappingsPorts, indicator=True)
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={'0':'Ports'})
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
check = pd.merge(allunique,PortMappingsPorts, indicator=True)
check = pd.merge(allunique,PortMappingsPorts, how='outer', indicator=True)
allunique_lower = allunique.str.lower()
allunique_lower = allunique['Ports'].str.lower()
PortMappingsPorts_lower = PortMappingsPorts['Ports'].str.lower()
allunique_lower = allunique['Ports'].str.lower()
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)
PortsNotMapped = check[check['_merge' == 'left_only']]
PortsNotMapped = check[check['_merge'] == 'left_only']
countries = pd.Series(ClipperRawData['offtake_country'].unique())
countries_check = pd.merge(PortsNotMapped, ClipperDataRaw.apply(lambda x: x.astype(str).str.lower()), how='left')
countries_check = pd.merge(PortsNotMapped, ClipperRawData.apply(lambda x: x.astype(str).str.lower()), how='left')
countries_check = pd.merge(PortsNotMapped[0], ClipperRawData[13].apply(lambda x: x.astype(str).str.lower()), how='left')
PortsNotMapped[0]
ClipperRawDataLoadPorts = ClipperRawData.rename(columns={'load_port':'Ports'})
countries_check = pd.merge(PortsNotMapped, ClipperRawDataLoadPorts.apply(lambda x: x.astype(str).str.lower()), how='left')
clean_countries_check = pd.DataFrame(countries_check[['Ports','Load Country', 'Load Region']].unique())
clean_countries_check = pd.DataFrame(countries_check[['Ports','load_country', 'load_region']].unique())
clean_countries_check = pd.DataFrame(countries_check[['Ports','load_country', 'load_region']].unique)
clean_countries_check = countries_check[['Ports','load_country', 'load_region']].
clean_countries_check = countries_check[['Ports','load_country', 'load_region']]
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
MappingData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Mapping")
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.Series(MedFlowsData['Load Port'].unique())
LoadPorts = pd.Series(MedFlowsData['Load Port'].str.lower().unique())
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.Series(MedFlowsData['Load Port'].str.lower().unique())
MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())


### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts_lower = pd.DataFrame(TargoPorts['Ports'].str.lower())
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())


### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts_lower = pd.DataFrame(TargoPorts['Ports'].str.lower())

F3check = pd.merge(TargoPorts,LoadPorts, how='outer', indicator=True)
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())
LoadPorts= LoadPorts.rename(columns={0:'Ports'})

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts_lower = pd.DataFrame(TargoPorts['Ports'].str.lower())

F3check = pd.merge(TargoPorts,LoadPorts, how='outer', indicator=True)
F3check = pd.merge(LoadPorts,TargoPorts, how='outer', indicator=True)
MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())
LoadPorts= LoadPorts.rename(columns={0:'Ports'})

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts_lower = pd.DataFrame(TargoPorts['Ports'].str.lower())

F3check = pd.merge(LoadPorts,TargoPorts, how='outer', indicator=True)
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F3.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())
LoadPorts= LoadPorts.rename(columns={0:'Ports'})

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts = pd.DataFrame(TargoPorts['Ports'].str.lower())

F3check = pd.merge(LoadPorts,TargoPorts, how='outer', indicator=True)
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F31.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())
LoadPorts= LoadPorts.rename(columns={0:'Ports'})

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts = pd.DataFrame(TargoPorts['Ports'].str.lower())

F3check = pd.merge(LoadPorts,TargoPorts, how='outer', indicator=True)
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

print(ClipperRawData.dtypes)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need as a pandas data series
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
PortMappingsPorts = pd.DataFrame(PortMappings['Clipper Port Name'])
PortMappingsPorts = PortMappingsPorts.rename(columns={'Clipper Port Name':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F31.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())
LoadPorts= LoadPorts.rename(columns={0:'Ports'})

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts = pd.DataFrame(TargoPorts['Ports'].str.lower())

F3check = pd.merge(LoadPorts,TargoPorts, how='outer', indicator=True)
import pandas as pd
import numpy as np

MedFlowsData = pd.read_excel('L:\TRADING\ANALYSIS\GLOBAL\MED Balances\Flows Med Sweet  F31.xlsm', sheetname="Apex edited Data")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
StaticData = pd.read_excel('L:\TRADING\Targo\Mapping.xlsx', sheetname="Static Data")
PortCoords = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")


### This is another way to sort for unique names in the list
#UniqueMappingDataLordPorts = list(set(MedFlowsData['Load Port']))

LoadPorts = pd.DataFrame(MedFlowsData['Load Port'].str.lower().unique())
LoadPorts= LoadPorts.rename(columns={0:'Ports'})

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
TargoPorts = pd.DataFrame(PortMappings['Targo Port Name'])
TargoPorts = TargoPorts.rename(columns={'Targo Port Name':'Ports'})
TargoPorts = pd.DataFrame(TargoPorts['Ports'].str.lower())

F3check = pd.merge(LoadPorts,TargoPorts, how='outer', indicator=True)
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

print(ClipperRawData.dtypes)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need as a pandas data series
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())


### add the load ports to the discharge ports and return as a DataFrame, rename column to Ports for comparing and also and lower case for comparison
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
PortMappingsPorts = pd.DataFrame(PortMappings['Clipper Port Name'])
PortMappingsPorts = PortMappingsPorts.rename(columns={'Clipper Port Name':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())

### merge both to see which is in which list
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)

PortsNotMapped = check[check['_merge'] == 'left_only']
ClipperPortsNotMapped = check[check['_merge'] == 'left_only']
TargoPortsNotMapped = check[check['_merge'] == 'right_only']
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

print(ClipperRawData.dtypes)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need as a pandas data series
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())


### add the load ports to the discharge ports and return as a DataFrame, rename column to Ports for comparing and also and lower case for comparison
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
PortMappingsPorts = pd.DataFrame(PortMappings['Clipper Port Name'])
PortMappingsPorts = PortMappingsPorts.rename(columns={'Clipper Port Name':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())

### merge both to see which is in which list
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)

ClipperPortsNotMapped = check[check['_merge'] == 'left_only']
TargoPortsNotMapped = check[check['_merge'] == 'right_only']
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")
x = PortCoordsTEST.loc[PortCoordsTEST['Latitude'] > 30]
x = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] > 30) & (PortCoordsTEST['Longtitude'] > 30) ]
x = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] > 30) & (PortCoordsTEST['Longitude'] > 30) ]
portlist = []
portlist.append('Ceyan')
PortCoordsTEST['Latitude']
PortCoordsTEST['TargoAssetName'] in portlist
portlist = []
portlist.append('Ceyhan')
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] in portlist]
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin portlist]
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)]
x['Latitude']+0.015
(x['Latitude']+0.015) < PortCoordsTEST['Latitude'] < (x['Latitude']-0.015)
x['Latitude']
PortCoordsTEST['Latitude']
x['Latitude']+0.015
x['Latitude']
y = PortCoordsTEST.loc[(x['Latitude']+0.015) < PortCoordsTEST['Latitude'] < (x['Latitude']-0.015)]
y = PortCoordsTEST.iloc[(x['Latitude']+0.015) < PortCoordsTEST['Latitude'] < (x['Latitude']-0.015)]
y = PortCoordsTEST.loc[(x['Latitude']+0.015) < x < (x['Latitude']-0.015)]
x
y = PortCoordsTEST.loc[(x['Latitude']+0.015) < PortCoordsTEST.loc(x) < (x['Latitude']-0.015)]
y = PortCoordsTEST.loc[(x['Latitude']+0.015) < PortCoordsTEST.loc(x['Latitude']) < (x['Latitude']-0.015)]
PortCoordsTEST.loc(x['Latitude'])
y = PortCoordsTEST.loc[(x['Latitude']+0.015)]
(x['Latitude']+0.015)
y = PortCoordsTEST.loc[PortCoordsTEST['Latitude'] > (x['Latitude']+0.015)]
portlist = []
portlist.append('Ceyhan')
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)]
x = x.sort_index(inplace=True)


y = PortCoordsTEST.loc[PortCoordsTEST['Latitude'] > (x['Latitude']+0.015)]
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)]
portlist = []
portlist.append('Ceyhan')
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)]
x = list(x['Latitude'])


y = PortCoordsTEST.loc[PortCoordsTEST['Latitude'] > (x+0.015)]
y = PortCoordsTEST.loc[PortCoordsTEST['Latitude'] > ([x+0.015])]
rows = PortCoordsTEST.loc[PortCoordsTEST['Latitude'] > 5]
rows = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist) | 
        PortCoordsTEST['TargoAssetName'].isbetween(30,40, inclusive=True)]
rows = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist) | 
        PortCoordsTEST['TargoAssetName'].between(30,40, inclusive=True)]
rows = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist) | 
        PortCoordsTEST['Latitude'].between(30,40, inclusive=True)]
PortCoordsTEST['TargoAssetName'].isin(portlist)
PortCoordsTEST['TargoAssetName'].isin(portlist).between(30,40, inclusive=True)
PortCoordsTEST['TargoAssetName'].isin(portlist)[PortCoordsTEST['Latitude'].between(30,40, inclusive=True)]
p = PortCoordsTEST['TargoAssetName'].isin(portlist)[PortCoordsTEST['Latitude'].between(30,40, inclusive=True)]
p = PortCoordsTEST['TargoAssetName'].isin(portlist)
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)]
ab = PortCoordsTEST.loc[PortCoordsTEST['Latitude'].between(x['Latitude']+1,x['Latitude']+5, inclusive=True)]
x['Latitude']+1
PortCoordsTEST['Latitude']
mask = x['Latitude']


ab = PortCoordsTEST.loc[PortCoordsTEST['Latitude'].between(mask+1,mask+5, inclusive=True)]
mask = PortCoordsTEST.mask((PortCoordsTEST['Latitude'] > x['Latitude']+5) & (PortCoordsTEST['Latitude'] < x['Latitude']+5))
mask = PortCoordsTEST.mask((PortCoordsTEST['Latitude'] > x.loc[0,'Latitude']+5) & (PortCoordsTEST['Latitude'] < x.loc[0,'Latitude']+5))
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)].reset_index()
mask = PortCoordsTEST.mask((PortCoordsTEST['Latitude'] > x.loc[0,'Latitude']+5) & (PortCoordsTEST['Latitude'] < x.loc[0,'Latitude']+5))
ab = PortCoordsTEST.loc[PortCoordsTEST['Latitude'].between(mask+1,mask+5, inclusive=True)]
portlist = []
portlist.append('Ceyhan','Ceyhan Botas')
portlist = []
portlist.append(['Ceyhan','Ceyhan Botas'])
portlist = []
portlist.append['Ceyhan','Ceyhan Botas']
portlist = []
portlist.append(['Ceyhan','Ceyhan Botas'])
portlist = []
portlist.append('Ceyhan','Ceyhan Botas')
portlist = []
portlist.append(list('Ceyhan','Ceyhan Botas'))
portlist = []
portlist.append('Ceyhan')
portlist = []
portlist.append(['Ceyhan Botas','Ceyhan'])
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)].reset_index()
portlist = []
portlist.append(set['Ceyhan Botas','Ceyhan'])
portlist = []
portlist = portlist.append(['Ceyhan Botas','Ceyhan'])
portlist = []
portlist = ['Ceyhan Botas','Ceyhan']
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)].reset_index()
y = PortCoordsTEST.where(PortCoordsTEST['Latitude'] > x['Latitude'])
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)].reset_index()

PortCoordsTEST.rest_index()
x = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)].reset_index()

PortCoordsTEST.reset_index()
y = PortCoordsTEST.where(PortCoordsTEST['Latitude'] > x['Latitude'])
y = PortCoordsTEST.where(PortCoordsTEST['Latitude'] > ( PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'].isin(portlist)])['Latitude'])
y = PortCoordsTEST.where(PortCoordsTEST['Latitude'] > 5)
y = PortCoordsTEST.loc(PortCoordsTEST['Latitude'] > 5)
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")
y = PortCoordsTEST.loc(PortCoordsTEST['Latitude'] > 5)
print(y)
y = PortCoordsTEST.loc[PortCoordsTEST['Latitude'] > 5]
y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < 30) & (PortCoordsTEST['Latitude'] > 31) ]
y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < 31) & (PortCoordsTEST['Latitude'] > 30) ]
y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < 31) &
                       (PortCoordsTEST['Latitude'] > 30) &
                       (PortCoordsTEST['Longitude'] > 30) &
                       (PortCoordsTEST['Longitude'] > 31)]
y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < 31) &
                       (PortCoordsTEST['Latitude'] > 30) &
                       (PortCoordsTEST['Longitude'] > 30) &
                       (PortCoordsTEST['Longitude'] < 31)]
y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < 31) &
                       (PortCoordsTEST['Latitude'] > 30) &
                       (PortCoordsTEST['Longitude'] > 20) &
                       (PortCoordsTEST['Longitude'] < 31)]
choice_lat = 30
choice_lon = 30
boundry = 5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] > min_lon) &
                       (PortCoordsTEST['Longitude'] < max_lon)]
choice_lat = 30
choice_lon = 30
boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] > min_lon) &
                       (PortCoordsTEST['Longitude'] < max_lon)]
choice_lat = 30
choice_lon = 40
boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] > min_lon) &
                       (PortCoordsTEST['Longitude'] < max_lon)]
choice_lat = 36
choice_lon = 35
boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] > min_lon) &
                       (PortCoordsTEST['Longitude'] < max_lon)]
choice_lon = 36.5
choice_lat = 35.9
boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] > min_lon) &
                       (PortCoordsTEST['Longitude'] < max_lon)]
choice_lat = 40
choice_lon = 30

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
choice_lat = 40
choice_lon = 30

boundry = 10

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
choice_lat = 36
choice_lon = 35

boundry = 10

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
choice_lat = 36
choice_lon = 35

boundry = 1

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
choice_lat = 36.5
choice_lon = 35.5

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
port = ceyhan

choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == str(port)]['Latitude'].values
port = 'ceyhan'

choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == port]['Latitude'].values
choice_lon = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == port]['Longitude'].values
boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
port = 'ceyhan'

choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == port]['Latitude'].values
choice_lon = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == port]['Longitude'].values

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

port = 'ceyhan'

choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == port]['Latitude'].values
choice_lon = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == port]['Longitude'].values

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == port]['Latitude'].values
PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

portname = 'ceyhan'

choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values
PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

portname = 'Ceyhan'

choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values
choice_lon = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

portname = 'Ceyhan'

choice_lat = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values
choice_lon = PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
(PortCoordsTEST['Latitude'] < max_lat)
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

portname = 'Ceyhan'

choice_lat = int(PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values)
choice_lon = int(PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values)

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

portname = 'Ceyhan'

choice_lat = (PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values).astype(float64)
choice_lon = (PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values).astype(float64)

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

portname = 'Ceyhan'

choice_lat = (PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values).astype(float)
choice_lon = (PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values).astype(float)

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")

portname = 'Ceyhan'

choice_lat = float(PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values)
choice_lon = float (PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values)

boundry = 0.5

max_lat = choice_lat + boundry
min_lat = choice_lat - boundry
max_lon = choice_lon + boundry
min_lon = choice_lon - boundry

y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                       (PortCoordsTEST['Latitude'] > min_lat) &
                       (PortCoordsTEST['Longitude'] < max_lon) &
                       (PortCoordsTEST['Longitude'] > min_lon)]
def find_similar_ports(portname, limit):
    PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")
    choice_lat = float(PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values)
    choice_lon = float (PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values) 
    max_lat = choice_lat + limit
    min_lat = choice_lat - limit
    max_lon = choice_lon + limit
    min_lon = choice_lon - limit
    y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                           (PortCoordsTEST['Latitude'] > min_lat) &
                           (PortCoordsTEST['Longitude'] < max_lon) &
                           (PortCoordsTEST['Longitude'] > min_lon)]
    return y

find_similar_ports('Ceyhan',0.5)
mport pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

print(ClipperRawData.dtypes)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need as a pandas data series
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())


### add the load ports to the discharge ports and return as a DataFrame, rename column to Ports for comparing and also and lower case for comparison
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
PortMappingsPorts = pd.DataFrame(PortMappings['Clipper Port Name'])
PortMappingsPorts = PortMappingsPorts.rename(columns={'Clipper Port Name':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())

### merge both to see which is in which list
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)

ClipperPortsNotMapped = check[check['_merge'] == 'left_only']
TargoPortsNotMapped = check[check['_merge'] == 'right_only']
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)

print(ClipperRawData.dtypes)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values

### get the unique port names that we need as a pandas data series
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())


### add the load ports to the discharge ports and return as a DataFrame, rename column to Ports for comparing and also and lower case for comparison
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
PortMappingsPorts = pd.DataFrame(PortMappings['Clipper Port Name'])
PortMappingsPorts = PortMappingsPorts.rename(columns={'Clipper Port Name':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())

### merge both to see which is in which list
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)

ClipperPortsNotMapped = check[check['_merge'] == 'left_only']
TargoPortsNotMapped = check[check['_merge'] == 'right_only']


### Unique offtake countries
countries = pd.Series(ClipperRawData['offtake_country'].unique())
find_similar_ports('Ceyhan',0.5)
def find_similar_ports(portname, limit):
    PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")
    choice_lat = float(PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Latitude'].values)
    choice_lon = float (PortCoordsTEST.loc[PortCoordsTEST['TargoAssetName'] == portname]['Longitude'].values) 
    max_lat = choice_lat + limit
    min_lat = choice_lat - limit
    max_lon = choice_lon + limit
    min_lon = choice_lon - limit
    y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < max_lat) &
                           (PortCoordsTEST['Latitude'] > min_lat) &
                           (PortCoordsTEST['Longitude'] < max_lon) &
                           (PortCoordsTEST['Longitude'] > min_lon)]
    return y

find_similar_ports('Ceyhan',0.5)
tableports = find_similar_ports('Ceyhan',0.5)
tableports = find_similar_ports('Ceyhan',0.5)[:,3]
tableports = find_similar_ports('Ceyhan',0.5)[3]

## ---(Wed Dec 27 12:14:48 2017)---
import pandas as pd

PortCoordsTEST = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortCoords.xlsx', sheetname="PortCoords")
load_verification = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE', header='Load Port VErification')
import pandas as pd

load_verification = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE', header='Load Port VErification')
import pandas as pd

load_verification = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE', header='Load Port Verification')
load_verification = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE')
import pandas as pd

targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE')
targo_references['Load Port VErification']
targo_references['Load Port Verification']
import pandas as pd

targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE')

load_discharge = targo_references['Load Port Verification']
counterparty = targo_references['Equity']
grade = targo_references['Equity']
load_discharge = targo_references['Load Port Verification'].dropna
load_discharge = pd.Series[targo_references['Load Port Verification'].dropna]
load_discharge = pd.Series(targo_references['Load Port Verification'].dropna)
load_discharge = targo_references['Load Port Verification'].dropna()
import pandas as pd

targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE')

load_discharge = targo_references['Load Port Verification'].dropna()
counterparty = targo_references['Equity'].dropna()
grade = targo_references['Equity'].dropna()
import pandas as pd

targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE')

load_discharge = targo_references['Load Port Verification'].dropna()
counterparty = targo_references['Equity'].dropna()
grade = targo_references['Grade'].dropna()
import pandas as pd

targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
                                  sheetname='REFERENCE')

load_discharge = targo_references['Load Port Verification'].dropna()
counterparty = targo_references['Equity'].dropna()
grade = targo_references['Grades'].dropna()
targo_mappings = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\Targo Ports - Products V1.0.xlsx')
targo_mappings = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\Targo Ports - Products V1.0.xlsx', sheetname='Targo Products')
PortMappings = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\PortMappings.xlsx", sheet = 0)
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Raw data.csv")
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName'])
PortMappingsPorts = PortMappingsPorts.rename(columns={'ClipperPortName':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName']).dropna()
PortMappingsPorts = PortMappingsPorts.rename(columns={'ClipperPortName':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName']).unique().dropna()
PortMappingsPorts = PortMappingsPorts.rename(columns={'ClipperPortName':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName']).dropna()
PortMappingsPorts = PortMappingsPorts.unique()
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName'].unique()).dropna()
PortMappingsPorts = PortMappingsPorts.rename(columns={'ClipperPortName':'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())
print(PortMappings_headers)
PortMappings['ClipperPortName'].unique()
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName'].unique()).dropna()
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName'].unique()).dropna()
PortMappingsPorts = PortMappingsPorts.rename(columns={0:'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)
ClipperPortsNotMapped = check[check['_merge'] == 'left_only']
TargoPortsNotMapped = check[check['_merge'] == 'right_only']
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())
name_convention_check = pd.merge(ClipperPortsNotMapped,TargoPortNameColumn_lower, how='outer', indicator=True)
name_convention_check = name_convention_check[name_convention_check['_merge'] == 'left_only']
abc = name_convention_check[name_convention_check['_merge'] == 'left_only']
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())
name_convention_check = pd.merge(ClipperPortsNotMapped,TargoPortNameColumn_lower, how='outer', indicator=True)
abc = name_convention_check[name_convention_check['_merge'] == 'left_only']
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())
name_convention_check = pd.merge(ClipperPortsNotMapped,TargoPortNameColumn_lower, how='outer', on = 'Ports',indicator=True)
abc = name_convention_check[name_convention_check['_merge'] == 'left_only']
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())
TargoTempCheck = ClipperPortsNotMapped.drop(columns='_merge')
TargoTempCheck = ClipperPortsNotMapped.drop('_merge')
ClipperPortsNotMapped
TargoTempCheck = ClipperPortsNotMapped['Ports']
TargoTempCheck = ClipperPortsNotMapped['Ports']
name_convention_check = pd.merge(TargoTempCheck,TargoPortNameColumn_lower, how='outer',indicator=True)
abc = name_convention_check[name_convention_check['_merge'] == 'left_only']
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())

TargoTempCheck = pd.DataFrame(ClipperPortsNotMapped['Ports'])
name_convention_check = pd.merge(TargoTempCheck,TargoPortNameColumn_lower, how='outer',indicator=True)
abc = name_convention_check[name_convention_check['_merge'] == 'left_only']
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheetname='Data')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
import pandas as pd

ClipperRawData = pd.read_csv("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheet='Data')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
import pandas as pd

ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheet='Data')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values


### get the unique port names that we need as a pandas data series
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName'].unique()).dropna()
PortMappingsPorts = PortMappingsPorts.rename(columns={0:'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())

### merge both to see which is in which list
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)
ClipperPortsNotMapped = check[check['_merge'] == 'left_only']
TargoPortsNotMapped = check[check['_merge'] == 'right_only']

### This is the targo port name column
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())


### Then check to see if the ports we think are missing from above are in TargoPortName, returns completely missing ports
TargoTempCheck = pd.DataFrame(ClipperPortsNotMapped['Ports'])
name_convention_check = pd.merge(TargoTempCheck,TargoPortNameColumn_lower, how='outer',indicator=True)
NotAnywhere = name_convention_check[name_convention_check['_merge'] == 'left_only']
targo_mappings = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\Targo Ports - Products V1.0.xlsx', sheetname='Targo Products')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
gasoline_tracking = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ProductsClipperInput\GASTRACKER.xlsx", sheet='Tracker')
import pandas as pd
targo_mappings = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\Targo Ports - Products V1.0.xlsx', sheetname='Targo Products')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
gas_tracking = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ProductsClipperInput\GASTRACKER.xlsx", sheet='Tracker')
targo_mappings = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\Targo Ports - Products V1.0.xlsx', sheetname='Targo Products')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
gas_tracking = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ProductsClipperInput\GASTRACKER.xlsx", sheetname='Tracker')
import pandas as pd

# =============================================================================
# targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
#                                   sheetname='REFERENCE')
# =============================================================================



targo_mappings = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\Targo Ports - Products V1.0.xlsx', sheetname='Targo Products')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')
gas_tracking = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\ProductsClipperInput\GASTRACKER.xlsx", sheetname='Tracker')
targo_references = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx", sheetname='REFERENCE')

load_discharge = targo_references['Load Port Verification'].dropna()
counterparty = targo_references['Equity'].dropna()
grade = targo_references['Grades'].dropna()
port_ref = targo_references['Load Port Verification'].dropna()
counterparty_ref = targo_references['Equity'].dropna()
grade_ref = targo_references['Grades'].dropna()
port_ref = pd.DataFrame(targo_references['Load Port Verification'].dropna()).unique()
port_ref = pd.DataFrame(targo_references['Load Port Verification'].unique()).dropna()
port_ref = pd.DataFrame(targo_references['Load Port Verification'].unique()).dropna()
port_ref = port_ref.rename(columns={0:'Ports'})
port_ref = pd.DataFrame(port_ref['Ports'].str.lower())

counterparty_ref = pd.DataFrame(targo_references['Equity'].unique()).dropna()
counterparty_ref = counterparty_ref.rename(columns={0:'Equity'})
counterparty_ref = pd.DataFrame(counterparty_ref['Equity'].str.lower())

grade_ref = pd.DataFrame(targo_references['Grades'].unique()).dropna()
grade_ref = grade_ref.rename(columns={0:'Grades'})
grade_ref = pd.DataFrame(grade_ref['Grades'].str.lower())
chrts = pd.DataFrame(gas_tracking['CHRTS'].unique()).dropna()
chrts = chrts.rename(columns={0:'Equity'})
chrts = pd.DataFrame(chrts['Equity'].str.lower())
chrts = pd.DataFrame(gas_tracking['CHRTS'].unique()).dropna()
chrts = pd.DataFrame(gas_tracking['CHRTS']).dropna()
chrts = pd.DataFrame((gas_tracking['CHRTS']).dropna().unique())
gas_tracking['CHRTS']
(gas_tracking['CHRTS']).dropna()
chrts = pd.DataFrame((gas_tracking['CHRTS'].astype(str)).dropna().unique())
chrts = pd.DataFrame((gas_tracking['CHRTS'].astype(str)).dropna().unique())
chrts = chrts.rename(columns={0:'Equity'})
chrts = pd.DataFrame(chrts['Equity'].str.lower())
gas_tracking['CHRTS'].rstrip()).dropna().unique()
chrts = pd.DataFrame((gas_tracking['CHRTS'].rstrip().dropna().unique())
gas_tracking['CHRTS'].rstrip()
gas_tracking['CHRTS'].str.rstrip()
gas_tracking['CHRTS'].str.rstrip().dropna().unique()
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna().unique())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna().unique())
chrts = chrts.rename(columns={0:'Equity'})
chrts = pd.DataFrame(chrts['Equity'].str.lower())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna())
chrts = chrts.rename(columns={0:'Equity'})
chrts = pd.DataFrame(chrts['Equity'].str.lower().unique())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().unique().dropna())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna().unique())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna().unique())
chrts = chrts.rename(columns={0:'Equity'})
chrts = pd.DataFrame(chrts['Equity'].str.lower().unique())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna().unique())
chrts = pd.DataFrame(chrts[0].str.lower().unique())
chrts = chrts.rename(columns={0:'Equity'})
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna().unique())
chrts = pd.DataFrame(chrts[0].str.lower().unique())
chrts = chrts.rename(columns={0:'Equity'})

chrt_check = pd.merge(counterparty_ref,chrts, how='outer', indicator=True)
#ClipperPortsNotMapped = chrt_check[chrt_check['_merge'] == 'left_only']
NotInCHRTCheck = chrt_check[chrt_check['_merge'] == 'right_only']
targo_references = pd.read_excel("L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx", sheetname='REFERENCE')
port_ref = pd.DataFrame(targo_references['Load Port Verification'].unique()).dropna()
port_ref = port_ref.rename(columns={0:'Ports'})
port_ref = pd.DataFrame(port_ref['Ports'].str.lower())

counterparty_ref = pd.DataFrame(targo_references['Equity'].unique()).dropna()
counterparty_ref = counterparty_ref.rename(columns={0:'Equity'})
counterparty_ref = pd.DataFrame(counterparty_ref['Equity'].str.lower())

grade_ref = pd.DataFrame(targo_references['Grades'].unique()).dropna()
grade_ref = grade_ref.rename(columns={0:'Grades'})
grade_ref = pd.DataFrame(grade_ref['Grades'].str.lower())
chrts = pd.DataFrame(gas_tracking['CHRTS'].str.rstrip().dropna().unique())
chrts = pd.DataFrame(chrts[0].str.lower().unique())
chrts = chrts.rename(columns={0:'Equity'})

chrt_check = pd.merge(counterparty_ref,chrts, how='outer', indicator=True)
#ClipperPortsNotMapped = chrt_check[chrt_check['_merge'] == 'left_only']
NotInCHRTCheck = chrt_check[chrt_check['_merge'] == 'right_only']
LoadPorts = pd.Series(gas_tracking['LOAD'].str.rstrip().str.lower().dropna().unique())
LoadPorts = pd.Series(gas_tracking['LOAD'].str.rstrip().str.lower().dropna().unique())
DischargePorts = pd.Series(gas_tracking['DISCHARGE'].str.rstrip().str.lower().dropna().unique())
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
LoadPorts = pd.Series(gas_tracking['LOAD'].str.rstrip().str.lower().dropna().unique())
DischargePorts = pd.Series(gas_tracking['DISCH'].str.rstrip().str.lower().dropna().unique())
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
LoadPorts = pd.Series(gas_tracking['LOAD'].str.rstrip().str.lower().dropna().unique())
DischargePorts = pd.Series(gas_tracking['DISCH'].str.rstrip().str.lower().dropna().unique())
ports=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
ports= ports.rename(columns={0:'Ports'})

port_check = pd.merge(port_ref,ports, how='outer', indicator=True)
port_check = pd.merge(port_ref,ports, how='outer', indicator=True)
NotInPORTCheck = port_check[port_check['_merge'] == 'right_only']
LoadPorts = pd.Series(gas_tracking['LOAD'].str.rstrip().str.lower().dropna().unique())
DischargePorts = pd.Series(gas_tracking['DISCH'].str.rstrip().str.lower().dropna().unique())
ports=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
ports= ports.rename(columns={0:'Ports'})
port_ref = pd.DataFrame(targo_references['Load Port Verification'].unique()).dropna()
port_ref = port_ref.rename(columns={0:'Ports'})
port_ref = pd.DataFrame(port_ref['Ports'].str.lower())
LoadPorts = pd.Series(gas_tracking['LOAD'].str.rstrip().str.lower().dropna().unique())
DischargePorts = pd.Series(gas_tracking['DISCH'].str.rstrip().str.lower().dropna().unique())
ports=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
ports= ports.rename(columns={0:'Ports'})

port_check = pd.merge(port_ref,ports, how='outer', indicator=True)
NotInPORTCheck = port_check[port_check['_merge'] == 'right_only']
targo_mappings_port_list = pd.DataFrame(targo_mappings['Port'].str.rstrip().dropna().unique())
targo_mappings_port_list = pd.DataFrame(targo_mappings['Port'].str.rstrip().dropna().unique()).rename(columns={0:'Ports'})
targo_mappings_port_list = pd.DataFrame(targo_mappings['Port'].str.rstrip().dropna().unique()).rename(columns={0:'Ports'})
product_port_check = pd.merge(port_ref,targo_mappings_port_list, how='outer', indicator=True)
NotInPORTCheckVSTargoPortList = product_port_check[product_port_check['_merge'] == 'right_only']
targo_mappings_port_list = pd.DataFrame(targo_mappings['Port'].str.rstrip().str.lower().dropna().unique()).rename(columns={0:'Ports'})
product_port_check = pd.merge(port_ref,targo_mappings_port_list, how='outer', indicator=True)
NotInPORTCheckVSTargoPortList = product_port_check[product_port_check['_merge'] == 'right_only']
import pandas as pd

ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheet='Data')
PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapping')

print(ClipperRawData.dtypes)

### give me the unique column headers
ClipperRawData_headers = ClipperRawData.columns.values
PortMappings_headers = PortMappings.columns.values


### get the unique port names that we need as a pandas data series
LoadPorts = pd.Series(ClipperRawData['load_port'].unique())
DischargePorts = pd.Series(ClipperRawData['offtake_port'].unique())
allunique=pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
allunique= allunique.rename(columns={0:'Ports'})
allunique_lower = pd.DataFrame(allunique['Ports'].str.lower())

### turn the original port mappinngs into a dataframe for comparison, rename for merging and make sure and lower case for check
PortMappingsPorts = pd.DataFrame(PortMappings['ClipperPortName'].unique()).dropna()
PortMappingsPorts = PortMappingsPorts.rename(columns={0:'Ports'})
PortMappingsPorts_lower = pd.DataFrame(PortMappingsPorts['Ports'].str.lower())

### merge both to see which is in which list
check = pd.merge(allunique_lower,PortMappingsPorts_lower, how='outer', indicator=True)
ClipperPortsNotMapped = check[check['_merge'] == 'left_only']
TargoPortsNotMapped = check[check['_merge'] == 'right_only']

### This is the targo port name column
TargoPortNameColumn = pd.DataFrame(PortMappings['TargoPortName'].unique()).dropna()
TargoPortNameColumn = TargoPortNameColumn.rename(columns={0:'Ports'})
TargoPortNameColumn_lower = pd.DataFrame(TargoPortNameColumn['Ports'].str.lower())


### Then check to see if the ports we think are missing from above are in TargoPortName, returns completely missing ports
TargoTempCheck = pd.DataFrame(ClipperPortsNotMapped['Ports'])
name_convention_check = pd.merge(TargoTempCheck,TargoPortNameColumn_lower, how='outer',indicator=True)
NotAnywhere = name_convention_check[name_convention_check['_merge'] == 'left_only']
import pandas as pd

# =============================================================================
# targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
#                                   sheetname='REFERENCE')
# =============================================================================

PortMappings = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Mapped Ports')
AllTargoPorts = pd.read_excel("L:\TRADING\Targo\Mapping.xlsx", sheet = 'Targo Ports')
import pandas as pd

# =============================================================================
# targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
#                                   sheetname='REFERENCE')
# =============================================================================

PortMappings = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheet = 'Mapped Ports')
AllTargoPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheet = 'Targo Ports')
import pandas as pd

# =============================================================================
# targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
#                                   sheetname='REFERENCE')
# =============================================================================

PortMappings = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheet = 'Mapped Ports')
AllTargoPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheet = 'TARGO Ports')
import pandas as pd

# =============================================================================
# targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
#                                   sheetname='REFERENCE')
# =============================================================================

PortMappings = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'Mapped Ports')
AllTargoPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'TARGO Ports')
ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheetname='Data')
LoadPorts = pd.DataFrame(ClipperRawData['load_port'].str.lower().dropna().unique())
LoadPorts = pd.DataFrame(ClipperRawData['load_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
LoadPorts = pd.DataFrame(ClipperRawData['load_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
DischargePorts = pd.DataFrame(ClipperRawData['offtake_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
AllClipperPorts = pd.DataFrame((LoadPorts.append(DischargePorts)).unique())
AllClipperPorts = pd.DataFrame(LoadPorts.append(DischargePorts))
AllClipperPorts = pd.DataFrame(LoadPorts.append(DischargePorts)).drop_duplicates()
CurrentClipperVsMappedPorts = pd.merge(PortMappings['Location'].str.lower().rename(columns={0:'Ports'})
                                            ,AllClipperPorts, how='outer', indicator=True)
CurrentClipperVsMappedPorts = pd.merge(pd.DataFrame(PortMappings['Location'].str.lower().rename(columns={0:'Ports'}))
                                            ,AllClipperPorts, how='outer', indicator=True)
CurrentClipperVsMappedPorts = pd.merge(pd.DataFrame(PortMappings['Location'].str.lower()).rename(columns={0:'Ports'})
                                            ,AllClipperPorts, how='outer', indicator=True)
pd.DataFrame(PortMappings['Location'].str.lower()).rename(columns={0:'Ports'})
CurrentClipperVsMappedPorts = pd.merge((pd.DataFrame(PortMappings['Location'].str.lower()).rename(columns={0:'Ports'}))
                                            ,AllClipperPorts, how='outer', indicator=True)
MappedPorts = pd.DataFrame(PortMappings['Location'].str.lower()).rename(columns={0:'Ports'})
LoadPorts = pd.DataFrame(ClipperRawData['load_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
DischargePorts = pd.DataFrame(ClipperRawData['offtake_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
AllClipperPorts = pd.DataFrame(LoadPorts.append(DischargePorts)).drop_duplicates()
MappedPorts = pd.DataFrame(PortMappings['Location'].str.lower()).rename(columns={'Location':'Ports'})

CurrentClipperVsMappedPorts = pd.merge(MappedPorts,AllClipperPorts, how='outer', indicator=True)
LoadPorts = pd.DataFrame(ClipperRawData['load_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
DischargePorts = pd.DataFrame(ClipperRawData['offtake_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
AllClipperPorts = pd.DataFrame(LoadPorts.append(DischargePorts)).drop_duplicates()
MappedPorts = pd.DataFrame(PortMappings['Location'].str.lower()).rename(columns={'Location':'Ports'})

CurrentClipperVsMappedPorts = pd.merge(MappedPorts,AllClipperPorts, how='outer', indicator=True)

InMapped = CurrentClipperVsMappedPorts[CurrentClipperVsMappedPorts['_merge'] == 'left_only']
AllClipperPorts = CurrentClipperVsMappedPorts[CurrentClipperVsMappedPorts['_merge'] == 'right_only']
LoadPorts = pd.DataFrame(ClipperRawData['load_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
DischargePorts = pd.DataFrame(ClipperRawData['offtake_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
AllClipperPorts = pd.DataFrame(LoadPorts.append(DischargePorts)).drop_duplicates()
MappedPorts = pd.DataFrame(PortMappings['Location'].str.lower()).rename(columns={'Location':'Ports'})

CurrentClipperVsMappedPorts = pd.merge(MappedPorts,AllClipperPorts, how='outer', indicator=True)

InMapped = CurrentClipperVsMappedPorts[CurrentClipperVsMappedPorts['_merge'] == 'left_only']
InClipper = CurrentClipperVsMappedPorts[CurrentClipperVsMappedPorts['_merge'] == 'right_only']
TargoPorts = pd.DataFrame(AllTargoPorts['Name'].str.lower()).rename(columns={'Name':'Ports'})
CurrentClipperVsAllTargoPorts = pd.merge(TargoPorts,AllClipperPorts, how='outer', indicator=True)
InTargo = CurrentClipperVsAllTargoPorts[CurrentClipperVsAllTargoPorts['_merge'] == 'left_only']
InClipperVsTargo = CurrentClipperVsAllTargoPorts[CurrentClipperVsAllTargoPorts['_merge'] == 'right_only']
CheckVsTargoList = pd.merge(pd.DataFrame(AllTargoPorts['Name'].str.lower()).rename(columns={'Name':'Ports'}),
                            pd.DataFrame(InClipper['Ports']), how='outer', indicator=True)
CheckVsTargoList = pd.merge(pd.DataFrame(AllTargoPorts['Name'].str.lower()).rename(columns={'Name':'Ports'}),
                            pd.DataFrame(InClipper['Ports']), how='outer', indicator=True)
CheckVsTargoList = CheckVsTargoList[CheckVsTargoList['_merge'] == 'left_only']
CheckVsTargoList = pd.merge(pd.DataFrame(AllTargoPorts['Name'].str.lower()).rename(columns={'Name':'Ports'}),
                            pd.DataFrame(InClipper['Ports']), how='outer', indicator=True)
CheckVsTargoList = CheckVsTargoList[CheckVsTargoList['_merge'] == 'right_only']

## ---(Tue Jan  2 11:47:44 2018)---
PortMappings = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'Mapped Ports')
AllTargoPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'TARGO Ports, Regions, Countries')
ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheetname='Data')
import pandas as pd

# =============================================================================
# targo_references = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload PRODUCT TEST.xlsx', 
#                                   sheetname='REFERENCE')
# =============================================================================

PortMappings = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'Mapped Ports')
AllTargoPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'TARGO Ports, Regions, Countries')
ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheetname='Data')
LoadPorts = pd.DataFrame(ClipperRawData['load_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
DischargePorts = pd.DataFrame(ClipperRawData['offtake_port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
AllClipperPorts = pd.DataFrame(LoadPorts.append(DischargePorts)).drop_duplicates()
MappedPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'Mapped Ports')
Mapped_Ports = pd.DataFrame(MappedPorts['ExternalValue'].str.lower()).rename(columns={'ExternalValue':'Ports'})
CurrentClipperVsMappedPorts = pd.merge(Mapped_Ports,AllClipperPorts, how='outer', indicator=True)
InMapped = CurrentClipperVsMappedPorts[CurrentClipperVsMappedPorts['_merge'] == 'left_only']
InClipper = CurrentClipperVsMappedPorts[CurrentClipperVsMappedPorts['_merge'] == 'right_only']
TargoLloydsPorts = pd.DataFrame(AllTargoPorts['name'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
TargoLloydsPorts = pd.DataFrame(AllTargoPorts['Name'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
MappedPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'Mapped Ports')
AllTargoPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'TARGO Ports, Regions, Countries')
InClipperList = InClipper['Ports']
InClipperList = pd.DataFrame(InClipper['Ports'])
TargoLloydsPorts = pd.DataFrame(AllTargoPorts['Name'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
InClipperVSLloydsList = pd.merge(InClipperList,TargoLloydsPorts, how='outer', indicator=True)
StillToBeMapped = InClipperVSLloydsList[InClipperVSLloydsList['_merge'] == 'left_only']
StillToBeMapped = InClipperVSLloydsList[InClipperVSLloydsList['_merge'] != 'both']
DirectClipperPorts = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'Direct Clipper Ports')
DirectClipperPortsList = pd.DataFrame(DirectClipperPorts['Port'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
XChckStillVsDircet = pd.merge(StillToBeMapped,DirectClipperPortsList, how='outer', indicator=True)
StillToBeMapped = InClipperVSLloydsList[InClipperVSLloydsList['_merge'] == 'left_only']
XChckStillVsDircet = pd.merge(pd.DataFrame(StillToBeMapped['Ports']),DirectClipperPortsList, how='outer', indicator=True)
UnmappedToSubregion = pd.read_excel("L:\TRADING\Targo\TargoMappingView.xlsx", sheetname = 'UNMAPPED TO SUBREGION')
UnmappedToSubregionList = pd.DataFrame(UnmappedToSubregion['Name'].str.lower().dropna().unique()).rename(columns={0:'Ports'})
XChckStillvsUnmapped =  pd.merge(pd.DataFrame(StillToBeMapped['Ports']),UnmappedToSubregionList, how='outer', indicator=True)
XChckStillvsUnmappedLeft = XChckStillvsUnmapped[XChckStillvsUnmapped['_merge'] == 'left_only']
XChckStillvsUnmappedBoth = XChckStillvsUnmapped[XChckStillvsUnmapped['_merge'] == 'both']
import pandas as pd

PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < lat+limit) &
                           (PortCoordsTEST['Latitude'] > lat-limit) &
                           (PortCoordsTEST['Longitude'] < lon+limit) &
                           (PortCoordsTEST['Longitude'] > lon-limit)
   return y

def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < lat+limit) &
                           (PortCoordsTEST['Latitude'] > lat-limit) &
                           (PortCoordsTEST['Longitude'] < lon+limit) &
                           (PortCoordsTEST['Longitude'] > lon-limit)
    return y

def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsTEST.loc[(PortCoordsTEST['Latitude'] < lat+limit) &
                           (PortCoordsTEST['Latitude'] > lat-limit) &
                           (PortCoordsTEST['Longitude'] < lon+limit) &
                           (PortCoordsTEST['Longitude'] > lon-limit)]
    return y

find_similar_ports(57,9,2)
def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsList.loc[(PortCoordsList['Latitude'] < lat+limit) &
                           (PortCoordsList['Latitude'] > lat-limit) &
                           (PortCoordsList['Longitude'] < lon+limit) &
                           (PortCoordsList['Longitude'] > lon-limit)]
    return y


find_similar_ports(57,9,2)
def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsList.loc[(PortCoordsList['Lat'] < lat+limit) &
                           (PortCoordsList['Lat'] > lat-limit) &
                           (PortCoordsList['Lon'] < lon+limit) &
                           (PortCoordsList['Lon'] > lon-limit)]
    return y


find_similar_ports(57,9,2)
def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsList.loc[(PortCoordsList['Lat'] < lat+limit) &
                           (PortCoordsList['Lat'] > lat-limit) &
                           (PortCoordsList['Long'] < lon+limit) &
                           (PortCoordsList['Long'] > lon-limit)]
    return y


find_similar_ports(57,9,2)
find_similar_ports(57,9,1)
def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsList.loc[(PortCoordsList['Lat'] < lat+limit) &
                           (PortCoordsList['Lat'] > lat-limit) &
                           (PortCoordsList['Long'] < lon+limit) &
                           (PortCoordsList['Long'] > lon-limit)]
    y = pd.DataFrame(y)
    return y


find_similar_ports(57,9,1)
import pandas as pd

PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")


def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsList.loc[(PortCoordsList['Lat'] < lat+limit) &
                           (PortCoordsList['Lat'] > lat-limit) &
                           (PortCoordsList['Long'] < lon+limit) &
                           (PortCoordsList['Long'] > lon-limit)]
    y = pd.DataFrame(y)
    return y


SimilarPorts = find_similar_ports(57,9,1)
SimilarPorts = find_similar_ports(57,9,0.5)
def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsList.loc[(PortCoordsList['Lat'] < lat+limit) &
                           (PortCoordsList['Lat'] > lat-limit) &
                           (PortCoordsList['Long'] < lon+limit) &
                           (PortCoordsList['Long'] > lon-limit)]
    #y = pd.DataFrame(y)
    return y

SimilarPorts = find_similar_ports(57,9,0.5)
SimilarPorts = find_similar_ports(57.0469,9.9366,0.2)
import pandas as pd

PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")


def find_similar_ports(lat,lon,limit):
    PortCoordsList = pd.read_excel('L:\TRADING\Targo\TargoMappingView.xlsx', sheetname="ALl Ports")
    y = PortCoordsList.loc[(PortCoordsList['Lat'] < lat+limit) &
                           (PortCoordsList['Lat'] > lat-limit) &
                           (PortCoordsList['Long'] < lon+limit) &
                           (PortCoordsList['Long'] > lon-limit)]
    y = pd.DataFrame(y)
    return y



### Enter the port coordinates below

### (lat, lon, +-limit)

SimilarPorts = find_similar_ports(57.6667,10.4167,0.2)

## ---(Fri Jan  5 17:03:25 2018)---
import eia
api = eia.API(1737754fd87b25da81a3ebbd5e3a3267)
api = eia.API('1737754fd87b25da81a3ebbd5e3a3267')
keyword_search = api.data_by_keyword(keyword=['crude oil', 'price'],
                                     filters_to_keep=['brent'],
                                     filters_to_remove=['AEO2015'],
                                     rows=1000)

for key,value in keyword_search.items():
    print(key,value)

mport pandas as pd

ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheet='Data')
import pandas as pd

ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheet='Data')
import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)
"""

import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)
import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)

## ---(Tue Jan  9 10:51:02 2018)---
import pandas as pd

DOEAPIDATA = pd.read_excel("L:\TRADING\ANALYSIS\BALANCES\US Balance v5.xlsm",
                        sheet_name="DOE_API_W", header=1, index_col = 1)
import pandas as pd

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v5.xlsm",
                        sheet_name="DOE_API_W", header=1, index_col = 1)
print(DOEAPIDATA)
import pandas as pd

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v5.xlsm",
                        sheetname="DOE_API_W", header=1, index_col = 1)
print(DOEAPIDATA)
import pandas as pd

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v5.xlsm",
                        sheetname="DOE_API_W", header=1, index_col = 1, skiprows = 1)
print(DOEAPIDATA)
import pandas as pd

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v5.xlsm",
                        sheetname="DOE_API_W", header=1, index_col = 1, skiprows = 2)

print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1, skiprows = 1)

print(DOEAPIDATA)
print(type(DOEAPIDATA))
%clear
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1)

print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1, skiprows=1)

print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1, skiprows=0)

print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1)

print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop(1)


print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop([0])


print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop([0], axis=0)


print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop([0,0])


print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop('Timestamp')


print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop('Timestamp').drop('Retrieving...', axis=1)




print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//EVERYTHING.xlsx",
                        sheetname=0, header=0)
test_path='L:/TRADING/ANALYSIS/Python/MIMA/RefineryDatabaseChecks/'
#ref_data = pd.read_excel("M://test_data_for_ref_resample.xlsx", sheetname=0, header=0)
#ref_data['Start_Date'].apply(lambda x : x.date() in ref_data['Start_Date'])
ref_data = ref_data[(ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2')]
pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)
pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')
for index, row in ref_data.iterrows():
    end_date = row[5]

for index, row in ref_data.iterrows():
    end_date = row[5]
    print(row)
    print(index)

for index, row in ref_data[20].iterrows():
    end_date = row[5]
    print(row)
    print(index)

ref_data.iterrows()
for index, row in ref_data.iterrows():

for index, row in ref_data.iterrows():
    #end_date = row[5]
    print(row)

for index, row in ref_data.iterrows():
    #end_date = row[5]
    #print(row)
    print(index)

for row in ref_data.iterrows():
    end_date = row[5]
    #print(row)
    start_date = row[4]
    name = row[1]
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]
    #ref_test_df[name] = row['CAP_OFFLINE']
    #ref_sum_df = ref_test_df.sum(axis=1).to_frame(name)

for index, row in ref_data.iterrows():
    end_date = row[5]
    #print(row)
    start_date = row[4]
    name = row[1]
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]
    #ref_test_df[name] = row['CAP_OFFLINE']

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

ref_data = pd.read_excel("L://TRADING//ANALYSIS//REFINERY DATABASE//Latest ref sheets//Latest//EVERYTHING.xlsx",
                        sheetname=0, header=0)
test_path='L:/TRADING/ANALYSIS/Python/MIMA/RefineryDatabaseChecks/'
#ref_data = pd.read_excel("M://test_data_for_ref_resample.xlsx", sheetname=0, header=0)
#ref_data['Start_Date'].apply(lambda x : x.date() in ref_data['Start_Date'])


# =============================================================================
### slice maintenance list to include only the crude and the condensate units that we are interested in
# =============================================================================

ref_data = ref_data[(ref_data['UNIT_NAME'] == 'CRUDE #1')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #2')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #3')|
                     (ref_data['UNIT_NAME'] == 'CRUDE #4')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #1')|
                     (ref_data['UNIT_NAME'] == 'CONDENSATE DISTILLATION #2')]

#ref_data = ref_data[(ref_data['UNIT_NAME'] == 'CRUDE #1')]

### Set start and end dates for the entire dataframe
pd_index_start_date = datetime(1900, 1, 1)
pd_index_end_date = datetime(2030, 12, 1)


### define the length of the dataframe 
#total_time_period = pd_index_end_date - pd_index_start_date


### create the index by creating a list of days from the start date, 
### in days, for the duration of the time period detrmined above and 
### add one due to zero indexing
#pd_index_days_list=[pd_index_start_date + timedelta(days=x) 
#        for x in range(total_time_period.days +1)]

###ORRRRRR.......

# =============================================================================
### create a list of dates, daily, between the start and end dates
# =============================================================================

pd_index_days_list = pd.date_range(pd_index_start_date, pd_index_end_date, 
                                   freq = 'D')


# =============================================================================
### create a list of all individual refineries for column headers
# =============================================================================

ref_list=pd.Series(ref_data['REFINERY'].unique())



# =============================================================================
### create the dataframe with the col headers and index determined above
# =============================================================================

maintenance_framework = pd.DataFrame(0, index= pd_index_days_list, columns=ref_list)


# =============================================================================
### for each row in the database (each row represents a maintenance outage)
### take the start and end dates to get duration from cols 4 and 5
### get the name from column 1, then create a date range for that maintenance 
### 
# =============================================================================

for index, row in ref_data.iterrows():
    end_date = row[5]
    #print(row)
    start_date = row[4]
    name = row[1]
    #num_days_offline = end_date - start_date
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    ref_test_df = pd.DataFrame(row['CAP_OFFLINE'], index=date_range, columns= [row['REFINERY']])
    maintenance_framework.loc[ref_test_df.index,name]=maintenance_framework.loc[ref_test_df.index,name]+ref_test_df.loc[ref_test_df.index,name]
    #ref_test_df[name] = row['CAP_OFFLINE']

import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)


### calculate decade
d = df.index.day -1 - np.clip((df.index.day-1) // 10, 0, 2)*10
print(d)
date = df.index.values - np.array(d, dtype="timedelta64[D]")
f.groupby(date).mean()
df.groupby(date).mean()
DOEAPIDATA.index.day
import pandas as pd

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop('Timestamp').drop('Retrieving...', axis=1)
DOEAPIDATA.index.day
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col = 1).drop('Timestamp').drop('Retrieving...', axis=1)

DOEAPIDATA.index.day
DOEAPIDATA.index
maintenance_framework.index
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index = 1).drop('Timestamp').drop('Retrieving...', axis=1)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index = 1).drop('Retrieving...', axis=1)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index = 1).drop('Retrieving...', axis=1)




print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index = 0).drop('Retrieving...', axis=1)




print(DOEAPIDATA)
print(type(DOEAPIDATA))
%clear
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index = 0).drop('Retrieving...', axis=1)




print(DOEAPIDATA)
print(type(DOEAPIDATA))
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1).drop('Retrieving...', axis=1)

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1).drop('Retrieving...', axis=1)




print(DOEAPIDATA)
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1).drop('Retrieving...', axis=1)




print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =0).drop('Retrieving...', axis=1)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =0)




print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =1)




print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =1).drop(columns=0)




print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =1).drop(columns=[0])




print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =1)




print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =1).drop(['Timestamp'])




print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.index.day
import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)
df.index.day
DOEAPIDATA = pd.DataFrame(DOEAPIDATA)
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1, index_col =1).drop(['Timestamp'])
DOEAPIDATA = pd.DataFrame(DOEAPIDATA, index=0)
DOEAPIDATA.index.day
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header=1)


#DOEAPIDATA = pd.DataFrame(DOEAPIDATA, index=0)



print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
                        sheetname="DOE_API_W", header=1)
DOEAPIDATA.loc[:,1]
DOEAPIDATA.loc[:,1]
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W")
DOEAPIDATA
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)
DOEAPIDATA
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)



#DOEAPIDATA = pd.DataFrame(DOEAPIDATA, index=0)



print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Timestamp'])

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Timestamp'], axis=0)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop[0]

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
@author: mima
"""

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'])

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'], axis = 1)

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'], axis = 1)
DOEAPIDATA.set_index(DOEAPIDATA['Unnamed: 1'])

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)

df.describe()
df.dtypes()
df.dtypes
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'], axis = 1)


print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtype
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'], axis = 1)


print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = DOEAPIDATA.convert_objcts(convert_numeric=True)


print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
@author: mima
"""

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = DOEAPIDATA.convert_objects(convert_numeric=True)


print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = DOEAPIDATA.convert_objects(convert_numeric=True)


print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
DOEAPIDATA.index.day
DOEAPIDATA
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop([0]).drop(['Retrieving...'], axis = 1)
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1)
DOEAPIDATA
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = DOEAPIDATA.convert_objects(convert_numeric=True)


print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = pd.to_numeric(DOEAPIDATA)
DOEAPIDATA = DOEAPIDATA.convert_objects(convert_numeric=True)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = DOEAPIDATA.convert_objects(convert_numeric=True)


print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA.dtypes
DOEAPIDATA.index.day
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1)
DOEAPIDATA
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)
DOEAPIDATA
DOEAPIDATA.index.day
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA.index.day
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
d
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA.apply(pd.to_numeric)

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
#DOEAPIDATA = pd.DataFrame(DOEAPIDATA, index=0)






d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date).mean()
DOEAPIDATA.apply(partial(pd.to_numeric, errors='ignore')).info()
DOEAPIDATA.apply(pd.to_numeric, errors='ignore').info()
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA.apply(pd.to_numeric, errors='ignore').info()

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
#DOEAPIDATA = pd.DataFrame(DOEAPIDATA, index=0)






d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1, dtype=float).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = DOEAPIDATA.astype(float)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = DOEAPIDATA.astype(float).info()
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = DOEAPIDATA.astype(float).info()
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)
DOEAPIDATA = DOEAPIDATA.astype(float).info()
DOEAPIDATA
print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)
DOEAPIDATA.astype(float).info()
DOEAPIDATA
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA.index.day
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date).mean()
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date1 = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date1).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, index_col = 1, dtype=float).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, dtype=float).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0, dtype=float)
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)
DOEAPIDATA
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Timestamp']).drop(['Retrieving...'], axis = 1)
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)
DOEAPIDATA
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Retrieving...'], axis = 1).drop(['Timestamp'])
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Retrieving...'], axis = 1)
DOEAPIDATA
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Retrieving...'], axis = 1)

DOEAPIDATA = DOEAPIDATA.astype(float).info()
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Retrieving...'], axis = 1)
DOEAPIDATA
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Retrieving...'], axis = 1).drop([0])
DOEAPIDATA
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(['Retrieving...'], axis = 1).drop([0])
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)
DOEAPIDATA
print(DOEAPIDATA.head())
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0], axis = 1).drop([0])
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop(0, axis = 1).drop([0])
DOEAPIDATA
print(DOEAPIDATA.head())
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])
print(DOEAPIDATA.head())
DOEAPIDATA.columns[1]
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(DOEAPIDATA.columns[0])
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)
DOEAPIDATA
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])
print(DOEAPIDATA.head())
DOEAPIDATA.drop(DOEAPIDATA.columns[0])
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA
DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
print(DOEAPIDATA.head())
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date1 = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date1).mean()
DOEAPIDATA.dtypes
DOEAPIDATA.apply(pd.to_numeric, errors='ignore').info()
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date1 = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date1).mean()
print(DOEAPIDATA.head())
DOEAPIDATA.dtypes
DOEAPIDATA = DOEAPIDATA.apply(pd.to_numeric, errors='ignore').info()
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date1 = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date1).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).drop(DOEAPIDATA.columns[0], axis=1)

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)
DOEAPIDATA = DOEAPIDATA.drop([0]).drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = DOEAPIDATA.apply(pd.to_numeric, errors='ignore').info()
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])
DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)

df.dtypes

DOEAPIDATAnum = DOEAPIDATA.apply(pd.to_numeric).info()
d = DOEAPIDATAnum.index.day -1 - np.clip((DOEAPIDATAnum.index.day-1) // 10, 0, 2)*10
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date1 = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby().mean()
DOEAPIDATA.groupby(date1).mean()
print(date1)
DOEAPIDATA.index.day
import pandas as pd
import numpy as np

begin = pd.datetime(2013,1,1)
end = pd.datetime(2013,2,20)

dtrange = pd.date_range(begin, end)

p1 = np.random.rand(len(dtrange)) + 5
p2 = np.random.rand(len(dtrange)) + 10

df = pd.DataFrame({'p1': p1, 'p2': p2}, index=dtrange)

df.dtypes


### calculate decade
d = df.index.day -1 - np.clip((df.index.day-1) // 10, 0, 2)*10
date = df.index.values - np.array(d, dtype="timedelta64[D]")
df.groupby(date).mean()

print(d)

###  What lines 23-25 mean
### we created an index od dtranges bages betweendays as values
print(df.index.day)

### zero-indexing adjustment
print(df.index.day-1)

### divisor and rounded down to nearest whole for 1st, 2nd and 3rd
print((df.index.day-1) // 10)

### numpy's clip takes our array and clips to whole numbers betwen 0 and 2 for 1st, 2bd, 3rd
print(np.clip((df.index.day-1) // 10, 0, 2))
df.index.day
mike = DOEAPIDATA.resample('d').mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0]).set_index('Date')


DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)

DOEAPIDATA = DOEAPIDATA.set_index('Date')


DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)

DOEAPIDATAnum = DOEAPIDATA.apply(pd.to_numeric).info()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])
DOEAPIDATA = DOEAPIDATA.set_index('Date')
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATAnum = DOEAPIDATA.apply(pd.to_numeric, errors='coerce').info()
DOEAPIDATA.groupby(date1).mean()
DOEAPIDATA = DOEAPIDATA.dropna(axis=0, how = 'any')
DOEAPIDATAnum = DOEAPIDATA.apply(pd.to_numeric, errors='coerce').info()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])
DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
#DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
#DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)


#DOEAPIDATA = DOEAPIDATA.dropna(axis=0, how = 'any')
DOEAPIDATA = DOEAPIDATA.apply(pd.to_numeric, errors='coerce').info()
DOEAPIDATA.groupby(date1).mean()
DOEAPIDATA = pd.DataFrame(DOEAPIDATA)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
#DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
DOEAPIDATA.dtypes
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date1 = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
DOEAPIDATA.groupby(date1).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
#DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
d = DOEAPIDATA.index.day -1 - np.clip((DOEAPIDATA.index.day-1) // 10, 0, 2)*10
date1 = DOEAPIDATA.index.values - np.array(d, dtype="timedelta64[D]")
decade_DOEAPIDATA = DOEAPIDATA.groupby(date1).mean()
DOEAPIDATA.index.day
mike = DOEAPIDATA.resample('d').mean()
mike = DOEAPIDATA.resample('d').ffill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
print(date1)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
#DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)


#DOEAPIDATA = DOEAPIDATA.dropna(axis=0, how = 'any')
#DOEAPIDATA = DOEAPIDATA.apply(pd.to_numeric, errors='coerce').info()
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
#DOEAPIDATA = pd.DataFrame(DOEAPIDATA, index=0)



mike = DOEAPIDATA.resample('d').bfill()


d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("M://DOE TEST SHEET.xlsx",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v6.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v6.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    print(d)

df=pd.DataFrame(d)
df['Data'][0]
df['data'][0]
df['data'].iloc[0]
df['data'].iloc[:,0]
df['data'].loc[:,0]
df['data']
df['data'][2:4]
df['data'][2:9]
df['data'][:][0]
df['data']
x = df['data']
df['Date'] = [row[0] for row in df['data']]
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]


connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=EIAData; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO Series"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]


connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=EIAData; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO Series"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]


connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
Values.dtype
Values.dtype()
Values[0].dtype()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

df = pd.DataFrame(df).astype(float)


connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
df["last_updated"].dtype()
df.dtype()
df.dtype
df.dtypes
df["last_updated"].dtypes
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     
#
#cursor = connection.cursor()
#SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
#          "VALUES (?,?,1,?,?,?)")
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
#cursor.execute(SQLCommand,Values)
#connection.commit()
#connection.close()

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id) "
          "VALUES (?)")
Values = df["series_id"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")     

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = [1,1,1,1,1]
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")   

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = [1,1,1,1,1]
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")  
cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = [1,1,1,1,1]
cursor.execute(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")   

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = [1,1,1,1,1]
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")   

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

#Values = [1,1,1,1,1]
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
#
df["series_id"].tolist
df["series_id"].tolist()
Values = df["series_id"].tolist(), df["name"].tolist(), df["f"].tolist(), df["units"].tolist(), df["last_updated"].tolist()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")   

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

#Values = [1,1,1,1,1]
Values = df["series_id"].tolist(), df["name"].tolist(), df["f"].tolist(), df["units"].tolist(), df["last_updated"].tolist()
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")   

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

#Values = [1,1,1,1,1]
Values = [df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
Values
Values.head()
Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
Values.head()
type(Values)
Values
type(Values)
type(df["series_id"])
Values[0]
print(Values)
dfSQL = list(zip(df.Series_id, df.name))
dfSQL = list(zip(df.series_id, df.name))
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")   

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(series_id, Name, CategoryId, f, Units, updated) "
          "VALUES (?,?,1,?,?,?)")

#Values = [1,1,1,1,1]
Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]

#df.dtypes
#df = pd.DataFrame(df).astype(float)

#df["last_updated"].dtypes
#
#connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")   

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(Id, Name, CategoryId, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

#Values = [1,1,1,1,1]
Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated))[0] 
Values
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v6.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v7.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v7.xlsm",
                        sheetname="DOE_API_W", header =0)
DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v7.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])
DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance v7.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
df['Newdate'] = [row[0]+"01" for row in df['Date'] if len(df['Date']) <8 ]
with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row[0]+"01" for row in df['Date'] if len(df['Date']) <8 ]
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row[0]+"01" for row in df['Date'] if len(df['Date']) <8 ]
df['Newdate'] = [row[0]+"01" for row in df['Date']]
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['data']
df['Date']
import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

df=pd.DataFrame(d)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)
        df = pd.DataFrame(d)

from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)
        df = pd.DataFrame(d)

from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)
        df = pd.DataFrame(d)

from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)
        df = pd.DataFrame(d)

from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)
        df = pd.DataFrame(d)

f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r')
f
f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r').read()

## ---(Wed Jan 10 19:05:21 2018)---
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []

f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r')
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []

f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r').read()

## ---(Wed Jan 10 19:09:33 2018)---
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f

with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:

open('L:\\TRADING\SQLDataUpload\PET.txt') as f
open('L:\\TRADING\SQLDataUpload\PET.txt')
f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r')
f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r')
print(f)
f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r').read()
print(f)

## ---(Wed Jan 10 19:14:02 2018)---
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)
        df = pd.DataFrame(d)

i = 0
data = []

f = open('L:\\TRADING\SQLDataUpload\PET.txt', 'r')

with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)
        df = pd.DataFrame(d)

import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

import json 
import pandas as pd
import pyodbc

with open('L:\\TRADING\SQLDataUpload\PETTest.txt') as json_data:
    d = json.load(json_data)
    seriesd = d['data']
    print(d)

import json 
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r')
json_data
import json 
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
attempt = pd.read_json(d)
json_try = json_data.to_json()
json_data =  pd.DataFrame(open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read())
json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
json_data = pd.DataFrame(json_data)
import json 
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
json_try = json_data.to_json(json_data)
df = pd.read_json(json_data)
import json 
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
import json 
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]
import json 
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1].astype(float) for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]
row[1]
df.types
df.types()
df.type()
df.dtypes()
df.dtypes
import json 
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]

df.dtypes
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated))[0] 
df['Newdate'] = pd.to_datetime(df['Newdate'], format=%Y%m%d)
new_date = datetime.strptime(df['Newdate'], '%YYYY%mm%dd'
new_date = datetime.strptime(df['Newdate'], '%YYYY%mm%dd')
import json 
import pandas as pd
import pyodbc
from datetime import datetime

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]

new_date = datetime.strptime(df['Newdate'], '%YYYY%mm%dd')
mike = [datetime.strptime(row,'%YYYY%mm%dd') for row in df['Newdate']]
mike = [datetime.strptime(row,'%Y%m%d') for row in df['Newdate']]
mike = [datetime.strptime(row,'%Y%m%d').strftime('%d-%m-%Y') for row in df['Newdate']]
import json 
import pandas as pd
import pyodbc
from datetime import datetime

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]

### first we need to take the string and turn that into a datime object and then create a string from the datetime in our format
df['mike'] = [datetime.strptime(row,'%Y%m%d').strftime('%d-%m-%Y') for row in df['Newdate']]
df.columns
Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated))[0] 
df.dtypes
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated))[0] 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EIASeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
cursor.executemany(SQLCommand,Values)
connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)

connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise

df.dtypes
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, str(df.last_updated))) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units) "
          "VALUES (?,?,1,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()


df["last_updated"]
df["last_updated"].strftime('%d-%m-%Y')
mike1 = datetime.strptime(df["last_updated"],'%Y-%m-%d').strftime('%d-%m-%Y')
df["last_updated"].dtype
str(df["last_updated"]).dtype
str(df["last_updated"])
df["last_updated"][0:11]
mike1 = datetime.strptime('2018-01-04T14:44:31-05:00','%Y-%m-%d').strftime('%d-%m-%Y')
mike1
mike1 = datetime.strptime('2018-01-04T14:44:31-05:00','%Y-%m-%d').strftime('%d-%m-%Y').date()
mike1 = datetime.strptime('2018-01-04T14:44:31-05:00','%Y-%m-%d %H:%M:%S').strftime('%d-%m-%Y').date()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, str(df.last_updated)) )
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, str(df.last_updated)) )
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, str(df.last_updated.split('T')[0])) )
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()
connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, str(df.last_updated).split('T')[0])) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()

Values = list(zip(df.series_id, df.name, df.f, df.units, str(df.last_updated).split('T')[0])) 
Values
df['last_updated2'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') for row in df['last_updated']]
import json 
import pandas as pd
import pyodbc
from datetime import datetime
import dateutil.parser
df['last_updated2'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') for row in df['last_updated']]
import json 
import pandas as pd
import pyodbc
from datetime import datetime
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]

### first we need to take the string and turn that into a datime object and then create a string from the datetime in our format
df['mike'] = [datetime.strptime(row,'%Y%m%d').strftime('%d-%m-%Y') for row in df['Newdate']]
df['last_updated2'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') for row in df['last_updated']]
df.columns
mike1 = datetime.strptime('2018-01-04T14:44:31-05:00','%Y-%m-%d %H:%M:%S').strftime('%d-%m-%Y').date()



df.dtypes

df["last_updated"][0:11]

str(df["last_updated"])


connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated2)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()
import json 
import pandas as pd
import pyodbc
from datetime import datetime
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
#json_data = pd.DataFrame(json_data)
#json_try = json_data.to_json(json_data)

df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]

### first we need to take the string and turn that into a datime object and then create a string from the datetime in our format
df['mike'] = [datetime.strptime(row,'%Y%m%d').strftime('%d-%m-%Y') for row in df['Newdate']]
df['last_updated2'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') for row in df['last_updated']]
df.columns




df.dtypes

df["last_updated"][0:11]

str(df["last_updated"])


connection = pyodbc.connect("Driver={SQL Server}; Server=STUKLS022; Database=TradeTracker; uid=GAMI ;pwd=pw;Trusted_Connection=yes")    

cursor = connection.cursor()
SQLCommand = ("INSERT INTO EiaSeriesTest"  "(Id, Name, CategoryID, f, units, updated) "
          "VALUES (?,?,1,?,?,?)")

Values = list(zip(df.series_id, df.name, df.f, df.units, df.last_updated2)) 
#Values = df["series_id"], df["name"], df["f"], df["units"], df["last_updated"]
try:
    cursor.executemany(SQLCommand,Values)
except:
    print(Values)
    raise


connection.commit()
connection.close()



import json 
import pandas as pd
import pyodbc
from datetime import datetime
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['Newdate'] = [row+"01" if len(row) <8 else row for row in df['Date']]
df['mike'] = [datetime.strptime(row,'%Y%m%d').strftime('%d-%m-%Y') 
                                    for row in df['Newdate']]
df['last_updated2'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]
d = json.loads(line)
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)

from datetime import datetime, time 
import json
import pandas as pd
import pyodbc


i = 0
data = []
with open('L:\\TRADING\SQLDataUpload\PET.txt') as f:
    for line in f:
        i = i + 1
        d = json.loads(line)

Values = list(zip(df.series_id,  
                  pd.to_datetime(df['Newdate'], format='%Y%m%d'),
                  df['Amount'].astype(float)))
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [row[0] for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [row[0]+"01" for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d') for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d') if len(row) <8 else
          datetime.strptime(row[0], '%Y%m%d') for row in df['data']]
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d') if len(row) <8 else
          datetime.strptime(row[0], '%Y%m%d') for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d') if len(row) <8 else
          datetime.strptime(row[0], '%Y%m%d') for row in df['data']]
df['Amount'] = [row[1] for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if 
                len(row) <8 
                else
                datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row in df['data']]
df['Amount'] = [row[1].astype(float) for row in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if len(row) <8 
                else datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row 
                in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]


#connection = pyodbc.connect('''Driver={SQL Server}; 
#                            Server=STUKLS022; 
#                            Database=EIAData; 
#                            uid=GAMI ;
#                            pwd=pw;
#                            Trusted_Connection=yes''')    

connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO EIA Series Test"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))

cursor.executemany(SQLCommand,Values)

SQLCommand = (
            "INSERT INTO SeriesData"  
            "(series_id, Period, Value ) "
            "VALUES (?,?,?)"
              )

Values = list(zip(df.series_id,  
                  df.Date,
                  df.Amount))

cursor.executemany(SQLCommand,Values)


SQLCommand = (
            "INSERT INTO Category Test"  
            "(id, Name, ParentID) "
             "VALUES (?,?,?)"
             )

Values = list(zip(df.category_id, 
                  df.name, 
                  df.parent_category_id ))

cursor.execute(SQLCommand,Values)
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if len(row) <8 
                else datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row 
                in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]


#connection = pyodbc.connect('''Driver={SQL Server}; 
#                            Server=STUKLS022; 
#                            Database=EIAData; 
#                            uid=GAMI ;
#                            pwd=pw;
#                            Trusted_Connection=yes''')    

connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))

cursor.executemany(SQLCommand,Values)

SQLCommand = (
            "INSERT INTO SeriesData"  
            "(series_id, Period, Value ) "
            "VALUES (?,?,?)"
              )

Values = list(zip(df.series_id,  
                  df.Date,
                  df.Amount))

cursor.executemany(SQLCommand,Values)


SQLCommand = (
            "INSERT INTO Category"  
            "(id, Name, ParentID) "
             "VALUES (?,?,?)"
             )

Values = list(zip(df.category_id, 
                  df.name, 
                  df.parent_category_id ))

cursor.execute(SQLCommand,Values)
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if len(row) <8 
                else datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row 
                in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]


#connection = pyodbc.connect('''Driver={SQL Server}; 
#                            Server=STUKLS022; 
#                            Database=EIAData; 
#                            uid=GAMI ;
#                            pwd=pw;
#                            Trusted_Connection=yes''')    

connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))

cursor.executemany(SQLCommand,Values)

SQLCommand = (
            "INSERT INTO SeriesData"  
            "(series_id, Period, Value ) "
            "VALUES (?,?,?)"
              )

Values = list(zip(df.series_id,  
                  df.Date,
                  df.Amount))

cursor.executemany(SQLCommand,Values)


SQLCommand = (
            "INSERT INTO Category"  
            "(id, Name, ParentID) "
             "VALUES (?,?,?)"
             )

Values = list(zip(df.series_id, 
                  df.name, 
                  df.description ))

cursor.execute(SQLCommand,Values)
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if len(row) <8 
                else datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row 
                in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]


#connection = pyodbc.connect('''Driver={SQL Server}; 
#                            Server=STUKLS022; 
#                            Database=EIAData; 
#                            uid=GAMI ;
#                            pwd=pw;
#                            Trusted_Connection=yes''')    

connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))

cursor.executemany(SQLCommand,Values)

SQLCommand = (
            "INSERT INTO SeriesData"  
            "(series_id, Period, Value ) "
            "VALUES (?,?,?)"
              )

Values = list(zip(df.series_id,  
                  df.Date,
                  df.Amount))

cursor.executemany(SQLCommand,Values)


SQLCommand = (
            "INSERT INTO Category"  
            "(id, Name, ParentID) "
             "VALUES (?,?,?)"
             )

Values = list(zip(df.series_id, 
                  df.name, 
                  df.description ))

cursor.executemany(SQLCommand,Values)

connection.commit()
connection.close()
Values = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
Values
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
Values
"""
Created on Thu Jan 11 09:37:29 2018

@author: mima
"""

from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if len(row) <8 
                else datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row 
                in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]


#connection = pyodbc.connect('''Driver={SQL Server}; 
#                            Server=STUKLS022; 
#                            Database=EIAData; 
#                            uid=GAMI ;
#                            pwd=pw;
#                            Trusted_Connection=yes''')    

connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))

cursor.executemany(SQLCommand,Values)

SQLCommand = (
            "INSERT INTO SeriesData"  
            "(series_id, Period, Value ) "
            "VALUES (?,?,?)"
              )

Values = set(list(zip(df.series_id,  
                  df.Date,
                  df.Amount)))

cursor.executemany(SQLCommand,Values)


SQLCommand = (
            "INSERT INTO Category"  
            "(id, Name, ParentID) "
             "VALUES (?,?,?)"
             )

Values = set(list(zip(df.series_id, 
                  df.name, 
                  df.description )))

cursor.executemany(SQLCommand,Values)

connection.commit()
connection.close()
Values = set(list(zip(df.series_id,  
                  df.Date,
                  df.Amount)))
len(Values)
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
len(Values)
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))

[cursor.executemany(SQLCommand,Values) if len(Values) > 1 else cursor.execute(SQLCommand,Values)]


SQLCommand = (
            "INSERT INTO SeriesData"  
            "(series_id, Period, Value ) "
            "VALUES (?,?,?)"
              )

Values = set(list(zip(df.series_id,  
                  df.Date,
                  df.Amount)))

[cursor.executemany(SQLCommand,Values) if len(Values) > 1 else cursor.execute(SQLCommand,Values)]


SQLCommand = (
            "INSERT INTO Category"  
            "(id, Name, ParentID) "
             "VALUES (?,?,?)"
             )

Values = set(list(zip(df.series_id, 
                  df.name, 
                  df.description )))

[cursor.executemany(SQLCommand,Values) if len(Values) > 1 else cursor.execute(SQLCommand,Values)]

connection.commit()
connection.close()
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
if len(Values) >1:
    cursor.executemany(SQLCommand,Values)
else:
    cursor.execute(SQLCommand,Values)

Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
Values
cursor.executemany(SQLCommand,Values)
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
cursor.executemany(SQLCommand,Values)
cursor.execute(SQLCommand,Values)
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
Values
zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)
Values = list(set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
Values = list(set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
if len(Values) >1:
    cursor.executemany(SQLCommand,Values)
else:
    cursor.execute(SQLCommand,Values)

connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
cursor.execute(SQLCommand,Values)
Values
Values = set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
Values
cursor.execute(SQLCommand,Values)
Values = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
Values
cursor.execute(SQLCommand,Values)
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
cursor.executemany(SQLCommand,Values)
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
Values
Values = set(list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))[0]
Values = list(set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))
cursor.execute(SQLCommand,Values)
Values1 = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
cursor.execute(SQLCommand,Values1)
cursor.execute(SQLCommand,Values)
Values = list(set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))[0]
cursor.execute(SQLCommand,Values)
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
Values = list(set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)))[0]
if len(Values) >1:
    cursor.executemany(SQLCommand,Values1)
else:
    cursor.execute(SQLCommand,Values)

connection.commit()
connection.close()
set(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)
Values = zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated)
Values
print(Values)
Values = (zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
print(Values)
connection.close()

## ---(Thu Jan 11 11:24:31 2018)---
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if len(row) <8 
                else datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row 
                in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]
df_test = pd.concat([f.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1)
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1)
df_test = df_test.unique()
df_test = df_test.unique
df_test = df_test.drop_duplicates()
df_test = df_test.drop_duplicates().values.tolist()
Values1 = list(zip(df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated))
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()
SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
cursor.execute(SQLCommand,df_test)
df_test = df_test.drop_duplicates().values
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1)
df_test = df_test.drop_duplicates().values
cursor.execute(SQLCommand,df_test)
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1)

df_test = list(df_test.drop_duplicates().values)
cursor.execute(SQLCommand,df_test)
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1)
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1).to_list()
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1).to_list
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1).tolist()
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1).values.tolist()
cursor.executemany(SQLCommand,df_test)
connection.commit()
connection.close()
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1).values.drop_duplicates.tolist()
df_test = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1).drop_duplicates().values.tolist()
cursor.executemany(SQLCommand,df_test)
connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()

SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )
cursor.executemany(SQLCommand,df_test)
connection.commit()
connection.close()
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PETTest.txt','r').read()
df = pd.read_json(json_data)
df['Date'] = [datetime.strptime((row[0]+"01"), '%Y%m%d').strftime('%d-%m-%Y')
                if len(row) <8 
                else datetime.strptime(row[0], '%Y%m%d').strftime('%d-%m-%Y')
                for row 
                in df['data']]
df['Amount'] = [float(row[1]) for row in df['data']]
df['last_updated'] = [dateutil.parser.parse(row).strftime('%d-%m-%Y') 
                                    for row in df['last_updated']]


#connection = pyodbc.connect('''Driver={SQL Server}; 
#                            Server=STUKLS022; 
#                            Database=EIAData; 
#                            uid=GAMI ;
#                            pwd=pw;
#                            Trusted_Connection=yes''')    

connection = pyodbc.connect('''Driver={SQL Server}; 
                            Server=STCHGS112; 
                            Database=MIMAWorkSpace; 
                            uid=MIMA ;
                            pwd=pw;
                            Trusted_Connection=yes''')    

cursor = connection.cursor()

SQLCommand = (
            "INSERT INTO Series"  
            "(id, Name, CategoryId, f, Units, updated) "
            "VALUES (?,?,1,?,?,?)"
             )

Values = pd.concat([df.series_id,
                  df.name,
                  df.f,
                  df.units, 
                  df.last_updated], axis = 1).drop_duplicates().values.tolist()

cursor.executemany(SQLCommand,Values)

SQLCommand = (
            "INSERT INTO SeriesData"  
            "(series_id, Period, Value ) "
            "VALUES (?,?,?)"
              )

Values = pd.concat([df.series_id,  
                  df.Date,
                  df.Amount], axis = 1).drop_duplicates().values.tolist()

cursor.executemany(SQLCommand,Values)

SQLCommand = (
            "INSERT INTO Category"  
            "(id, Name, ParentID) "
             "VALUES (?,?,?)"
             )


Values = pd.concat([df.series_id, 
                  df.name, 
                  df.description], axis = 1).drop_duplicates().values.tolist()

cursor.executemany(SQLCommand,Values)

connection.commit()
connection.close()
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PET.txt','r').read()
df = pd.read_json(json_data)
json_data =  open('L:\\TRADING\SQLDataUpload\PET.txt','r').read()
json_data.head()
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

json_data =  open('L:\\TRADING\SQLDataUpload\PET.txt','r').read()
df = pd.read_json(json_data)
df.head()
from datetime import datetime, time 
import json
import pandas as pd
import pyodbc
import dateutil.parser

#json_data =  open('L:\\TRADING\SQLDataUpload\PET.txt','r').read()

df = pd.read_json('L:\\TRADING\SQLDataUpload\PET.txt', lines=True)
df['data']
df[145616]
df.loc[145616]
df.head()
df['category_id']
category_id = df['category_id']
category_id = df['category_id'][139158]
category_id = df['category_id'][139168]
from pandas.io.json import json_normalize
lol = json_normalize(df)
json_normalize(df)
df.loc[145616]
df.loc[145]
farm = df[df['series_id' == PET.KDRVAFNUS1.A]]
farm = df[df['series_id' == 'PET.KDRVAFNUS1.A']]
farm = df[df['series_id'] == 'PET.KDRVAFNUS1.A']
farm = df[df['category_id'] == 235678]
farm = df[df['parent_category_id'] == 235678]
farm = df[df['series_id'] == 'PET.WCRFPUS2.4']
farm = df[df['series_id'] == 'PET.WCRFPUS2.w']
farm = df[df['series_id'] == 'PET.WCRFPUS2.W']
farm = df[df['category_id'] == 235678]
import pandas
ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheet='Data')

## ---(Mon Jan 22 17:08:08 2018)---
import pandas as pd
import numpy as np


med_data = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx')
grade_list med_data['Grade'].unique()
grade_list = med_data['Grade'].unique()
grade_list = med_data['Grade'].unique
grade_list = med_data['Grade'].unique
print(grade_list)
grade_list = med_data['Grade'].unique()
print(grade_list)
[conditions_cpc if med_data['Grade'] == 'CPC' else conditions_other]
[conditions_cpc if med_data['Grade'] == 'CPC' else conditions_other in med_data['Grade']]
['1' for row in med_data['Grade'] if med_data['Grade'] == 'CPC' else '2' ]
[print('1') for row in med_data['Grade'] if med_data['Grade'] == 'CPC' else '2' 
[print('1') if med_data['Grade'] == 'CPC' else print('2') for row in med_data['Grade'] ]
med_data['Grade']
print('1') if med_data['Grade'] == 'CPC'
[print('1') if med_data['Grade'] == 'CPC' else print('2') for row in med_data['Grade']
[print('1') if med_data['Grade'] == 'CPC' else print('2') for row in med_data['Grade'] ]
print('1') if (med_data['Grade'] == 'CPC') else print('2') for row in med_data['Grade'] ]
[print('1') if (med_data['Grade'] == 'CPC') else print('2') for row in med_data['Grade'] ]
med_data['Grade'] == 'CPC'
print('2')
print('1') if (med_data['Grade'] == 'CPC') else print('2')
print('2')
print('1')
(med_data['Grade'] == 'CPC')
med_data['Grade']
row
[print('1') if (med_data[med_data['Grade'] == 'CPC']) else print('2') for row in med_data['Grade'] ]
(med_data[med_data['Grade'] == 'CPC']
(med_data[med_data['Grade'] == 'CPC'])
[print('1') if row == 'CPC' else print('2') for row in med_data['Grade'] ]
def standardise_cargo_size(i):
    [replacement for condition, replacement in [conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade'] ]]

ar = map(standardise_cargo_size, med_data['Quantity'])
ar
print(ar)    
standardise_cargo_size(med_data['Quantity'])
conditions_other = [(lambda i: i < 300, 250),
                    (lambda i: 300 <= i  < 400, 315),
                    (lambda i: 400 <= i < 500, 450),
                    (lambda i: 500 <= i < 600, 510),
                    (lambda i: 600 <= i < 700, 630),
                    (lambda i: 700 <= i < 900, 750),
                    (lambda i: 900 <= i <= 1200, 1020),
                    (lambda i: i > 1200, 2050)]

conditions_cpc = [(lambda i: i < 500, 330),
                  (lambda i: 500 <= i < 600, 535),
                  (lambda i: 600 <= i < 720, 670),
                  (lambda i: 720 <= i <= 760, 760)]
def standardise_cargo_size(i):
    [replacement for condition, replacement in [conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade'] ]]

ar = map(standardise_cargo_size, med_data['Quantity'])
print(ar)  
standardise_cargo_size(med_data['Quantity'])
def standardise_cargo_size2(i):
    for condition, replacement in conditions_other:
        if conbdition(i):
            return replacement
    return i

med_data['Quantity']
ar = map(standardise_cargo_size2, med_data['Quantity'])
print(ar)
ar = list(map(standardise_cargo_size2, med_data['Quantity']))
for condition, replacement in conditions_other:
    if condition(i):
        return replacement

return i
ar = list(map(standardise_cargo_size2, med_data['Quantity']))
def standardise_cargo_size2(i):
    for condition, replacement in conditions_other:
        if condition(i):
            return replacement
    return i

ar = list(map(standardise_cargo_size2, med_data['Quantity']))
def standardise_cargo_size(i):
    [replacement for condition, replacement in [conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']]]

ar = list(map(standardise_cargo_size, med_data['Quantity']))
def standardise_cargo_size2(i):
    for condition, replacement in conditions_other:
        if condition(i):
            return replacement
    return i

ar = list(map(standardise_cargo_size2, med_data['Quantity']))
def standardise_cargo_size(i):
    return [replacement for condition, replacement in [conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']]]



ar = list(map(standardise_cargo_size, med_data['Quantity']))
[conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']]
def standardise_cargo_size(i):
    return [replacement for condition, replacement in 
            [conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']] if condition(i)]

ar = list(map(standardise_cargo_size, med_data['Quantity']))
[replacement for condition, replacement in 
        [conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']] if condition(i)]
def standardise_cargo_size(i):
    return [replacement for condition(i), replacement in 
            [conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']]]

[conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']]
def standardise_cargo_size(i):
    return [replacement for condition, replacement in 
            [conditions_cpc(i) if row == 'CPC' else conditions_other(i) for row in med_data['Grade']]]

ar = list(map(standardise_cargo_size, med_data['Quantity']))
map(standardise_cargo_size, med_data['Quantity'])
standardise_cargo_size
ar = map(standardise_cargo_size, med_data['Quantity'])
ar = map(standardise_cargo_size, med_data['Quantity'])

print(ar)
ar = list(map(standardise_cargo_size, med_data['Quantity']))
ar = pd.Series(map(standardise_cargo_size, med_data['Quantity']))
def standardise_cargo_size(i):
     [replacement for condition, replacement in 
            [conditions_cpc(i) if row == 'CPC' else conditions_other(i) for row in med_data['Grade']]]

ar = pd.Series(map(standardise_cargo_size, med_data['Quantity']))
[conditions_cpc(i) if row == 'CPC' else conditions_other(i) for row in med_data['Grade']]
def standardise_cargo_size(i):
     [replacement for condition, replacement in 
            [conditions_cpc(i) if row == 'CPC' else conditions_other(i) for row in med_data['Grade']]]

standardise_cargo_size(200) 
[conditions_cpc if row == 'CPC' else conditions_other for row in med_data['Grade']]
print(grade_list)
def standardise_cargo_size(i):
     [replacement for condition, replacement in 
            [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]

[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
def standardise_cargo_size_attempt(i):
     [replacement for condition(i), replacement in 
            [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]

def standardise_cargo_size(i):
     [replacement for i, replacement in 
            [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]

standardise_cargo_size(med_data['Quantity'])
def standardise_cargo_size(i):
     [replacement for i, replacement in 
            [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]

[replacement for i, replacement in 
       [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
[replacement for i, replacement in 
       [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
def standardise_cargo_size_attempt(i):
     [replacement for condition, replacement in 
            [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']](i)]

standardise_cargo_size(med_data['Quantity'])
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']](i)
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']](600)
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']
[i for condition, replacement in conditions_other if condition(i)]
[replacement for condition, replacement in conditions_other if condition(i)]
[replacement for condition, replacement in conditions_other if condition(248)]
conditions_other
[replacement for condition, replacement in [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']] if condition(248)]
[replacement for condition, replacement in conditions_other if condition(med_data['Quantity'])]
med_data['Quantity']
def standardise_cargo_size3(i):
[replacement for condition, replacement in conditions_other if condition(med_data['Quantity'])]
condition(med_data['Quantity'])
replacement for condition, replacement in conditions_other if condition(med_data['Quantity'])
[replacement for condition, replacement in conditions_other for row in condition(med_data['Quantity'])]
[replacement for condition, replacement in conditions_other if condition(1000)]
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
def choose_condition(i):
    if i == 'CPC Blend':
        conditions_cpc
    else:
        conditions_other

choose_condition(med_data['Grade'])
med_data['Grade'
med_data['Grade']
choose_condition('CPC Blend')
def choose_condition(i):
    if i == 'CPC Blend':
        conditions_cpc
    else:
        conditions_other


choose_condition('CPC Blend')
x = choose_condition('CPC Blend')
def choose_condition(i):
    if i == 'CPC Blend':
        conditions_cpc(250)
    else:
        conditions_other

x = choose_condition('CPC Blend')
e
conditions_cpc(250)
def choose_condition(k):
    if i == 'CPC Blend':
        conditions_cpc(250)
    else:
        conditions_other

conditions_cpc(250)
conditions_cpc = [(lambda i: i < 500, 330),
                  (lambda i: 500 <= i < 600, 535),
                  (lambda i: 600 <= i < 720, 670),
                  (lambda i: 720 <= i <= 760, 760)]
els
conditions_cpc(250)
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
[conditions_cpc(250) if row == 'CPC BLEND' else conditions_other(250) for row in med_data['Grade']]
[replacement for function, replacement in conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade'] if function(i)]
[replacement for condition, replacement in 
       [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']] if condition(250)]
[conditions_cpc if row == 'CPC BLEND' else conditions_other(250) for row in med_data['Grade']]
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
[x for x in [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]
[x[1] for x in [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]
[x[0] for x in [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]
[x[0] for x in 
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
[function, replacement for x in [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]
[x(i) for x in [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]
[x(2520) for x in [conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]]
conditions_other[0]
[conditions_cpc if row == 'CPC BLEND' else conditions_other for row in med_data['Grade']]
return replacement if condition(i) else i for condition, replacement in conditions_cpc
def apply_condition(quantity, grade):
    if grade == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(quantity):
                return replacement
    else:
        for condition, replacement in conditions_other:
            if condition(quantity):
                return replacement

apply_condition(250, 'x')
apply_condition(med_data['Quantity'], med_data['Grade'])
for row in med_data:
    apply_condition(med_data['Quantity'], med_data['Grade'])

def apply_condition(df):
    if df['Grade'] == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(df['Quantity']):
                return replacement
    else:
        for condition, replacement in conditions_other:
            if condition(df['Quantity']):
                return replacement

def apply_condition(df):
    if df['Grade'] == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(df['Quantity']):
                return replacement
    else:
        for condition, replacement in conditions_other:
            if condition(df['Quantity']):
                return replacement

apply_condition(med_data)
condition(df['Quantity'])
for condition, replacement in conditions_cpc:
    if condition(df['Quantity']):
        return replacement

for condition, replacement in conditions_cpc:
    if condition(df['Quantity']):

if df['Grade'] == 'CPC Blend':
    for condition, replacement in conditions_cpc:
        if condition(df['Quantity']):
            return replacement
else:
    for condition, replacement in conditions_other:
        if condition(df['Quantity']):
            return replacement

if df['Grade'] == 'CPC Blend':
    for condition, replacement in conditions_cpc:
        if condition(df['Quantity']):
            return replacement

if df['Grade'] == 'CPC Blend':
    for condition, replacement in conditions_cpc:
        if condition(df['Quantity']):
    return replacement
else:
    for condition, replacement in conditions_other:
        if condition(df['Quantity']):
    return replacement

new_df = pd.concat([med_data['Quantity'], med_data['Grade']])

def apply_condition(df):
    if df['Grade'] == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(df['Quantity']):
                return replacement

if df['Grade'] == 'CPC Blend':
    for condition, replacement in conditions_cpc:
        if condition(df['Quantity']):
            return replacement

if med_data['Grade'] == 'CPC Blend':
    for condition, replacement in conditions_cpc:
        if condition(med_data['Quantity']):
            return replacement

def apply_condition(df):
    if med_data['Grade'] == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(med_data['Quantity']):
                return replacement
    else:
        for condition, replacement in conditions_other:
            if condition(df['Quantity']):
                return replacement
    return df['Quantity']

apply_condition(med_data)
def apply_condition(df):
    if med_data['Grade'] == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(med_data['Quantity']):
                return replacement

def apply_condition(df):
    if df['Grade'] == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(med_data['Quantity']):
                return df['replacement']
    else:
        for condition, replacement in conditions_other:
            if condition(df['Quantity']):
                return df['replacement']
    return df['Quantity']

apply_condition(med_data)
med_data['Quantity'
med_data['Quantity']
df['Grade']
apply_condition(med_data)
def apply_condition(df):
    if df['Grade'] == 'CPC Blend':
        for condition, replacement in conditions_cpc:
            if condition(df['Quantity']):
                return df['replacement'].all()
    else:
        for condition, replacement in conditions_other:
            if condition(df['Quantity']):
                return df['replacement'].all()
    return df['Quantity'].all()


apply_condition(med_data)
apply_condition(med_data).all()
def cond_other(i):
    if i < 300:
        return 250

cond_other(300)
cond_other(100)
def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    else i > 1200:
        return 2050

def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050
    else:
        print('error')

def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050
    else:
        print('error')


med_data['Quantity'].apply(cond_other)
def cond_cpc(i):
    if i < 500:
        return 330
    elif 500 <= i  < 600:
        return 535
    elif 600 <= i < 720:
        return 670
    elif 720 <= i < 760:
        return 760
    else:
        print('error')

print(grade_list)
def cond_cpc(i):
    if i < 500:
        return 330
    elif 500 <= i  < 600:
        return 535
    elif 600 <= i < 720:
        return 670
    elif 720 <= i < 760:
        return 760
    else:
        print('error')


def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050
    else:
        print('error')


def standardize_cargos(cargo):
    if cargo['Grade'] == 'CPC BLEND':
        return cond_cpc(cargo['Quantity'])
    else:
        return cond_other(cargo['Quantity'])

med_data['Quantity'].apply(standardize_cargos, axis=1)
med_data['Quantity'].apply(standardize_cargos)
med_data.apply(standardize_cargos, axis=1)
def cond_cpc(i):
    if i < 500:
        return 330
    elif 500 <= i  < 600:
        return 535
    elif 600 <= i < 720:
        return 670
    elif 720 <= i < 760:
        return 760
    else:
        print('error')


def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050
    else:
        print('error')


def standardize_cargos(cargo):
    if cargo['Grade'] == 'CPC BLEND':
        return cond_cpc(cargo['Quantity'])
    else:
        return cond_other(cargo['Quantity'])



med_data.apply(standardize_cargos, axis=1)
def cond_cpc(i):
    if i < 500:
        return 330
    elif 500 <= i  < 600:
        return 535
    elif 600 <= i < 720:
        return 670
    elif 720 <= i < 760:
        return 760



def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050



def standardize_cargos(cargo):
    if cargo['Grade'] == 'CPC BLEND':
        return cond_cpc(cargo['Quantity'])
    else:
        return cond_other(cargo['Quantity'])



med_data.apply(standardize_cargos, axis=1)
def cond_cpc(i):
    if i < 500:
        return 330
    elif 500 <= i  < 600:
        return 535
    elif 600 <= i < 720:
        return 670
    elif 720 <= i < 760:
        return 760



def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050



def standardize_cargos(cargo):
    if cargo['Grade'] == 'CPC BLEND':
        return cond_cpc(cargo['Quantity'])
    else:
        return cond_other(cargo['Quantity'])



med_data['converted'] = med_data.apply(standardize_cargos, axis=1)
quantities = pd.concat([med_data['Grade'], med_data['Quantity'], med_data['converted'])
quantities = pd.concat([med_data['Grade'], med_data['Quantity'], med_data['converted']])
quantities = pd.concat([med_data['Grade'], med_data['Quantity'], med_data['converted']], axis = 1)
def cond_cpc(i):
    if i < 500:
        return 330
    elif 500 <= i  < 600:
        return 535
    elif 600 <= i < 720:
        return 670
    elif 720 <= i < 760:
        return 760
    else:
        return i



def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050
    else:
        return i



def standardize_cargos(cargo):
    if cargo['Grade'] == 'CPC BLEND':
        return cond_cpc(cargo['Quantity'])
    else:
        return cond_other(cargo['Quantity'])



med_data['converted'] = med_data.apply(standardize_cargos, axis=1)

quantities = pd.concat([med_data['Grade'], med_data['Quantity'], med_data['converted']], axis = 1)
print(grade_list)
grade_list = list(med_data['Grade'].unique())
TARGO_DATA = pd.read_excel('L:\TRADING\ANALYSIS\BALANCES\NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
cpc_load = TARGO_DATA[TARGO_DATA['Grade'] == 'CPC Blend']['Load Point']
cpc_load = TARGO_DATA[TARGO_DATA['Grade'] == 'CPC Blend']['LoadPoint']
cpc_load = TARGO_DATA[TARGO_DATA['Grade'] == 'CPC Blend']['LoadPoint'].unique()
cpc_load = TARGO_DATA[TARGO_DATA['Grade'] == 'CPC Blend']['LoadPoint'].unique().tolist()
for grade in TARGO_DATA['Grade']:
    load_Ports = TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique().tolist()
    return load_ports

for grade in TARGO_DATA['Grade']:
    load_Ports = TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique().tolist()

return load_ports
for grade in list(med_data['Grade'].unique()):
    load_ports = TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique().tolist()

return load_ports
load_ports =[]
for grade in list(med_data['Grade'].unique()):
    load_port.append(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique().tolist())

return load_ports
load_ports =[]
for grade in list(med_data['Grade'].unique()):
    load_ports.append(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique().tolist())

return load_ports
load_ports =[]
for grade in list(med_data['Grade'].unique()):
    load_ports.append(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique().tolist())
    return load_ports

load_ports =[]
for grade in list(med_data['Grade'].unique()):
    load_ports.append(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique().tolist())


list(med_data['Grade'].unique())
load_ports =[]
for grade in list(med_data['Grade'].unique()):
    load_ports.append(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'])

TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique
(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint']).unique
TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint']
TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()
TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique().dropna()
TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique.dropna()
TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique
unique(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'])
pd.dataframe(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna(
pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna()
for grade in list(med_data['Grade'].unique()):
    for i in pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna():
        df['Grade'] = grade
        df['LoadPoint'] = i

for grade in list(med_data['Grade'].unique()):
    df = pd.Dataframe(columns=['Grade','LoadPoint'])
    for i in pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna():
        df['Grade'] = grade
        df['LoadPoint'] = i

df = pd.Dataframe(columns=['Grade','LoadPoint'])
import pandas as pd
import numpy as np
df = pd.Dataframe(columns=['Grade','LoadPoint'])
for grade in list(med_data['Grade'].unique()):
    df = pd.DataFrame(columns=['Grade','LoadPoint'])
    for i in pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna():
        df['Grade'] = grade
        df['LoadPoint'] = i

pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna()
for i in pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna():
    df['Grade'] = grade
    df['LoadPoint'] = i

for i in pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna():
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i

df = pd.DataFrame(columns=['Grade','LoadPoint'])
for i in pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna():
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i

pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna()
for i in pd.DataFrame(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna():
    print(i)

TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint']
TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique().dropna()
ist(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna()
list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()).dropna()
list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique(
list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    print(i)

for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i

df['Grade'] 
df = pd.DataFrame(columns=['Grade','LoadPoint'])
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i

list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
df = pd.DataFrame([])
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i

return df
df = pd.DataFrame([])
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i
    return df

for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i
        return df

for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i
    return df

for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i

return df
for grade in list(med_data['Grade'].unique()):
    df = pd.DataFrame([])
    for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
        df['Grade'] = 'AZERI'
        df['LoadPoint'] = i
        return df

df = pd.DataFrame([])
df['Grade'] = 'AZERI'
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'] = 'AZERI'
    df['LoadPoint'] = i

df = pd.DataFrame([])
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'].append('AZERI')
    df['LoadPoint'].append(i)

df = pd.DataFrame(['Grade','LoadPoint'])
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'].append('AZERI')
    df['LoadPoint'].append(i)

df = pd.DataFrame(['Grade','LoadPoint'])
df = pd.DataFrame(column=['Grade','LoadPoint'])
df = pd.DataFrame(columns=['Grade','LoadPoint'])
df = pd.DataFrame(columns=['Grade','LoadPoint'])
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    df['Grade'].append('AZERI')
    df['LoadPoint'].append(i)

list(med_data['Grade'].unique())
list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    print(i)

for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    print(i)
    df['Grade'] = i

df['Grade']
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    print(i)
    df['Grade'].append(i)

ist(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    print(i)
    load_ports.append(i)

load_ports =[]
for i in list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique()):
    print(i)
    load_ports.append(i)

list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
r port in UniquePortsForGrade:
    load_ports.append(zip('Grade',port))
for port in UniquePortsForGrade:
    load_ports.append(zip('Grade',port))

UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for port in UniquePortsForGrade:
    load_ports.append(zip('Grade',port))

load_ports =[]
UniquePortsForGrade
port
load_ports =[]
UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for port in UniquePortsForGrade:
    load_ports.append(zip('Grade',port))

UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for port in UniquePortsForGrade:
    load_ports.append(zip(['Grade'],port))

UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for port in UniquePortsForGrade:
    load_ports.append(['Grade',port])

load_ports =[]
UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == 'AZERI']['LoadPoint'].unique())
for port in UniquePortsForGrade:
    load_ports.append(['Grade',port])

med_data['Grade'].dropna().unique()
load_ports =[]
for grade in list(med_data['Grade'].dropna().unique()):
    #df = pd.DataFrame(columns=['Grade','LoadPoint'])
    UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique())
    for port in UniquePortsForGrade:
        load_ports.append(['Grade',port])

load_ports =[]
for grade in list(med_data['Grade'].dropna().unique()):
    #df = pd.DataFrame(columns=['Grade','LoadPoint'])
    UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique())
    for port in UniquePortsForGrade:
        load_ports.append(['Grade',port])

load_ports =[]
for grade in list(med_data['Grade'].dropna().unique()):
    #df = pd.DataFrame(columns=['Grade','LoadPoint'])
    UniquePortsForGrade = list(TARGO_DATA[TARGO_DATA['Grade'] == grade]['LoadPoint'].unique())
    for port in UniquePortsForGrade:
        load_ports.append([grade,port])

list(med_data['Grade'].dropna().unique()
med_data['LoadPoints']
load_port == nan
import mysql
import pypyodbc
import pypyodbc

connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')
connection.close()
import pypyodbc

connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

cursor = connection.cursor()

cursor.execute("""
               SELECT Supply.Country, Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
               """)



connection.close()
tables = cursor.fetchall()
import pypyodbc

connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

cursor = connection.cursor()

cursor.execute("""
               SELECT Supply.Country, Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
               """)

tables = cursor.fetchall()
print(tables)
IEA_test_list = []
for row in tables:
    IEA_test_list.append({'Country':row.Country, 'Product':row.Product, 'Period':row.Period, 'Asof':row.Asof, 'PeriodType':row.PeriodType})

IEAtest = pd.DataFrame(IEA_test_list)
import pypyodbc
import pandas as pd

connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

cursor = connection.cursor()

cursor.execute("""
               SELECT Supply.Country, Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
               """)

tables = cursor.fetchall()

IEA_test_list = []
for row in tables:
    print(row.Country)

for row in tables:
    print(row)

for row in tables:
    print(row.Supply.Country)

connection.close()
import pypyodbc
import pandas as pd

connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

cursor = connection.cursor()

cursor.execute("""
               SELECT Supply.Country [Country], Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
               """)

tables = cursor.fetchall()
for row in tables:
    print(row.Country)

for row in tables:
    print(row)
    dtype(row)

for row in tables:
    print(row.dtype)

for row in tables:
    print(row)

query = """
               SELECT Supply.Country [Country], Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
query = """
               SELECT Supply.Country [Country], Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
               """
connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')
pdtable = pd.read_sql(query, connection)
query = """
               SELECT Supply.Country, Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
               """
pdtable = pd.read_sql(query, connection)
import pypyodbc
import pandas as pd

iea_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')
iea_query = """
               SELECT Supply.Country, Supply.Product, Supply.Period, Supply.Quantity, Supply.Asof, Supply.PeriodType
               FROM IEAData.dbo.Supply Supply
               WHERE (Supply.Country In ('USA')) 
               AND Supply.PeriodType = 'MTH'
               AND Supply.Asof = (Select MAX(Asof) FROM  IEAData.dbo.Supply)
               AND Supply.Product IN ('CRUDE', 'COND')
               AND Period >= '1-jan-14'
               ORDER BY Supply.Country
               """

jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               """               


iea_table = pd.read_sql(iea_query, iea_connection)
jodi_table = pd.read_sql(jodi_query, jodi_connection)
print(jodi_table.head())
list(jodi_table['description'].unique())
iea_query = """
                SELECT CrudeData.Country, CrudeData.Product, CrudeData.Balance, CrudeData.Period, CrudeData.Quantity, CrudeData.Asof, CrudeData.PeriodType
                FROM IEAData.dbo.CrudeData CrudeData
                ORDER BY CrudeData.Country
                """
iea_table = pd.read_sql(iea_query, iea_connection)
list(iea_table['country'].unique())
iea_query = """
                SELECT CrudeData.Country, CrudeData.Product, Balance.Desc, CrudeData.Period, CrudeData.Quantity, CrudeData.Asof, CrudeData.PeriodType
                FROM IEAData.dbo.CrudeData CrudeData
                INNER JOIN IEAData.dbo.Balance Balance ON CrudeData.Balance = Balance.Flow
                ORDER BY CrudeData.Country
                """
iea_table = pd.read_sql(iea_query, iea_connection)
iea_query = """
                SELECT CrudeData.Country, CrudeData.Product, Balance.[Desc], CrudeData.Period, CrudeData.Quantity, CrudeData.Asof, CrudeData.PeriodType
                FROM IEAData.dbo.CrudeData CrudeData
                INNER JOIN IEAData.dbo.Balance Balance ON CrudeData.Balance = Balance.Flow
                ORDER BY CrudeData.Country
                """
iea_table = pd.read_sql(iea_query, iea_connection)
list(jodi_table['description'].unique())
list(jodi_table['Country'].unique())
list(jodi_table['country'].unique())
len(list(jodi_table['country'].unique()))
jodi_pivot = pd.pivot_table(jodi_table, values = 'quantity', index = 'month', columns = 'countries')
jodi_pivot = pd.pivot_table(jodi_table, values = 'quantity', index = 'month', columns = 'country')
jodi_table_kbbl = jodi_table[jodi_table['unit'] == 'KBBL']
jodi_pivot = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = 'description')
jodi_pivot = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_table_kbd = jodi_table[jodi_table['unit'] == 'KBD']
jodi_pivot = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_table_kbbl = jodi_table[jodi_table['unit'] == 'KBBL']
jodi_table_kbd = jodi_table[jodi_table['unit'] == 'KBD']


jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
list(jodi_table['month'].unique())
jodi_table_kbd['month'].strftime('%m/%d/%Y')
jodi_table_kbd['month'].dt.strftime('%m/%d/%Y')
jodi_table_kbd_0809 = jodi_table_kbd['01/01/2010' < jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009']
jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009'
jodi_table_kbd_0809 = jodi_table_kbd['01/01/2010' > jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009']
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009') & (jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') < '01/01/2010')]
jodi_table_kbd['month'].dt.strftime('%m/%d/%Y')
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009')]
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009') & (jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') < '01/01/2010')]
(jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009')
jodi_table_kbd['month'].dt.strftime('%m/%d/%Y')
(jodi_table_kbd['month'].dt.strftime('%m/%d/%Y') > '01/01/2009')
jodi_table_kbd['month'].dt.strftime('%Y/%m/%d') > '01/01/2009'
jodi_table_kbd['month'].dt.strftime('%Y/%m/%d')
jodi_table_kbd['month'].dt.strftime('%m/%d/%Y')
from datetime import datetime
(jodi_table_kbd['month'] > datetime(2013, 1, 1))
(jodi_table_kbd['month'] > datetime(2013, 15, 1))
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'] > datetime(2009, 1, 1)) & (jodi_table_kbd['month'] < datetime(2011, 1, 1))]
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'] > datetime(2009, 1, 1)) &
                                     (jodi_table_kbd['month'] < datetime(2011, 1, 1)) &
                                     (jodi_table_kbd['product'] == 'TOTCRUDE')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'] > datetime(2009, 11, 1)) &
                                     (jodi_table_kbd['month'] < datetime(2010, 2, 1)) &
                                     (jodi_table_kbd['product'] == 'TOTCRUDE')
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'] > datetime(2009, 11, 1)) &
                                     (jodi_table_kbd['month'] < datetime(2010, 2, 1)) &
                                     (jodi_table_kbd['product'] == 'TOTCRUDE')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_0809, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
list(jodi_table['description'].unique())
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'] > datetime(2009, 11, 1)) &
                                     (jodi_table_kbd['month'] < datetime(2010, 2, 1)) &
                                     (jodi_table_kbd['product'] == 'TOTCRUDE') &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_0809, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_0809.dropna(axis=0), values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_table_kbd_0809
jodi_table_kbd_0809.dropna(axis=0)
jodi_table_kbd_0809.dropna(axis=1)
jodi_table_kbd_0809.dropna(axis=1, how='all')
jodi_table_kbd_0809_nozeros = jodi_table_kbd_0809.loc[(jodi_table_kbd_0809 != 0)]
jodi_table_kbd_0809_nozeros = jodi_table_kbd_0809.loc[(jodi_table_kbd_0809 != 0).all(axis=1), :]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_0809_nozeros, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_table_kbd_0809 = jodi_table_kbd[(jodi_table_kbd['month'] > datetime(2007, 11, 1)) &
                                     (jodi_table_kbd['month'] < datetime(2012, 2, 1)) &
                                     (jodi_table_kbd['product'] == 'TOTCRUDE') &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_0809, values = 'quantity', index = 'month', columns = ['description'], aggfunc='sum')
jodi_table_kbd_final = jodi_table_kbd[(jodi_table_kbd['product'] == 'TOTCRUDE') &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_final, values = 'quantity', index = 'month', columns = ['description'], aggfunc='sum')
list(jodi_table['description'].unique())
jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbd = jodi_table[jodi_table['unit'] == 'KBD']
list(jodi_table['product'].unique())
jodi_table['product']
jodi_table_kbd_final = jodi_table_kbd[(jodi_table_kbd['product'] == 'TOTCRUDE') &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_table_kbbl_final = jodi_table_kbbl[(jodi_table_kbbl['product'] == 'TOTCRUDE') &
                                     (jodi_table_kbbl['description'] == 'Crude Prodction')
                                     ]
jodi_table_kbd_final = jodi_table_kbd[(jodi_table_kbd['product'] == 'CRUDEOIL') &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
jodi_table_kbd_final = jodi_table_kbd[(jodi_table_kbd['product'] == 'CRUDEOIL') &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_final, values = 'quantity', index = 'month', columns = ['description'], aggfunc='sum')
list(jodi_table['product'].unique())
jodi_table_kbd_final = jodi_table_kbd[((jodi_table_kbd['product'] == 'CRUDEOIL') |
                                      (jodi_table_kbd['product'] == 'OTHERCRUDE'))
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_table_kbd_final = jodi_table_kbd[((jodi_table_kbd['product'] == 'CRUDEOIL') |
                                      (jodi_table_kbd['product'] == 'OTHERCRUDE')) &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_final, values = 'quantity', index = 'month', columns = ['description'], aggfunc='sum')
jodi_table_kbd_final = jodi_table_kbd[(jodi_table_kbd['product'].isin(['CRUDEOIL','OTHERCRUDE'])) &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_final, values = 'quantity', index = 'month', columns = ['description'], aggfunc='sum')
jodi_table_kbd_final = jodi_table_kbd[(jodi_table_kbd['product'].isin(['CRUDEOIL','OTHERCRUDE','NGL'])) &
                                     (jodi_table_kbd['description'] == 'Crude Prodction')
                                     ]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd_final, values = 'quantity', index = 'month', columns = ['description'], aggfunc='sum')
jodi_table[jodi_table['country'] == 'CHINA']
list(jodi_table['product'].unique())
jodi_connection.close()
iea_connection.close()
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE','NGL']))]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE','NGL']))]

jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE','NGL']))]
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE','NGL']))]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_table_kbbl
list(jodi_table['product'].unique())
list(jodi_table['description'].unique())
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))]

jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
world_sd = pd.concat([jodi_pivot_kbd[['Crude Prodction',
                                      'Refinery Runs',
                                      'Exports',
                                      'Imports',
                                      'Other sources',
                                      'Statistical Difference',
                                      'Products Transferred/Backfloes',
                                      'Direct Use']], 
                jodi_pivot_kbbl[['Closing Stock Level','Stock Change']]], ignore_index=False)
world_sd = pd.concat([jodi_pivot_kbd[['Crude Prodction',
                                      'Refinery Runs',
                                      'Exports',
                                      'Imports',
                                      'Other sources',
                                      'Statistical Difference',
                                      'Products Transferred/Backfloes',
                                      'Direct Use']], 
                jodi_pivot_kbbl[['Closing Stock Level','Stock Change']]], ignore_index=True)
world_sd = pd.concat([jodi_pivot_kbd[['Crude Prodction',
                                      'Refinery Runs',
                                      'Exports',
                                      'Imports',
                                      'Other sources',
                                      'Statistical Difference',
                                      'Products Transferred/Backfloes',
                                      'Direct Use']], 
                jodi_pivot_kbbl[['Closing Stock Level','Stock Change']]], axis=0)
world_sd = pd.concat([jodi_pivot_kbd[['Crude Prodction',
                                      'Refinery Runs',
                                      'Exports',
                                      'Imports',
                                      'Other sources',
                                      'Statistical Difference',
                                      'Products Transferred/Backfloes',
                                      'Direct Use']], 
                jodi_pivot_kbbl[['Closing Stock Level','Stock Change']]], axis=1)

## ---(Wed Jan 24 08:57:19 2018)---
import pandas as pd
import numpy as np

TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
med_data = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx')
grade_list = list(med_data['Grade'].unique())
print(grade_list)
import pandas as pd
import numpy as np
med_data = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx')
grade_list = list(med_data['Grade'].unique())
print(grade_list)
targo_vessel_list = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx', sheetname = 'Targo Vessel')
F31_vessel_list = list(med_data['Vessel'].unique())
F31_vessel_list = med_data['Vessel'].unique()
F31_vessel_list = pd.DataFrame(med_data['Vessel'].unique())
F31_vessel_list = pd.DataFrame(med_data['Vessel'].unique()).rename(columns={0:'Vessel'})
merged_list = pd.merge(F31_vessel_list,targo_vessel_list, how='left',indicator=True)
ToBeReplaced = merged_list[merged_list['_merge'] == 'left_only']
def cond_cpc(i):
    if i < 500:
        return 330
    elif 500 <= i  < 600:
        return 535
    elif 600 <= i < 720:
        return 670
    elif 720 <= i < 760:
        return 760
    else:
        return i

def cond_other(i):
    if i < 300:
        return 250
    elif 300 <= i  < 400:
        return 315
    elif 400 <= i < 500:
        return 450
    elif 500 <= i < 600:
        return 510
    elif 600 <= i < 700:
        return 630
    elif 700 <= i < 900:
        return 750
    elif 900 <= i <= 1200:
        return 1020
    elif i > 1200:
        return 2050
    else:
        return i

def standardize_cargos(cargo):
    if cargo['Grade'] == 'CPC BLEND':
        return cond_cpc(cargo['Quantity'])
    else:
        return cond_other(cargo['Quantity'])

def standardize_cargos(cargo):
    if cargo['Grade'] == 'CPC BLEND':
        return cond_cpc(cargo['Quantity'])
    else:
        return cond_other(cargo['Quantity'])

ToBeReplaced = ToBeReplaced['Vessel']
med_data['Vessel']
med_data['Vessel'].isin(ToBeReplaced)
ToBeReplaced = list(ToBeReplaced)
med_data['Vessel'].replace(ToBeReplaced,'')
med_data['Vessel'] = med_data['Vessel'].replace(ToBeReplaced,'')
med_data['Quantity'] = med_data.apply(standardize_cargos, axis=1)
targo_vessel_list = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx', sheetname = 'Targo Vessel')
grade_table = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx', sheetname = 'Port names')
med_data.Grade
med_data.loc[med_data.GRade == 'AZERI'] 
med_data.loc[med_data.Grade == 'AZERI'] 
for i,j in grade_table:
    med_data.loc[med_data.Grade == i, 'Load Location'] = j

grade_table
for i,j in grade_table.iterrows():
    med_data.loc[med_data.Grade == i, 'Load Location'] = j

for i,j in grade_table.iterrows():
    med_data.loc[med_data.Grade == i, 'Load Location'] = j

grade_table
for row in grade_table.iterrows():
    med_data.loc[med_data.Grade == row[0], 'Load Location'] = row[1]

for row in grade_table:
    med_data.loc[med_data.Grade == row[0], 'Load Location'] = row[1]

row[0]
row
grade_table
row
for row in grade_table:
    med_data.loc[med_data.Grade == row[0], 'Load Location'] = row[1]

med_data.loc[med_data.Grade == 'AZERI']
dfFiltered = med_data.loc[grade_table['Grade']]
dfFiltered = med_data[grade_table['Grade']]
dfFiltered = med_data[med_data['Grade']==list(grade_table['Grade'])]
dfFiltered = med_data.loc[grade_table['Grade']]
med_data[grade_table['Grade']]
dfFiltered = med_data[grade_table['Grade']]
grade_table['Grade']
med_data.Grade
dfFiltered = med_data.loc[med_data.Grade ==  grade_table['Grade']]
list(grade_table)
grade_table.iterrows()
for row in grade_table.iterrows():
    print(row)

for row in grade_table.iterrows():
    print(row[0])

print(row[1])
for row in grade_table.iterrows():
    print(row[2])

for row in grade_table.iterrows():
    print(row[0])

for row in grade_table.iterrows():
    print(row)

for i,j in grade_table.iterrows():
    print(i)
    print(j)

for i,j in grade_table.iterrows():
    print(i)
    print(j)
    med_data.loc[med_data.Grade == i, 'Load Location'] = j

list(grade_table['Grade'])
med_data.loc[list(grade_table['Grade'])]
dfFiltered = med_data[x where x['Grade'] in list(grade_table['Grade'])]
dfFiltered = med_data[x for x['Grade'] in list(grade_table['Grade'])]
med_data
dfFiltered = med_data.loc[med_data['Grade'] in list(grade_table['Grade'])]
for i,j in grade_table.iterrows():
    print(i)
    print(j)

for i,j in grade_table.iterrows():
    print(i)

for i,j in grade_table.iterrows():
    #print(i)
    print(j)

for i,j in grade_table.iterrows():
    #print(i)
    print(j[:,1])

for i,j in grade_table.iterrows():
    #print(i)
    print(j)

for i,j in grade_table.iterrows():
    #print(i)
    print(j)
    return j

for i,j in grade_table.iterrows():
    #print(i)
    print(j)

for i,j in grade_table.iterrows():
    #print(i)
    print(j)
    pritn(j.Grade)

for i,j in grade_table.iterrows():
    #print(i)
    print(j)
    print(j.Grade)

for i,j in grade_table.iterrows():
    #print(i)
    #print(j)
    print(j.Grade)

for i,j in grade_table.iterrows():
    #print(i)
    #print(j)
    print(j.Grade)
    print(j.Port)

for i,j in grade_table.iterrows():
    #print(i)
    #print(j)
    #print(j.Grade)
    #print(j.Port)
    
    med_data.loc[med_data.Grade == j.Grade, 'Load Location'] = j.Port

from datetime import datetime
med_data.loc[(med_data.Grade == 'CPC') & (med.data['Laycan From'] > datetime(2017,10,31)]
med.data['Laycan From'] > datetime(2017,10,31)
med_data['Laycan From'] > datetime(2017,10,31)
med_data.loc[(med_data.Grade == 'CPC') & (med_data['Laycan From'] > datetime(2017,10,31)]
med_data.loc[(med_data.Grade == 'CPC') & (med_data['Laycan From'] > datetime(2017,10,31))]
med_data.loc[(med_data.Grade == 'CPC Blend') & (med_data['Laycan From'] > datetime(2017,10,31))]
med_data.loc[(med_data.Grade == 'CPC BLEND') & (med_data['Laycan From'] > datetime(2017,10,31))]
cpc_nov = med_data.loc[(med_data.Grade == 'CPC BLEND') & (med_data['Laycan From'] > datetime(2017,10,31))]
med_data.to_excel(test_path+'CLEANED_MED.xlsx')
test_path='L:/TRADING/ANALYSIS/Python/MIMA/InputTargoStuff'
med_data.to_excel(test_path+'CLEANED_MED.xlsx')
test_path='L:/TRADING/ANALYSIS/Python/MIMA/InputTargoStuff/'
med_data.to_excel(test_path+'CLEANED_MED.xlsx')
dfFil = med_data.loc[grade_table['Grade'].index,:]
med_data[med_data['Grade'].isin(grade_table['Grade'])]
list(grade_table['Grade'])
med_data.loc[list(grade_table['Grade']),:]
med_data = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx', sheetname = 'Sheet3')
import pandas as pd
import numpy as np
from datetime import datetime

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
med_data = pd.read_excel('L:\TRADING\ANALYSIS\Python\MIMA\InputTargoStuff\TARGO Upload History Med with Validation V2.xlsx', sheetname = 'Sheet3')
med_data = pd.read_excel'(M:/meddata.xlsx')
med_data = pd.read_excel('M:\meddata.xlsx')
import pandas as pd
import pypyodbc
from datetime import datetime

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=TradeTracker;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

query = """
        SELECT Assay.AssayID, Assay.Grade, Assay.API, Assay.Density
        FROM TradeTracker.dbo.Assay Assay
        """

crudes = pd.read_sql(query, cxn)
crude_api = 38.3

bt_factor = 1 / (141.5 / (131.5 + crude_api) * 0.159)
crude_api = 38.3

bt_factor = 1 / (141.5 / ((131.5 + crude_api) * 0.159))
crude_api = 38.3

bt_factor = 1 / (141.5 / (131.5 + crude_api) * 0.159)
crude_api = 38.3

bt_factor = 1 / ((141.5 / (131.5 + crude_api)) * 0.159)
def conversion_to_BT(df):
    crudes.bt = 1 / ((141.5 / (131.5 + crudes.api)) * 0.159) 

def conversion_to_BT(df):
    1 / ((141.5 / (131.5 + crudes.api)) * 0.159)    

def conversion_to_BT():
    1 / ((141.5 / (131.5 + crudes.api)) * 0.15

def conversion_to_BT():
    1 / ((141.5 / (131.5 + crudes.api)) * 0.159) 

crudes.bt = crudes.apply(conversion_to_BT, axis=1)
def conversion_to_BT(crudes):
    1 / ((141.5 / (131.5 + crudes.api)) * 0.159)    


crudes.bt = crudes.apply(conversion_to_BT, axis=1)
def conversion_to_BT(crudes):
    1 / ((141.5 / (131.5 + crudes.api)) * 0.159)   

crudes['bt'] = crudes.apply(conversion_to_BT, axis=1)
def conversion_to_BT(crudes):
    1 / ((141.5 / (131.5 + crudes['api'])) * 0.159)    


crudes['bt'] = crudes.apply(conversion_to_BT, axis=1)
def conversion_to_BT(crudes):
    return 1 / ((141.5 / (131.5 + crudes['api'])) * 0.159)    


crudes['bt'] = crudes.apply(conversion_to_BT, axis=1)
import pandas as pd
import pypyodbc
from datetime import datetime

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=TradeTracker;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

query = """
        SELECT Assay.AssayID, Assay.Grade, Assay.API, Assay.Density
        FROM TradeTracker.dbo.Assay Assay
        """

crudes = pd.read_sql(query, cxn)

def conversion_to_BT(crudes):
    return 1 / ((141.5 / (131.5 + crudes['api'])) * 0.159)    


crudes['bt'] = crudes.apply(conversion_to_BT, axis=1)
cxn.close()
import pandas as pd
import pypyodbc

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=TradeTracker;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

query = """
        SELECT Assay.AssayID, Assay.Grade, Assay.API, Assay.Density
        FROM TradeTracker.dbo.Assay Assay
        """

crudes = pd.read_sql(query, cxn)

def conversion_to_BT(crudes):
    return 1 / ((141.5 / (131.5 + crudes['api'])) * 0.159)    


crudes['bt'] = crudes.apply(conversion_to_BT, axis=1)
cxn.close()


path='L:/TRADING/ANALYSIS/Python/MIMA/'
crudes.to_excel(path+'grade_densities.xlsx')

## ---(Tue Jan 30 09:19:34 2018)---
import pypyodbc
import pandas as pd
from datetime import datetime

iea_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=IEAData;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')


iea_query = """
                SELECT CrudeData.Country, CrudeData.Product, Balance.[Desc], CrudeData.Period, CrudeData.Quantity, CrudeData.Asof, CrudeData.PeriodType
                FROM IEAData.dbo.CrudeData CrudeData
                INNER JOIN IEAData.dbo.Balance Balance ON CrudeData.Balance = Balance.Flow
                ORDER BY CrudeData.Country
                """
jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               """               


iea_table = pd.read_sql(iea_query, iea_connection)
jodi_table = pd.read_sql(jodi_query, jodi_connection)

jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))]

jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')


jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
import pypyodbc
import pandas as pd
from datetime import datetime

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)

jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))]

jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD') & (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')


jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = 'description', aggfunc='sum')
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')&
                             (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))&
                             (jodi_table['country'].isin(NWECountries))]
NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')&
                             (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))&
                             (jodi_table['country'].isin(NWECountries))]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')&
                            (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))&
                            (jodi_table['country'].isin(NWECountries))]
NWECountriesCAP = map(lambda x: x.upper(), NWECountries)
NWECountriesCAP
NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')&
                             (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))&
                             (jodi_table['country'].isin(NWECountriesCAP))]

jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')&
                            (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))&
                            (jodi_table['country'].isin(NWECountriesCAP))]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')


jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
import pypyodbc
import pandas as pd
from datetime import datetime

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)

jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')&
                             (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))&
                             (jodi_table['country'].isin(NWECountriesCAP))]

jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')&
                            (jodi_table['product'].isin(['CRUDEOIL','OTHERCRUDE']))&
                            (jodi_table['country'].isin(NWECountriesCAP))]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')


jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbd.query('description == ["Crude Production"]')
jodi_pivot_kbd.columns
jodi_pivot_kbd[('Crude Production')]
jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """  
jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]

jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum',margins=True)
jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')


jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbbl.columns
jodi_pivot_kbd[('Closing Stock Level','Statistical Difference', 'Stock Change')]
jodi_pivot_kbd[('Closing Stock Level'),('Statistical Difference'), ('Stock Change')]
jodi_pivot_kbd[('Closing Stock Level')&('Statistical Difference')&('Stock Change')]
jodi_pivot_kbd[('Closing Stock Level')]
jodi_pivot_kbbl[('Closing Stock Level')]
jodi_pivot_kbbl.columns
jodi_pivot_kbbl.loc[:,(slice('Closing Stock Level','Statistical Difference', 'Stock Change'))]
jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level','Statistical Difference', 'Stock Change'],:]]
idx = pd.IndexSlice
jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level','Statistical Difference', 'Stock Change'],:]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level','Statistical Difference', 'Stock Change'],:]]
jodi_kbbl_data.head()     
jodi_pivot_kbd.columns
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[['Crude Production', 'Exports', 'Imports', 'Refinery Runs'],:]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level','Statistical Difference', 'Stock Change'],:]]
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[['Crude Production', 'Exports', 'Imports', 'Refinery Runs'],:]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level', 'Stock Change'],:]]
jodi_kbbl_data.head()
joined_table = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
joined_table.head()
tab_tots = joined_table.groupby(level='description').sum()
tab_tots.index = [tab_tots.index, ['Total'] * len(tab_tots)]
print(tab_tots)
tab_tots = joined_table.groupby(level='description', axis=1).sum()
tab_tots = joined_table.groupby(level='description', axis=1).sum()
tab_tots.index = [tab_tots.index, ['Total'] * len(tab_tots)]
print(tab_tots)
tab_tots = joined_table.groupby(level='description', axis=1).sum()
print(tab_tots)
import pypyodbc
import pandas as pd
from datetime import datetime

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[['Crude Production', 'Exports', 'Imports', 'Refinery Runs'],:]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level', 'Stock Change'],:]]

NWE_Countries = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total = NWE_Countries.groupby(level='description', axis=1).sum()
import pypyodbc
import pandas as pd
from datetime import datetime

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                AND  (RefineryStatusInPeriod.[UnitType] ='CRUDE' OR RefineryStatusInPeriod.[UnitType] ='CONDENSATE DISTILLATION'  )
                '''

refdb_table = pd.read_sql(refdb_query, refdb_connection)               



import pypyodbc
import pandas as pd
from datetime import datetime

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                --AND  (RefineryStatusInPeriod.[UnitType] ='CRUDE' OR RefineryStatusInPeriod.[UnitType] ='CONDENSATE DISTILLATION'  )
                '''

refdb_table = pd.read_sql(refdb_query, refdb_connection)   
import pypyodbc
import pandas as pd
from datetime import datetime

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                AND PeriodType == 'Month'
                --AND  (RefineryStatusInPeriod.[UnitType] ='CRUDE' OR RefineryStatusInPeriod.[UnitType] ='CONDENSATE DISTILLATION'  )
                '''

refdb_table = pd.read_sql(refdb_query, refdb_connection) 
import pypyodbc
import pandas as pd
from datetime import datetime

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                AND PeriodType == 'Month'
                '''
refdb_table = pd.read_sql(refdb_query, refdb_connection)     
import pypyodbc
import pandas as pd
from datetime import datetime

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                AND PeriodType = 'Month'
                '''

refdb_table = pd.read_sql(refdb_query, refdb_connection)       
refdb_pivot = pd.pivot_table(refdb_table, values = 'Amount', index='Period', columns = ['UnitType','Country'], aggfunc='sum')
refdb_pivot = pd.pivot_table(refdb_table, values = 'amount', index='Period', columns = ['UnitType','Country'], aggfunc='sum')

refdb_pivot = pd.pivot_table(refdb_table, values = 'amount', index='period', columns = ['UnitType','Country'], aggfunc='sum')

refdb_pivot = pd.pivot_table(refdb_table, values = 'amount', index='period', columns = ['unittype','country'], aggfunc='sum')

import pypyodbc
import pandas as pd
from datetime import datetime

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                AND PeriodType = 'Month'
                AND Status = 'Avail Capacity'
                '''

refdb_table = pd.read_sql(refdb_query, refdb_connection)               

refdb_pivot = pd.pivot_table(refdb_table, values = 'amount', index='period', columns = ['unittype','country'], aggfunc='sum')

mike = pd.Series(refdb_table['Country'].unique())
mike = pd.Series(refdb_table['country'].unique())
import pypyodbc
import pandas as pd
from datetime import datetime

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[['Crude Production', 'Exports', 'Imports', 'Refinery Runs'],:]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level', 'Stock Change'],:]]

NWE_Countries = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total = NWE_Countries.groupby(level='description', axis=1).sum()
import pypyodbc
import pandas as pd
from datetime import datetime

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                AND PeriodType = 'Month'
                AND Status = 'Avail Capacity'
                '''

refdb_table = pd.read_sql(refdb_query, refdb_connection)               

refdb_pivot = pd.pivot_table(refdb_table, values = 'amount', index='period', columns = ['unittype','country'], aggfunc='sum')
refdb_pivot = pd.pivot_table(refdb_table, values = 'amount', index='period', columns = ['country','unittype'], aggfunc='sum')
refdb_pivot
mike = pd.Series(refdb_table['unittype'].unique())
pd.Series(refdb_table['unittype'].unique())
NWE_Total_ref = refdb_pivot.groupby(level='unittype', axis=1).sum()
import pypyodbc
import pandas as pd
from datetime import datetime

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['description','country'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[['Crude Production', 'Exports', 'Imports', 'Refinery Runs'],:]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level', 'Stock Change'],:]]

NWE_Countries = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund = NWE_Countries.groupby(level='description', axis=1).sum()

import pypyodbc
import pandas as pd


clipper_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=TradeTracker;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

clipper_query = '''
                SELECT
                Cargoes."Vessel Name",
                Cargoes."Vessel IMO",
                Cargoes."Grade",
                Cargoes.Volume,
                Cargoes."Cargo Type",
                Cargoes.Shipper,
                Cargoes."Load Date",
                Cargoes."Load Port",
                Cargoes."Discharge Port",
                Cargoes."Clipper Probability",
                Cargoes."Load Asset",
                Cargoes."Discharge Date",
                Cargoes.Refiner,
                Cargoes.API,
                Cargoes.Sulphur,
                Cargoes."LoadMth",
                Cargoes.DischargeMth,
                Cargoes.LoadRegion,
                Cargoes.LoadCountry,
                Cargoes. dischargeRegion,
                Cargoes.DischargeCountry,
                Cargoes.DefaultRefinery,
                Cargoes."SOCAR Default Buyer",
                Cargoes."Default Grade",
                Cargoes.Owner,
                Cargoes.Refinery,
                Cargoes."Vessel Size",
                Cargoes."SOCAR Volume",
                Cargoes.NewVol,
                Cargoes.Export,
                Cargoes.VT,
                Cargoes.LoadDecade,
                Cargoes.DischargeDecade,
                Cargoes.Route,
                Cargoes.LoadSTSVessel,
                Cargoes.OfftakeSTSVessel
                
                
                FROM TradeTracker.dbo.CleanerDataClipper AS Cargoes
                WHERE Cargoes."Load Date" >=? AND Cargoes."Load Date" <=?
                '''

cliper_table = pd.read_sql(clipper_query, clipper_connection) 
import pypyodbc
import pandas as pd


clipper_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=TradeTracker;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

clipper_query = '''
                SELECT
                Cargoes."Vessel Name",
                Cargoes."Vessel IMO",
                Cargoes."Grade",
                Cargoes.Volume,
                Cargoes."Cargo Type",
                Cargoes.Shipper,
                Cargoes."Load Date",
                Cargoes."Load Port",
                Cargoes."Discharge Port",
                Cargoes."Clipper Probability",
                Cargoes."Load Asset",
                Cargoes."Discharge Date",
                Cargoes.Refiner,
                Cargoes.API,
                Cargoes.Sulphur,
                Cargoes."LoadMth",
                Cargoes.DischargeMth,
                Cargoes.LoadRegion,
                Cargoes.LoadCountry,
                Cargoes. dischargeRegion,
                Cargoes.DischargeCountry,
                Cargoes.DefaultRefinery,
                Cargoes."SOCAR Default Buyer",
                Cargoes."Default Grade",
                Cargoes.Owner,
                Cargoes.Refinery,
                Cargoes."Vessel Size",
                Cargoes."SOCAR Volume",
                Cargoes.NewVol,
                Cargoes.Export,
                Cargoes.VT,
                Cargoes.LoadDecade,
                Cargoes.DischargeDecade,
                Cargoes.Route,
                Cargoes.LoadSTSVessel,
                Cargoes.OfftakeSTSVessel
                
                
                FROM TradeTracker.dbo.CleanerDataClipper AS Cargoes
                '''

cliper_table = pd.read_sql(clipper_query, clipper_connection) 
unique_loadregion = cliper_table['LoadRegion'].unique()
unique_loadregion = cliper_table['loadregion'].unique()
unique_loadregion = list(cliper_table['loadregion'].unique())
unique_dischargeregion = list(cliper_table['dischargeregion'].unique())
cliper_table[cliper_table['loadregion'] == 'NWE']
cliper_table['loadregion']
cliper_table
cliper_table['loadregion'] == 'NWE'
cliper_table['loadregion'] == 'AG'
cliper_table[cliper_table['loadregion'] == 'AG']
cliper_table[cliper_table['loadregion'].str.rstrip() == 'AG']
import pypyodbc
import pandas as pd


clipper_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=TradeTracker;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

clipper_query = '''
                SELECT
                Cargoes."Vessel Name",
                Cargoes."Vessel IMO",
                Cargoes."Grade",
                Cargoes.Volume,
                Cargoes."Cargo Type",
                Cargoes.Shipper,
                Cargoes."Load Date",
                Cargoes."Load Port",
                Cargoes."Discharge Port",
                Cargoes."Clipper Probability",
                Cargoes."Load Asset",
                Cargoes."Discharge Date",
                Cargoes.Refiner,
                Cargoes.API,
                Cargoes.Sulphur,
                Cargoes."LoadMth",
                Cargoes.DischargeMth,
                Cargoes.LoadRegion,
                Cargoes.LoadCountry,
                Cargoes. dischargeRegion,
                Cargoes.DischargeCountry,
                Cargoes.DefaultRefinery,
                Cargoes."SOCAR Default Buyer",
                Cargoes."Default Grade",
                Cargoes.Owner,
                Cargoes.Refinery,
                Cargoes."Vessel Size",
                Cargoes."SOCAR Volume",
                Cargoes.NewVol,
                Cargoes.Export,
                Cargoes.VT,
                Cargoes.LoadDecade,
                Cargoes.DischargeDecade,
                Cargoes.Route,
                Cargoes.LoadSTSVessel,
                Cargoes.OfftakeSTSVessel
                
                
                FROM TradeTracker.dbo.CleanerDataClipper AS Cargoes
                '''

clipper_table = pd.read_sql(clipper_query, clipper_connection) 
unique_loadregion = list(clipper_table['loadregion'].unique())
unique_dischargeregion = list(clipper_table['dischargeregion'].unique())
clipper_table['journeytime'] = clipper_table['load date'] - clipper_table['discharge date']
vt_table = pd.pivot_table(clipper_table, values = 'vt', index='loadregion', columns = 'dischargeregion', aggfunc='mean')
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
import pypyodbc
import pandas as pd

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[['Crude Production', 'Exports', 'Imports', 'Refinery Runs'],:]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[['Closing Stock Level', 'Stock Change'],:]]

NWE_Countries = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund = NWE_Countries.groupby(level='description', axis=1).sum()
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund = NWE_Countries.groupby(level='description', axis=1).sum()
NWE_Countries
NWE_Countries.head()
import pypyodbc
import pandas as pd

refdb_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

refdb_query = '''
                SELECT RefineryStatusInPeriod.Region, RefineryStatusInPeriod.SubRegion, Country.Country, RefineryStatusInPeriod.RefineryName, RefineryStatusInPeriod.Status, RefineryStatusInPeriod.UnitName, RefineryStatusInPeriod.UnitType, RefineryStatusInPeriod.PeriodType, RefineryStatusInPeriod.Period, RefineryStatusInPeriod.Amount
                FROM RefineryInfo.dbo.RefineryStatusInPeriod RefineryStatusInPeriod
                LEFT JOIN RefineryInfo.dbo.Refinery Refinery
                ON  RefineryStatusInPeriod.RefineryName = Refinery.RefineryName
                INNER JOIN Country Country
                ON Refinery.Country = Country.CountryID
                WHERE (RefineryStatusInPeriod.Region='NW EUROPE')
                AND PeriodType = 'Month'
                AND Status = 'Avail Capacity'
                '''

refdb_table = pd.read_sql(refdb_query, refdb_connection) 
refdb_pivot = pd.pivot_table(refdb_table, values = 'amount', index='period', columns = ['unittype', 'country'], aggfunc='sum')
NWE_Total_ref = refdb_pivot.groupby(level='unittype', axis=1).sum()
NWE_Total_ref.head()
NWE_Countries.head()
jodi_products_query = """
                   SELECT TOP 1000 Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()

NWE_Countries_products.head()
import pypyodbc
import pandas as pd

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')
jodi_products_query = """
                   SELECT TOP 1000 Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()

NWE_Countries_products.head()
jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()
idx = pd.IndexSlice
jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()

NWE_Countries_products.head()
jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:,:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()

NWE_Countries_products.head()
NWE_Total_fund_products.to_excel(path+'NWE_Total_fund_products.xlsx')
path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Total_fund_products.to_excel(path+'NWE_Total_fund_products.xlsx')
path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Total_fund_products.xlsx')
path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')
NWE_Countries_products.columns = NWE_Countries_products.columns.to_series().str.join('_')    
NWE_Countries_products.columns = NWE_Countries_products.columns.to_series().str.join('_')  
NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
import pypyodbc
import pandas as pd
idx = pd.IndexSlice

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:,:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund = NWE_Countries.groupby(level='description', axis=1).sum()

NWE_Countries.head()
import pypyodbc
import pandas as pd
idx = pd.IndexSlice

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_crude = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund_crude = NWE_Countries_crude.groupby(level='description', axis=1).sum()

NWE_Countries_crude.head()



# =============================================================================
# For Products
# =============================================================================

jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:,:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()

NWE_Countries_products.head()

path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')



# =============================================================================
# Now flaten the data in order to use in dataframe for testing
# =============================================================================

NWE_Countries_products.columns = NWE_Countries_products.columns.to_series().str.join('_')                
NWE_Total_fund_products.columns = NWE_Total_fund_products.columns.to_series().str.join('_')                      
NWE_Countries_crude.columns = NWE_Countries_crude.columns.to_series().str.join('_')                
NWE_Total_fund_crude.columns = NWE_Total_fund_crude.columns.to_series().str.join('_')                      

path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')
NWE_Total_fund_products.to_excel(path+'NWE_Total_fund_products.xlsx') 
NWE_Countries_crude.to_excel(path+'NWE_Countries_crude.xlsx') 
NWE_Total_fund_crude.to_excel(path+'NWE_Total_fund_crude.xlsx')                    
import pypyodbc
import pandas as pd
idx = pd.IndexSlice

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_crude = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund_crude = NWE_Countries_crude.groupby(level='description', axis=1).sum()

NWE_Countries_crude.head()



# =============================================================================
# For Products
# =============================================================================

jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:,:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()

NWE_Countries_products.head()

path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')



# =============================================================================
# Now flaten the data in order to use in dataframe for testing
# =============================================================================

NWE_Countries_products.columns = NWE_Countries_products.columns.to_series().str.join('_')                
NWE_Total_fund_products.columns = NWE_Total_fund_products.columns.to_series().str.join('_')                      
NWE_Countries_crude.columns = NWE_Countries_crude.columns.to_series().str.join('_')                
NWE_Total_fund_crude.columns = NWE_Total_fund_crude.columns.to_series().str.join('_')                      

path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')
NWE_Total_fund_products.to_excel(path+'NWE_Total_fund_products.xlsx') 
NWE_Countries_crude.to_excel(path+'NWE_Countries_crude.xlsx') 
NWE_Total_fund_crude.to_excel(path+'NWE_Total_fund_crude.xlsx')                    
import pypyodbc
import pandas as pd
idx = pd.IndexSlice

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_crude = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund_crude = NWE_Countries_crude.groupby(level='description', axis=1).sum()


# =============================================================================
# For Products
# =============================================================================

jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:,:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()


# =============================================================================
# Now flaten the data in order to use in dataframe for testing
# =============================================================================

NWE_Countries_products.columns = NWE_Countries_products.columns.to_series().str.join('_')                
NWE_Total_fund_products.columns = NWE_Total_fund_products.columns.to_series().str.join('_')                      
NWE_Countries_crude.columns = NWE_Countries_crude.columns.to_series().str.join('_')                
NWE_Total_fund_crude.columns = NWE_Total_fund_crude.columns.to_series().str.join('_')                      

path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')
NWE_Total_fund_products.to_excel(path+'NWE_Total_fund_products.xlsx') 
NWE_Countries_crude.to_excel(path+'NWE_Countries_crude.xlsx') 
NWE_Total_fund_crude.to_excel(path+'NWE_Total_fund_crude.xlsx')  
med_data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
import pandas as pd
import numpy as np
from datetime import datetime
med_data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')

df = df.loc[(df['Discharge Date']>df['Load Date'].shift(-1)) | df['Load Date']<df['Discharge Date'].shift(1)]
df = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')

df = df.loc[(df['Discharge Date']>df['Load Date'].shift(-1)) | (df['Load Date']<df['Discharge Date'].shift(1))]
df = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
sorted_df = df.sort_values(by=['Vessel'])
sorted_df = sorted_df.loc[(sorted_df['Discharge Date']>sorted_df['Load Date'].shift(-1)) | (sorted_df['Load Date']<sorted_df['Discharge Date'].shift(1))]
sorted_df = df.sort_values(by=['Vessel']).rest_index(drop=True)
sorted_df = df.sort_values(by=['Vessel']).reset_index(drop=True)
sorted_df = df.sort_values(by=('Vessel','Load Date')).reset_index(drop=True)
sorted_df = df.sort_values(by=(['Vessel','Load Date'])).reset_index(drop=True)
sorted_df = df.sort_values(by=['Vessel','Load Date']).reset_index(drop=True)
sorted_df = df.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)
sorted_df2 = sorted_df.loc[(sorted_df['Discharge Date']>sorted_df['Load Date'].shift(-1)) | (sorted_df['Load Date']<sorted_df['Discharge Date'].shift(1))]
sorted_df.loc[(sorted_df['Vessel']==sorted_df['Vessel'].shift(-1)) | (sorted_df['Vessel']==sorted_df['Vessel'].shift(1))]
vessel_filtered = sorted_df.loc[(sorted_df['Vessel']==sorted_df['Vessel'].shift(-1)) | (sorted_df['Vessel']==sorted_df['Vessel'].shift(1))]
df = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
sorted_df = df.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)
vessel_filtered = sorted_df.loc[(sorted_df['Vessel']==sorted_df['Vessel'].shift(-1)) | (sorted_df['Vessel']==sorted_df['Vessel'].shift(1))]
sorted_df2 = vessel_filtered.loc[(vessel_filtered['Discharge Date']>vessel_filtered['Load Date'].shift(-1)) | (vessel_filtered['Load Date']>vessel_filtered['Discharge Date'].shift(1))]
sorted_df2 = vessel_filtered.loc[(vessel_filtered['Discharge Date']>vessel_filtered['Load Date'].shift(-1)) | (vessel_filtered['Load Date']<vessel_filtered['Discharge Date'].shift(1))]
sorted_df = df.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)
sorted_df2 = sorted_df.loc[(sorted_df['Discharge Date']>sorted_df['Load Date'].shift(-1)) |
        (sorted_df['Load Date']<sorted_df['Discharge Date'].shift(1))]
sorted_df['Load Date']
sorted_df['Load Date']+1
from datetime import timedelta
sorted_df['Adjusted'] = sorted_df['Load Date']+timdedelta(days=2)
sorted_df['Adjusted'] = sorted_df['Load Date']+timedelta(days=2)
sorted_df3 = sorted_df2.loc[(sorted_df2['Load Date']<sorted_df2['Load Date'].shift(-1)+timedelta(days=5))]
df = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
sorted_df = df.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

sorted_df2 = sorted_df.loc[(sorted_df['Discharge Date']>sorted_df['Load Date'].shift(-1)) |
        (sorted_df['Load Date']<sorted_df['Discharge Date'].shift(1))]

sorted_df3 = sorted_df2.loc[(sorted_df2['Load Date']<sorted_df2['Load Date'].shift(-1)+timedelta(days=3))]
df = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
sorted_df = df.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)
import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)
df['stem_before'] = df['Load Date'].shift(-1)
df['stem_after'] = df['Load Date'].shift(1)
unique_vessel = pd.DataFrame(df['Vessel']).unique()
unique_vessel = pd.DataFrame(df['Vessel'].unique())
unique_vessel = list[df['Vessel'].unique()]
df['Vessel'].unique()
unique_vessel = list(df['Vessel'].unique())
for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]

temp_df = df.loc[df['Vessel']=='Abliani']
stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    for index, row in temp_df.itterow():
        if row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append[row['CARGO ID']]
        elif row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append[row['CARGO ID']]

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    for index, row in temp_df.itterows():
        if row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append[row['CARGO ID']]
        elif row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append[row['CARGO ID']]

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    for index, row in temp_df.iterrows():
        if row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append[row['CARGO ID']]
        elif row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append[row['CARGO ID']]

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    print(i)

for index, row in temp_df.iterrows():
    print(row)

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        print(row)

stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        print(row)
        if row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])
        elif row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])
        elif row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])

mike = df[df['CARGO ID'].isin(stems_list)]
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(-1)
df['stem_after'] = df['Load Date'].shift(1)
unique_vessel = list(df['Vessel'].unique())

stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])
        elif row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])


mike = df[df['CARGO ID'].isin(stems_list)]
import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
unique_vessel = list(df['Vessel'].unique())

stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])
        elif row['stem_after'] > (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])


mike = df[df['CARGO ID'].isin(stems_list)]
import pypyodbc
import pandas as pd
idx = pd.IndexSlice

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_crude = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund_crude = NWE_Countries_crude.groupby(level='description', axis=1).sum()
import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
unique_vessel = list(df['Vessel'].unique())
stems_list = []
df['Vessel']
stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if row['stem_after'] < (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])
        elif row['stem_before'] > (row['Load Date'] - timedelta(days=5)):
            stems_list.append(row['CARGO ID'])

mike = df[df['CARGO ID'].isin(stems_list)]
import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
unique_vessel = list(df['Vessel'].unique())



stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if row['stem_after'] < (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])
        elif row['stem_before'] > (row['Load Date'] - timedelta(days=5)):
            stems_list.append(row['CARGO ID'])


mike = df[df['CARGO ID'].isin(stems_list)]
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
unique_vessel = list(df['Vessel'].unique())

sorted_df2 = df.loc[(df['Discharge Date']>df['Load Date'].shift(-1)) |
        (df['Load Date']<df['Discharge Date'].shift(1))]
import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
unique_vessel = list(df['Vessel'].unique())

df = df.loc[(df['Discharge Date']>df['Load Date'].shift(-1)) |
        (df['Load Date']<df['Discharge Date'].shift(1))]

stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if row['stem_after'] < (row['Load Date'] + timedelta(days=5)):
            stems_list.append(row['CARGO ID'])
        elif row['stem_before'] > (row['Load Date'] - timedelta(days=5)):
            stems_list.append(row['CARGO ID'])


mike = df[df['CARGO ID'].isin(stems_list)]
import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
df['vessel_before'] = df['Vessel'].shift(1)
df['vessel_after'] = df['Vessel'].shift(-1)
unique_vessel = list(df['Vessel'].unique())
import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
df['vessel_before'] = df['Vessel'].shift(1)
df['vessel_after'] = df['Vessel'].shift(-1)
unique_vessel = list(df['Vessel'].unique())

#df = df.loc[(df['Discharge Date']>df['Load Date'].shift(-1)) |
#        (df['Load Date']<df['Discharge Date'].shift(1))]

stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if( row['stem_after'] < (row['Load Date'] + timedelta(days=5))) and (row['vessel_after'] == (row['Vessel'])):
            stems_list.append(row['CARGO ID'])
        elif row['stem_before'] > (row['Load Date'] - timedelta(days=5)) and (row['vessel_before'] == (row['Vessel'])):
            stems_list.append(row['CARGO ID'])


mike = df[df['CARGO ID'].isin(stems_list)]
import pandas as pd
import numpy as np
import pypyodbc
from datetime import datetime
from datetime import timedelta
import pandas as pd
import numpy as np
import pypyodbc
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

vcn = pypyodbc.connect('DRIVER=SQL Server Native Client 11.0;SERVER=STCHGS112;UID=gami;Trusted_Connection=Yes;APP=Microsoft Office 2010;WSID=STUKLW048;DATABASE=STG_Targo;')

vcn_query = '''
                SELECT distinct ShipDetails.Name Vessel, Ship.IMO IMO, ShipDetails.ValidUntil, Ship.ShipClass
                FROM STG_Targo.dbo.ShipDetails ShipDetails
                INNER JOIN Ship Ship
                ON Ship.Id = ShipDetails.IdShip
                ORDER BY Name
                '''

vcn_table= pd.read_sql(vcn_query, vcn)
vcn_table[vcn_table['vessel'] == mike['Vessel']]
vcn_table['vessel']
vcn_table.loc[vcn_table['vessel'] == mike['Vessel']]
i = 'Abliani'
x = vcn_table.loc[vcn_table['vessel'] ==i, vcn_table['shipclass'] ]
vcn_table['vessel'] =='Abliani'
vcn_table.loc[vcn_table['vessel'] =='Abliani', 'shipclass']
vcn_table.loc[vcn_table['vessel'].isin(mike['Vessel']), 'shipclass']
mike['Size'] = vcn_table.loc[vcn_table['vessel'].isin(mike['Vessel']), 'shipclass']
import pandas as pd
import numpy as np
import pypyodbc
from datetime import datetime
from datetime import timedelta

#TARGO_DATA = pd.read_excel('L://TRADING//ANALYSIS//BALANCES//NWE BALANCE.xlsm', sheetname = 'TARGO Stems')
data = pd.read_excel('M:\Med_History.xlsx', sheetname = 'Sheet1')
df = data.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)

vcn = pypyodbc.connect('DRIVER=SQL Server Native Client 11.0;SERVER=STCHGS112;UID=gami;Trusted_Connection=Yes;APP=Microsoft Office 2010;WSID=STUKLW048;DATABASE=STG_Targo;')

vcn_query = '''
                SELECT distinct ShipDetails.Name Vessel, Ship.IMO IMO, ShipDetails.ValidUntil, Ship.ShipClass
                FROM STG_Targo.dbo.ShipDetails ShipDetails
                INNER JOIN Ship Ship
                ON Ship.Id = ShipDetails.IdShip
                ORDER BY Name
                '''

vcn_table= pd.read_sql(vcn_query, vcn)

df['stem_before'] = df['Load Date'].shift(1)
df['stem_after'] = df['Load Date'].shift(-1)
df['vessel_before'] = df['Vessel'].shift(1)
df['vessel_after'] = df['Vessel'].shift(-1)
unique_vessel = list(df['Vessel'].unique())

#df = df.loc[(df['Discharge Date']>df['Load Date'].shift(-1)) |
#        (df['Load Date']<df['Discharge Date'].shift(1))]

stems_list = []

for i in unique_vessel:
    temp_df = df.loc[df['Vessel']==i]
    #print(i)
    for index, row in temp_df.iterrows():
        #print(row)
        if( row['stem_after'] < (row['Load Date'] + timedelta(days=5))) and (row['vessel_after'] == (row['Vessel'])):
            stems_list.append(row['CARGO ID'])
        elif row['stem_before'] > (row['Load Date'] - timedelta(days=5)) and (row['vessel_before'] == (row['Vessel'])):
            stems_list.append(row['CARGO ID'])


mike = df[df['CARGO ID'].isin(stems_list)]
vcn_table.loc[vcn_table['vessel'].isin(mike['Vessel']), 'shipclass']
mike['Size'] = vcn_table.loc[vcn_table['vessel'].isin(mike['Vessel']), 'shipclass']
vcn_table['vessel'].isin(mike['Vessel'])
mike.columns
mike.rename(columns={'Vessel;':'vessel'}
mike.rename(columns={'Vessel;':'vessel'})
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)
merged = pd.merge(mike, vcn_table, on=['vessel'], how='left', indicator=True)
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)
merged = pd.merge(mike, vcn_table, on=mike['vessel'], how='left', indicator=True)
mike.rename(columns={'Vessel':'vessel'})
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)
mike.columns
mike.rename(columns={'Vessel':'vessel'})
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)
mike = mike.rename(columns={'Vessel':'vessel'})
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)
mike = df[df['CARGO ID'].isin(stems_list)]
mike = mike.rename(columns={'Vessel':'vessel'})
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)

mike = df[df['CARGO ID'].isin(stems_list)]
mike = mike.rename(columns={'Vessel':'vessel'})
pd.Series(df['Vessel'].unique()
(df['Vessel'].unique())
(pd.Series(df['Vessel'].unique()))
(list(pd.Series(df['Vessel'].unique())))
vcn_table[vcn_table['vessel'].isin(list(pd.Series(df['Vessel'].unique()))), ['vessel','shipclass']]
vcn_table[vcn_table['vessel'].isin(list(pd.DataFrame(df['Vessel'].unique()))), ['vessel','shipclass']]
vcn_table[vcn_table['vessel'].isin(list(pd.DataFrame(df['Vessel'].unique()))), ('vessel','shipclass')]
vcn_table['vessel']
(list(pd.DataFrame(df['Vessel'].unique())))
vcn_table[vcn_table['vessel'].isin(list(pd.DataFrame(df['Vessel'].unique()))), 'vessel']
list(pd.DataFrame(df['Vessel'].unique()))
pd.DataFrame(df['Vessel'].unique()
vcn_table[vcn_table['vessel'].isin(unique_vessel_list), 'vessel']
unique_vessel_list = list(pd.DataFrame(df['Vessel'].unique()))
vcn_table[vcn_table['vessel'].isin(unique_vessel_list), 'vessel']
vcn_table[vcn_table['vessel'].isin(unique_vessel_list), 'vessel']
vcn_table['vessel']
vcn_table.loc[vcn_table['vessel'].isin(unique_vessel_list), 'vessel']
vcn_table.loc[vcn_table['vessel'].isin(unique_vessel_list), ['vessel','category']]
vcn_table.loc[vcn_table['vessel'].isin(unique_vessel_list), ('vessel','shipclass')]
unique_vessel_list = list(pd.DataFrame(df['Vessel'].unique()))
vcn_table['vessel'].isin(unique_vessel_list)
unique_vessel_list
df['Vessel']
list(pd.DataFrame(df['Vessel'].unique()))
unique_vessel_list = list(pd.DataFrame(df['Vessel'].unique()))
df['Vessel'].unique()
pd.DataFrame(df['Vessel'].unique())
list(pd.DataFrame(df['Vessel'].unique()))
unique_vessel_list = pd.DataFrame(df['Vessel'].unique())
unique_vessel_list = list(df['Vessel'].unique())
vcn_table.loc[vcn_table['vessel'].isin(unique_vessel_list), ('vessel','shipclass')]
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)
merged2 = merged.sort_values(by=['Vessel','Load Date'], ascending=True).reset_index(drop=True)
merged2 = merged.sort_values(by=['vessel','Load Date'], ascending=True).reset_index(drop=True)
merged2.columns
cols = ['Cargo ID', 'Grade', 'Quantity','shipclass','Load Date','Destination Location', 'Discharge Date','CARGO ID', 'imo']
merged2 = merged[cols].sort_values(by=['vessel','Load Date'], ascending=True).reset_index(drop=True)
merged2.columns
cols = ['Cargo ID', 'Grade','vessel','Quantity','shipclass','Load Date','Destination Location', 'Discharge Date','CARGO ID', 'imo']
merged = pd.merge(mike, vcn_table, on='vessel', how='left', indicator=True)
merged2 = merged[cols].sort_values(by=['vessel','Load Date'], ascending=True).reset_index(drop=True)
merged = pd.merge(mike, vcn_table[['vessel','shipclass']], on='vessel', how='left', indicator=True)
cols = ['Cargo ID', 'Grade','vessel','Quantity','shipclass','Load Date','Destination Location', 'Discharge Date','CARGO ID', 'imo']
merged2 = merged[cols].sort_values(by=['vessel','Load Date'], ascending=True).reset_index(drop=True)
cols = ['Cargo ID', 'Grade','vessel','Quantity','shipclass','Load Date','Destination Location', 'Discharge Date','CARGO ID']
merged2 = merged[cols].sort_values(by=['vessel','Load Date'], ascending=True).reset_index(drop=True)
path='L:/TRADING/ANALYSIS/Python/MIMA/'
merged2.to_excel(path+'merged2.xlsx')

## ---(Wed Feb  7 08:27:45 2018)---
import pypyodbc
import pandas as pd
idx = pd.IndexSlice

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_crude = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund_crude = NWE_Countries_crude.groupby(level='description', axis=1).sum()


# =============================================================================
# For Products
# =============================================================================

jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:,:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()


# =============================================================================
# Now flaten the data in order to use in dataframe for testing
# =============================================================================

NWE_Countries_products.columns = NWE_Countries_products.columns.to_series().str.join('_')                
NWE_Total_fund_products.columns = NWE_Total_fund_products.columns.to_series().str.join('_')                      
NWE_Countries_crude.columns = NWE_Countries_crude.columns.to_series().str.join('_')                
NWE_Total_fund_crude.columns = NWE_Total_fund_crude.columns.to_series().str.join('_')                      

path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')
NWE_Total_fund_products.to_excel(path+'NWE_Total_fund_products.xlsx') 
NWE_Countries_crude.to_excel(path+'NWE_Countries_crude.xlsx') 
NWE_Total_fund_crude.to_excel(path+'NWE_Total_fund_crude.xlsx')  
import pypyodbc
import pandas as pd
idx = pd.IndexSlice

jodi_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=JodiOil;'
                                'uid=gami;'
                                'Trusted_Connection=Yes;')

#NWECountries = ['Belgium','Denmark','Finland','France','Germany','Ireland','Netherlands','Norway','Poland','Sweden','United Kingdom']
#NWECountriesCAP = list(map(lambda x: x.upper(), NWECountries))


jodi_query = """
               SELECT Countries.Country, PrimaryTable.Product, WorldPrimaryFLows.Description, PrimaryTable.Unit, CONVERT(datetime, CONCAT(PrimaryTable.Month,'-01'), 121) as Month, PrimaryTable.Quantity, PrimaryTable.StatusCode
               FROM JodiOil.dbo.PrimaryTable PrimaryTable 
               INNER JOIN JodiOil.dbo.Countries Countries ON PrimaryTable.Country = Countries.[Country Code]
               INNER JOIN JodiOil.dbo.WorldPrimaryFLows WorldPrimaryFLows ON PrimaryTable.Flow = WorldPrimaryFLows.[Code]
               WHERE PrimaryTable.Product in('CRUDEOIL','OTHERCRUDE')
               AND Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
               """               

jodi_table = pd.read_sql(jodi_query, jodi_connection)
jodi_table_kbbl = jodi_table[(jodi_table['unit'] == 'KBBL')]
jodi_table_kbd = jodi_table[(jodi_table['unit'] == 'KBD')]

jodi_pivot_kbd = pd.pivot_table(jodi_table_kbd, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')
jodi_pivot_kbbl = pd.pivot_table(jodi_table_kbbl, values = 'quantity', index = 'month', columns = ['country','description'], aggfunc='sum')

jodi_pivot_kbbl.columns
jodi_pivot_kbd.columns

idx = pd.IndexSlice
jodi_kbd_data = jodi_pivot_kbd.loc[idx[:],idx[:,['Crude Production', 'Exports', 'Imports', 'Refinery Runs']]]
jodi_kbbl_data = jodi_pivot_kbbl.loc[idx[:],idx[:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_crude = pd.concat([jodi_kbd_data, jodi_kbbl_data], axis=1)
NWE_Total_fund_crude = NWE_Countries_crude.groupby(level='description', axis=1).sum()


# =============================================================================
# For Products
# =============================================================================

jodi_products_query = """
                   SELECT Countries.Country, SecondaryTable.Product, WorldSecondaryFLows.Description, SecondaryTable.Unit, CONVERT(datetime, CONCAT(SecondaryTable.Date,'-01'), 121) as Month, SecondaryTable.Quantity, SecondaryTable.Code
                   FROM JodiOil.dbo.SecondaryTable SecondaryTable 
                   INNER JOIN JodiOil.dbo.Countries Countries ON SecondaryTable.Country = Countries.[Country Code]
                   INNER JOIN JodiOil.dbo.WorldSecondaryFLows WorldSecondaryFLows ON SecondaryTable.Flow = WorldSecondaryFLows.[Code]
                   WHERE Countries.Country in ('Belgium','Denmark','Finland','France','Germany','Ireland','Lithuania','Netherlands','Norway','Poland','Sweden','United Kingdom')
                   """    

jodi_products_table = pd.read_sql(jodi_products_query, jodi_connection)
jodi_products_table_kbbl = jodi_products_table[(jodi_products_table['unit'] == 'KBBL')]
jodi_products_table_kbd = jodi_products_table[(jodi_products_table['unit'] == 'KBD')]

jodi_products_pivot_kbd = pd.pivot_table(jodi_products_table_kbd, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')
jodi_products_pivot_kbbl = pd.pivot_table(jodi_products_table_kbbl, values = 'quantity', index = 'month', columns = ['country','product','description'], aggfunc='sum')

jodi_products_pivot_kbbl.columns
jodi_products_pivot_kbd.columns

jodi_products_kbd_data = jodi_products_pivot_kbd.loc[idx[:],idx[:,:,['Demand', 'Exports', 'Imports']]]
jodi_products_kbbl_data = jodi_products_pivot_kbbl.loc[idx[:],idx[:,:, ['Closing Stock Level', 'Stock Change']]]

NWE_Countries_products = pd.concat([jodi_products_kbd_data, jodi_products_kbbl_data], axis=1)
NWE_Total_fund_products = NWE_Countries_products.groupby(level='description', axis=1).sum()


# =============================================================================
# Now flaten the data in order to use in dataframe for testing
# =============================================================================

NWE_Countries_products.columns = NWE_Countries_products.columns.to_series().str.join('_')                
#NWE_Total_fund_products.columns = NWE_Total_fund_products.columns.to_series().str.join('_')                      
NWE_Countries_crude.columns = NWE_Countries_crude.columns.to_series().str.join('_')                
#NWE_Total_fund_crude.columns = NWE_Total_fund_crude.columns.to_series().str.join('_')                      

path='L:/TRADING/ANALYSIS/Python/MIMA/'
NWE_Countries_products.to_excel(path+'NWE_Countries_products.xlsx')
NWE_Total_fund_products.to_excel(path+'NWE_Total_fund_products.xlsx') 
NWE_Countries_crude.to_excel(path+'NWE_Countries_crude.xlsx') 
NWE_Total_fund_crude.to_excel(path+'NWE_Total_fund_crude.xlsx')   
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_exce

## ---(Wed Feb  7 08:53:33 2018)---
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
import pandas as pd
import pypyodbc
from datetime import timedelta
ClipperRawData = pd.read_excel("L:\TRADING\ANALYSIS\Clipper Ship Tracking Analysis\Clipper Data\Clipper Global Crude Full History.xlsm", sheetname='Data')
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin('ABSHERON','Aegean Power','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara'))&(ClipperRawData['grade'].isin('AZERI'))]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(['ABSHERON','Aegean Power','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara']))&(ClipperRawData['grade'].isin('AZERI'))]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(list('ABSHERON','Aegean Power','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara'))&(ClipperRawData['grade'].isin('AZERI'))]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(list('ABSHERON','Aegean Power','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara')))&(ClipperRawData['grade'].isin('AZERI'))]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(list(['ABSHERON','Aegean Power','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara'])))&(ClipperRawData['grade'].isin('AZERI'))]
vessel_list = ['ABSHERON','Aegean Power','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara']
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&(ClipperRawData['grade'].isin('AZERI'))]
(ClipperRawData['vessel'].isin(vessel_list)
ClipperRawData['vessel'].isin(vessel_list)
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&(ClipperRawData['grade'].isin(['AZERI']))]
ClipperRawData['vessel']
ClipperRawData['vessel'].str.lstrip().isin(vessel_list)
ClipperRawData['vessel'].str.lstrip()
ClipperRawData['vessel'].str.rstrip()
.str.rstrip().isin(vessel_list))&(ClipperRawData['grade'].isin(['AZERI']))]
ClipperRawData['vessel']
ClipperRawData['vessel'].str.rstrip()
ClipperRawData['vessel'].str.lstrip()
clip_vessel_list = ClipperRawData['vessel'].str.lstrip()
clip_vessel_list = ClipperRawData['vessel']
ClipperRawData['vessel'].isin(vessel_list)
ClipperRawData['grade'].isin(['AZERI'])
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&(ClipperRawData['grade'].isin(['AZERI LIGHT']))]
from datetime import datetime
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&(ClipperRawData['grade'].isin(['AZERI LIGHT']))&(ClipperRawData['load_date'] > datetime(2017,5,1))]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1))]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country')]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country','offtake_date')]
vessel_list = ['ABSHERON','Aegean Power','Antartic','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara']

#clip_vessel_list = ClipperRawData['vessel'].str.lstrip()

filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country','offtake_date')]
vessel_list = ['ABSHERON','Aegean Power','Antartic','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','KRITI SAMARIA','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara','Seabravery','NS Captain','Alterego II','MILTIADIS M II',
                   'ALFA ALANDIA','AEGEAN NOBILITY','DOLVIKEN','Alexia','Dubai Glamour','Absheron']

#clip_vessel_list = ClipperRawData['vessel'].str.lstrip()

filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country','offtake_date')]
vessel_list = ['ABSHERON','Aegean Power','Antartic','Arctic','Astro Polaris','Azerbaijan','Brisith Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','Kriti Samaria','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara','Seabravery','NS Captain','Alterego ii','Miltiadis M ii',
                   'Alfa Alandia','Aegean Nobility','Dolviken','Alexia','Dubai Glamour','Absheron']

#clip_vessel_list = ClipperRawData['vessel'].str.lstrip()

filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country','offtake_date')]
vessel_list = ['ABSHERON','Aegean Power','Antartic','Arctic','Astro Polaris','Azerbaijan','British Robin','Finesse','KAVERI SPIRIT',
                   'Kimbolos Warrior','Kriti Island','Kriti Samaria','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara','Seabravery','NS Captain','Alterego ii','Miltiadis M ii',
                   'Alfa Alandia','Aegean Nobility','Dolviken','Alexia','Dubai Glamour','Absheron']

#clip_vessel_list = ClipperRawData['vessel'].str.lstrip()

filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country','offtake_date')]
vessel_list = ['ABSHERON','Aegean Power','Antartic','Arctic','Astro Polaris','Azerbaijan','British Robin','Finesse','Kaveri Spirit',
                   'Kimbolos Warrior','Kriti Island','Kriti Samaria','Leo Sun','Maersk Pearl','Maratha','Minerva Antartica','Minerva Nounou',
                   'Myrtos','Nevsky Prospect','Nissos Schinoussa','Nissos Serifos','Ns Century','NS Columbus','Ottoman Integrity',
                   'Panagia Armata','Parthenon TS','Pissiotis','Seaprince','Seaprincess','Stride','Yasa Golden Marmara','Seabravery','NS Captain','Alterego ii','Miltiadis M ii',
                   'Alfa Alandia','Aegean Nobility','Dolviken','Alexia','Dubai Glamour','Absheron']

#clip_vessel_list = ClipperRawData['vessel'].str.lstrip()

filtered_clipper = ClipperRawData.loc[(ClipperRawData['vessel'].isin(vessel_list))&
                                      (ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country','offtake_date')]
filtered_clipper = ClipperRawData.loc[(ClipperRawData['grade'].isin(['AZERI LIGHT']))&
                                      (ClipperRawData['load_date'] > datetime(2017,5,1))&
                                      (ClipperRawData['load_date'] < datetime(2018,1,1)), ('vessel','load_date','offtake_port','offtake_country','offtake_date')]
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()

## ---(Wed Feb 14 15:50:26 2018)---
series[-2]
"""

series = [3,10,12,13,12,10,12]
series[-2]
series = [3,10,12,13,12,10,12]
series[-2]
series = [3,10,12,13,12,10,12]
series[-3]

def average(series, n=None):
    if n is None:
        return average(series,len(series))
    return float(sum(series[-n:]))/n


def weighted_average(series, weights):
    result = 0.0
    weights.reverse()
    for n in range(len(weights)):
        result += series[-n-1] * weights[n]
    return result


weights = [0.1,0.2,0.3,0.4]
weighted_average(series, weights)
exponential_smoothing(series, 0.1)
weighted_average(series, weights)
def exponential_smoothing(series, alpha):
    result = [series[0]] # first value is the same as the series
    for n in range(1, len(series)):
        result.append(alpha * series[n] + (1-alpha) * result[n-1])
    return result


exponential_smoothing(series, 0.1)
double_exponential_smoothing(series, 0.9, 0.9)
def double_exponential_smoothing(series, alpha, beta):
    result = [series[0]]
    for n in range(1, len(series)+1):
        if n == 1:
            level, trend = series[0], series[1] - series[0]
        if n >= len(series): #we are forecasting
            value = result[-1]
        else:
            value = series[n]
        last_level, level = level, alpha*value + (1-alpha)*(level+trend)
        trend = beta*(level-last_level) + (1-beta)*trend
        result.append(level+trend)
    return result

double_exponential_smoothing(series, 0.9, 0.9)
series = [30,21,29,31,40,48,53,47,37,39,31,29,17,9,20,24,27,35,41,38,
          27,31,27,26,21,13,21,18,33,35,40,36,22,24,21,20,17,14,17,19,
          26,29,40,31,20,24,18,26,17,9,17,21,28,32,46,33,23,28,22,27,
          18,8,17,21,31,34,44,38,31,30,26,32]
def initial_trend(series, slen):
    sum = 0.0
    for i in range(slen):
        sum += float(series[i+slen] - series[i]) / slen
    return sum / slen

initial_trend(series, 12)
slen = 12
j = 1
series[slen*j:slen*j+slen]   
slen = 12
j = 2
series[slen*j:slen*j+slen]    
def initial_seasonal_components(series,slen):
    seasonals = {}
    season_averages = []
    n_seasons = int(len(series)/slen) # number of seasons is the length of the data series divided by season length
    # compute season averages
    for j in range (n_seasons):
        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))
    # compute initial values
    for i in range(slen):
        sum_of_vals_over_avg = 0.0
        for j in range(n_seasons):
            sum_of_vals_over_avg += series[slen*j+i] - season_averages[j]
        seasonals[i] = sum_of_vals_over_avg/n_seasons
    return seasonals

def initial_seasonal_components(series,slen):
    seasonals = {}
    season_averages = []
    n_seasons = int(len(series)/slen) # number of seasons is the length of the data series divided by season length
    # compute season averages
    for j in range (n_seasons):
        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))
    # compute initial values
    for i in range(slen): # for which period within the season
        sum_of_vals_over_avg = 0.0
        for j in range(n_seasons): # for which season or year
            # this is saying for i (the month we are concerned about), find how much the value of i differed from the yearly average. 
            # For example, when j = 0, then this is the first year and when i = 0 its the first month
            sum_of_vals_over_avg += series[slen*j+i] - season_averages[j]
        seasonals[i] = sum_of_vals_over_avg/n_seasons # we then assign the sum of the values divided by the number of seaons in the data 
        #for that month across the years to the seaosnals dictionary 
    return seasonals

initial_seasonal_components(series,12)
2%6
22%6
for i in range(5):
    print(i,i%5)

for i in range(20):
    print(i,i%5)

def triple_exponential_smoothiong(series, slen, alpha, beta, gamma, n_preds):
    result = []
    seasonals = initial_seasonal_components(series, slen)
    for i in range(len(series)+n_preds):
        if i == 0: # inital values
            smooth = series[0]
            trend = inital_trend(series, slen)
            result.append(series[0])
            continue
        if i >= len(series): # we are forecasting
            m = i - len(series) +1
            result.append((smooth + m*trend) + seasonals[i%slen]) # i%slen essentially repeats a repeating pattern in incremental amounts up to the values of slen, 
                                                                    # so in this case, we want the seasonals to be jan to dec over and over again, so we have i % sln to get 0-11 
        else:
            val = series[i]
            last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) + (1-alpha)*(smooth+trend)
            trend = beta * (smooth - last_smooth) + (1-beta)*trend
            seasonals[i%slen] = gamma*(val-smooth) + (1-gamma)*seasonals[i%slen]
            result.append(smooth+trend+seasonals[i%slen])
    return result

for num in range(2, 10):
    if num % 2 == 0:
        print("Found an even number", num)
        continue
    print("Found a number", num)

for num in range(2, 10):
    if num % 2 == 0:
        print("Found an even number", num)
        #continue
    print("Found a number", num)

for num in range(2, 10):
    if num % 2 == 0:
        print("Found an even number", num)
    else:
        print("Found a number", num)

def initial_trend(series, slen):
    sum = 0.0
    for i in range(slen):
        sum += float(series[i+slen] - series[i]) / slen
    return sum / slen


def initial_seasonal_components(series,slen):
    seasonals = {}
    season_averages = []
    n_seasons = int(len(series)/slen) # number of seasons is the length of the data series divided by season length
    # compute season averages
    for j in range (n_seasons):
        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))
    # compute initial values
    for i in range(slen): # for which period within the season
        sum_of_vals_over_avg = 0.0
        for j in range(n_seasons): # for which season or year
            # this is saying for i (the month we are concerned about), find how much the value of i differed from the yearly average. 
            # For example, when j = 0, then this is the first year and when i = 0 its the first month
            sum_of_vals_over_avg += series[slen*j+i] - season_averages[j]
        seasonals[i] = sum_of_vals_over_avg/n_seasons # we then assign the sum of the values divided by the number of seaons in the data 
        #for that month across the years to the seaosnals dictionary 
    return seasonals


### The algo makes use of the two functions above

def triple_exponential_smoothiong(series, slen, alpha, beta, gamma, n_preds):
    result = []
    seasonals = initial_seasonal_components(series, slen)
    for i in range(len(series)+n_preds):
        if i == 0: # inital values
            smooth = series[0]
            trend = inital_trend(series, slen)
            result.append(series[0])
            continue
        if i >= len(series): # we are forecasting
            m = i - len(series) +1 # how far in the future are we looking
            result.append((smooth + m*trend) + seasonals[i%slen]) # i%slen essentially repeats a repeating pattern in incremental amounts up to the values of slen, 
                                                                    # so in this case, we want the seasonals to be jan to dec over and over again, so we have i % sln to get 0-11 
        else:
            val = series[i]
            last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) + (1-alpha)*(smooth+trend)
            trend = beta * (smooth - last_smooth) + (1-beta)*trend
            seasonals[i%slen] = gamma*(val-smooth) + (1-gamma)*seasonals[i%slen]
            result.append(smooth+trend+seasonals[i%slen])
    return result

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()


print(date1)

print(DOEAPIDATA.head())
print(type(DOEAPIDATA))
DOEAPIDATA.dtypes
#DOEAPIDATA = pd.DataFrame(DOEAPIDATA, index=0)
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt
def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]]
		b = [Y[1] - Y[0]]
		y = [a[0] + b[0]]
  
		for i in range(len(Y)):
   
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)]
			y = [a[0] + b[0] + s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')

def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]]
		b = [Y[1] - Y[0]]
		y = [a[0] + b[0]]
  
		for i in range(len(Y)):
   
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)]
			y = [a[0] + b[0] + s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse

from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b


def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]]
		b = [Y[1] - Y[0]]
		y = [a[0] + b[0]]
  
		for i in range(len(Y)):
   
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)]
			y = [a[0] + b[0] + s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse

b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2] 
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE.py', wdir='C:/Users/mima/.spyder-py3')
data = pd.read_excel("M:\saud exports for holt test.xlsx")
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE.py', wdir='C:/Users/mima/.spyder-py3')
data[1:,1]
data = pd.read_excel("M:\saud exports for holt test.xlsx").tolist()
data = list(pd.read_excel("M:\saud exports for holt test.xlsx"))
data = pd.read_excel("M:\saud exports for holt test.xlsx")
data.iloc[1:,1]
data.iloc[:,1]
data = pd.read_excel("M:\saud exports for holt test.xlsx")
saud_exports = data.iloc[:,1]
additive(saud_exports, 12, 12)
saud_exports[:]
additive(data, 12, 12)
additive(saud_exports, 12, 12)
linear(saud_exports, 12)
saud_exports = data.iloc[:,1].values
linear(saud_exports, 12)
saud_exports = data.iloc[:,1]
saud_exports = list(data.iloc[:,1].values)
linear(saud_exports, 12)
additive(saud_exports, 12,12)
multiplicative(saud_exports, 12,12)
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper'
                '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection) 
clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection) 
multiplicative(saud_exports, 12,12)
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d')
mike
print(mike)
mike = DOEAPIDATA.resample('d').bfill()
mike = DOEAPIDATA.resample('d').bfill().mean()
mike = DOEAPIDATA.resample('d').bfill()
data_resample = data.groupby(level=0).resample('D')
data.set_index('DATE')
data_resample = data.groupby(level=0).resample('D')
data.set_index('DATE')
data_resample = data.groupby(level=0).resample('D')
data = data.set_index('DATE')
data_resample = data.groupby(level=0).resample('D')
print(data_resample)
tester = data_resample.groupby(level=0).transform(lambda x: x.iloc[0] / float(len(x)))
data_resample = data.groupby(level=0).resample('D')
data_resample = data.groupby(level=0).resample('D').()
data_resample = data.groupby(level=0).resample('D').asfreq()
data = pd.read_excel("M:\saud exports for holt test.xlsx")
data = pd.read_excel("M:\testdata.xlsx")
import pandas as pd
data = pd.read_excel("M:\testdata.xlsx")
data = pd.read_excel("M://testdata.xlsx")
data = data.set_index('DATE')
data_resample = data.groupby(level=0).resample('D').asfreq()
tester = data_resample.groupby(level=0).transform(lambda x: x.iloc[0] / float(len(x)))
data_resample = data.resample('D').asfreq()
data_resample = data.resample('D').asfreq().groupby(level=0)
tester = data_resample.groupby(level=0).transform(lambda x: x.iloc[0] / float(len(x)))
data_resample = data.resample('D').asfreq()
data['DAYS'] = data.index.dt.daysinmonth
data['DAYS'] = data.dt.daysinmonth
data['DAYS'] = data.daysinmonth
data['DAYS'] = data.dt.daysinmonth()
data
data.index
time = data.index.to_series()
data['DAYS'] = time.dt.daysinmonth()
time.daysinmonth()
data['Date'] = pd.to_datetime(data.index)
data['Date'].dt.daysinmonth
data['Date'] = pd.to_datetime(data.index).dt.daysinmonth
data['Date'] = (pd.to_datetime(data.index)).dt.daysinmonth
data['Date'] = pd.to_datetime(data.index)
data['Date'] = (pd.to_datetime(data.index)).dt.daysinmonth
data['DaysInMonth'] = pd.to_datetime(data.index)
data['DaysInMonth'] = data['DaysInMonth'].dt.daysinmonth
data = pd.read_excel("M://testdata.xlsx")
data['DaysInMonth'] = pd.to_datetime(data.index)
data['DaysInMonth'] = data['DaysInMonth'].dt.daysinmonth
saud_data = pd.read_excel("M://saudi data.xlsx")
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data / saud_data['DaysInMonth']
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth'].values)
kbd = saud_data.div(saud_data['DaysInMonth'])
saud_data['DaysInMonth']
saud_data.iloc['DaysInMonth'][0]
kbd = saud_data.div(saud_data['DaysInMonth'].iloc[0])
data = pd.read_excel("M://testdata.xlsx")

saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth'].iloc[0])
saud_data['DaysInMonth']
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth'].iloc[:,1])
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth'])
additive(saud_exports, 12,12)
multiplicative(saud_exports, 12,12)
kbd = saud_data / saud_data['DaysInMonth']
kbd = saud_data / saud_data['DaysInMonth'].values
kbd = saud_data.div(saud_data['DaysInMonth'], axis = 'index')
kbd = saud_data.div(saud_data['DaysInMonth'], axis = 'index').fillna(0)
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth']*1000, axis = 'index').fillna(0)
additive(kbd.CHINA, 12,12)
additive(list(kbd.CHINA), 12,12)
additive(list(kbd.[EAST AFRICA]), 12,12)
additive(list(kbd['EAST AFRICA']), 12,12)
def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(min(a[-1] + b[-1] + s[-m]),0)
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append((a[-1] + b[-1]) * s[-m])
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



data = pd.read_excel("M:\saud exports for holt test.xlsx")
saud_exports = list(data.iloc[:,1].values)
saud_exports[:]
additive(list(kbd['EAST AFRICA']), 12,12)
def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
            temp = max(a[-1] + b[-1] + s[-m],0)
			Y.append(temp)
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse

def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
            temp = max(a[-1] + b[-1] + s[-m],0)
            Y.append(temp)
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse

def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
            temp = max(a[-1] + b[-1] + s[-m],0)
            Y.append(temp)
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse

def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1] + s[-m]),0))
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse

additive(list(kbd['EAST AFRICA']), 12,12)
def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]] # first data point
		b = [Y[1] - Y[0]] # trend is first minus second
		y = [a[0] + b[0]] # hence estimate is first value plus first trend estimate
  
		for i in range(len(Y)):
            # for each sequential point in our data series, update the level and trend calcs and add to estimate list
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)] # initial seasonal indicies
			y = [a[0] + b[0] + s[0]] # initial observation is level plus trend plus seasonality
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i])) # take actual minus seasonbal index weighed agfainst expected 
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i]) # current slope estimate based on levels estimate weighed against previous trend value
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i]) # seasonality is rearranged version same as above but sub in the levels equation above and bound results in this
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse


def linear(x, fc, alpha = None, beta = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None):
  
		initial_values = array([0.3, 0.1])
		boundaries = [(0, 1), (0, 1)]
		type = 'linear'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type), bounds = boundaries, approx_grad = True)
		alpha, beta = parameters[0]
 
	a = [Y[0]]
	b = [Y[1] - Y[0]]
	y = [a[0] + b[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(a[-1] + b[-1])
  
		a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		y.append(a[i + 1] + b[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, rmse


def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1] + s[-m]),0))
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1]) * s[-m]),0)
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



data = pd.read_excel("M:\saud exports for holt test.xlsx")
saud_exports = list(data.iloc[:,1].values)
saud_exports[:]
additive(list(kbd['EAST AFRICA']), 12,12)
multiplicative(saud_exports, 12,12)
def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max(((a[-1] + b[-1]) * s[-m]),0))
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



data = pd.read_excel("M:\saud exports for holt test.xlsx")
saud_exports = list(data.iloc[:,1].values)
saud_exports[:]
additive(list(kbd['EAST AFRICA']), 12,12)
multiplicative(saud_exports, 12,12)

additive(list(kbd['EAST AFRICA']), 12,12)
multiplicative(list(kbd['EAST AFRICA']), 12,12)
additive(list(kbd['EAST AFRICA']), 12,12)
multiplicative(list(kbd['EAST AFRICA']), 12,12)
def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append((a[-1] + b[-1]) * s[-m])
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



data = pd.read_excel("M:\saud exports for holt test.xlsx")
saud_exports = list(data.iloc[:,1].values)
saud_exports[:]
additive(list(kbd['EAST AFRICA']), 12,12)
multiplicative(list(kbd['EAST AFRICA']), 12,12)
multiplicative(list(kbd['CHINA']), 12,12)
def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max(((a[-1] + b[-1]) * s[-m]),0))
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



data = pd.read_excel("M:\saud exports for holt test.xlsx")
saud_exports = list(data.iloc[:,1].values)
saud_exports[:]
additive(list(kbd['EAST AFRICA']), 12,12)
multiplicative(list(kbd['CHINA']), 12,12)
multiplicative(list(kbd['INDIA EC']), 12,12)
kbd.columns.values
len(kbd.columns.values)
for i in kbd.columns.values:
    print(i)

for i in kbd.columns.values:
    print(i)
    print(kbd[i])

forecast_group = pd.DataFrame()
for i in kbd.columns.values:
    forecast_group[i] = multiplicative(list(kbd[i]), 12,12)

for i in kbd.columns.values:
    print(multiplicative(list(kbd[i]), 12,12))

for i in kbd.columns.values:
    print(additive(list(kbd[i]), 12,12))

for i in kbd.columns.values:
    forecast_group[i] = additive(list(kbd[i]), 12,12)

forecast_group = pd.DataFrame()
for i in kbd.columns.values:
    forecast_group[i] = additive(list(kbd[i]), 12,12)

for i in kbd.columns.values:
    forecast_group[i] = additive(list(kbd[i]), 12,12)[12]

additive(list(kbd[i]), 12,12).loc[:13]
additive(list(kbd['CHINA']), 12,12)
for i in kbd.columns.values:
    temp = additive(list(kbd[i]), 12,12)
    j = [j[0] for j in temp]
    print(j)

for i in kbd.columns.values:
    temp = additive(list(kbd[i]), 12,12)
    [j[0] for j in temp]
    print(j)

for i in kbd.columns.values:
    print(additive(list(kbd[i]), 12,12))

for i in kbd.columns.values:
    a = additive(list(kbd[i]), 12,12)

for i in kbd.columns.values:
    a = additive(list(kbd[i]), 12,12)[0]

for i in kbd.columns.values:
    forecast_group[i] = additive(list(kbd[i]), 12,12)[0]

for i in kbd.columns.values:
    forecast_group.append(pd.DataFrame(additive(list(kbd[i]), 12,12)[0], columns = i))

for i in kbd.columns.values:
    forecast_group.append(pd.DataFrame(additive(list(kbd[i]), 12,12)[0], columns = i, axis = 1))

for i in kbd.columns.values:
    forecast_group.append(pd.DataFrame(additive(list(kbd[i]), 12,12)[0], columns = i))

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0], columns = i)], axis=1)

additive(list(kbd[i]), 12,12)[0]
for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0])], axis=1)

pd.DataFrame(additive(list(kbd[i]), 12,12)[0])
forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0])], axis=1)

forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={'0':i})], axis=1)

forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={0:i})], axis=1)

kbd = saud_data.div(saud_data['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)
for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={0:i})], axis=1)

forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={0:i})], axis=1)

forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


path='M:/'
forecast_group.to_excel(path+'AutoForecast.xlsx')
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)


# =============================================================================
# must define what our loss function is for the optimiser - here we are using the root mean squared error
# 1) Y is the data series we are looking at
# 2) the type is defined as 'linear', 'mulitiplicative' or 'additive' - will be using additive but others in here for good measure
# and takes the second positional argument
# 3) set rmse = 0 initially
# 4) a is the initial level
# 5) b is the initial trend
# 6) y is the expected value of Y which takes the inital values of a plus the initial values of b, i.e. first level with trend added
# 7) this will give a series of y which we then measure against the true value Y and calculate the rmse
# 8) s is the seaosnal index which when rearranged gives s equation below when you bound inital coefficient to be between 1 and 0
#  https://www.otexts.org/fpp/7/5
# 9) sum of the inidividual components are then added to list y
# 10) the arguments that are passed into the RMSE are given in the optimiser algo
# =============================================================================

def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]] # first data point
		b = [Y[1] - Y[0]] # trend is first minus second
		y = [a[0] + b[0]] # hence estimate is first value plus first trend estimate
  
		for i in range(len(Y)):
            # for each sequential point in our data series, update the level and trend calcs and add to estimate list
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)] # initial seasonal indicies
			y = [a[0] + b[0] + s[0]] # initial observation is level plus trend plus seasonality
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i])) # take actual minus seasonbal index weighed agfainst expected 
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i]) # current slope estimate based on levels estimate weighed against previous trend value
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i]) # seasonality is rearranged version same as above but sub in the levels equation above and bound results in this
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse


def linear(x, fc, alpha = None, beta = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None):
  
		initial_values = array([0.3, 0.1])
		boundaries = [(0, 1), (0, 1)]
		type = 'linear'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type), bounds = boundaries, approx_grad = True)
		alpha, beta = parameters[0]
 
	a = [Y[0]]
	b = [Y[1] - Y[0]]
	y = [a[0] + b[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(a[-1] + b[-1])
  
		a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		y.append(a[i + 1] + b[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, rmse


def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1] + s[-m]),5))
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max(((a[-1] + b[-1]) * s[-m]),5))
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



# =============================================================================
# data = pd.read_excel("M:\saud exports for holt test.xlsx")
# saud_exports = list(data.iloc[:,1].values)
# saud_exports[:]
# additive(list(kbd['CHINA']), 12,12)
# multiplicative(list(kbd['INDIA EC']), 12,12)
# =============================================================================

forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


path='M:/'
forecast_group.to_excel(path+'AutoForecast.xlsx')
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)


# =============================================================================
# must define what our loss function is for the optimiser - here we are using the root mean squared error
# 1) Y is the data series we are looking at
# 2) the type is defined as 'linear', 'mulitiplicative' or 'additive' - will be using additive but others in here for good measure
# and takes the second positional argument
# 3) set rmse = 0 initially
# 4) a is the initial level
# 5) b is the initial trend
# 6) y is the expected value of Y which takes the inital values of a plus the initial values of b, i.e. first level with trend added
# 7) this will give a series of y which we then measure against the true value Y and calculate the rmse
# 8) s is the seaosnal index which when rearranged gives s equation below when you bound inital coefficient to be between 1 and 0
#  https://www.otexts.org/fpp/7/5
# 9) sum of the inidividual components are then added to list y
# 10) the arguments that are passed into the RMSE are given in the optimiser algo
# =============================================================================

def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]] # first data point
		b = [Y[1] - Y[0]] # trend is first minus second
		y = [a[0] + b[0]] # hence estimate is first value plus first trend estimate
  
		for i in range(len(Y)):
            # for each sequential point in our data series, update the level and trend calcs and add to estimate list
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)] # initial seasonal indicies
			y = [a[0] + b[0] + s[0]] # initial observation is level plus trend plus seasonality
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i])) # take actual minus seasonbal index weighed agfainst expected 
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i]) # current slope estimate based on levels estimate weighed against previous trend value
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i]) # seasonality is rearranged version same as above but sub in the levels equation above and bound results in this
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse


def linear(x, fc, alpha = None, beta = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None):
  
		initial_values = array([0.3, 0.1])
		boundaries = [(0, 1), (0, 1)]
		type = 'linear'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type), bounds = boundaries, approx_grad = True)
		alpha, beta = parameters[0]
 
	a = [Y[0]]
	b = [Y[1] - Y[0]]
	y = [a[0] + b[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(a[-1] + b[-1])
  
		a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		y.append(a[i + 1] + b[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, rmse


def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1] + s[-m]),0))
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max(((a[-1] + b[-1]) * s[-m]),0))
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



# =============================================================================
# data = pd.read_excel("M:\saud exports for holt test.xlsx")
# saud_exports = list(data.iloc[:,1].values)
# saud_exports[:]
# additive(list(kbd['CHINA']), 12,12)
# multiplicative(list(kbd['INDIA EC']), 12,12)
# =============================================================================

forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


path='M:/'
forecast_group.to_excel(path+'AutoForecast.xlsx')
saud_data = pd.read_excel("M://saudi data.xlsx").transpose()
saud_data['DaysInMonth'] = pd.to_datetime(saud_data.index)
saud_data['DaysInMonth'] = saud_data['DaysInMonth'].dt.daysinmonth
kbd = saud_data.div(saud_data['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)


# =============================================================================
# must define what our loss function is for the optimiser - here we are using the root mean squared error
# 1) Y is the data series we are looking at
# 2) the type is defined as 'linear', 'mulitiplicative' or 'additive' - will be using additive but others in here for good measure
# and takes the second positional argument
# 3) set rmse = 0 initially
# 4) a is the initial level
# 5) b is the initial trend
# 6) y is the expected value of Y which takes the inital values of a plus the initial values of b, i.e. first level with trend added
# 7) this will give a series of y which we then measure against the true value Y and calculate the rmse
# 8) s is the seaosnal index which when rearranged gives s equation below when you bound inital coefficient to be between 1 and 0
#  https://www.otexts.org/fpp/7/5
# 9) sum of the inidividual components are then added to list y
# 10) the arguments that are passed into the RMSE are given in the optimiser algo
# =============================================================================

def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]] # first data point
		b = [Y[1] - Y[0]] # trend is first minus second
		y = [a[0] + b[0]] # hence estimate is first value plus first trend estimate
  
		for i in range(len(Y)):
            # for each sequential point in our data series, update the level and trend calcs and add to estimate list
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)] # initial seasonal indicies
			y = [a[0] + b[0] + s[0]] # initial observation is level plus trend plus seasonality
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i])) # take actual minus seasonbal index weighed agfainst expected 
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i]) # current slope estimate based on levels estimate weighed against previous trend value
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i]) # seasonality is rearranged version same as above but sub in the levels equation above and bound results in this
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse


def linear(x, fc, alpha = None, beta = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None):
  
		initial_values = array([0.3, 0.1])
		boundaries = [(0, 1), (0, 1)]
		type = 'linear'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type), bounds = boundaries, approx_grad = True)
		alpha, beta = parameters[0]
 
	a = [Y[0]]
	b = [Y[1] - Y[0]]
	y = [a[0] + b[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(a[-1] + b[-1])
  
		a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		y.append(a[i + 1] + b[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, rmse


def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1] + s[-m]),0))
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max(((a[-1] + b[-1]) * s[-m]),0))
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse



# =============================================================================
# data = pd.read_excel("M:\saud exports for holt test.xlsx")
# saud_exports = list(data.iloc[:,1].values)
# saud_exports[:]
# additive(list(kbd['CHINA']), 12,12)
# multiplicative(list(kbd['INDIA EC']), 12,12)
# =============================================================================

forecast_group = pd.DataFrame()

for i in kbd.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(kbd[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


path='M:/'
forecast_group.to_excel(path+'AutoForecast.xlsx')
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE.py', wdir='C:/Users/mima/.spyder-py3')
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STUKLS022;'
                                'Database=RefineryInfo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = "clipper" '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection) 
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection) 
clipper_db_table.loaddate
clipper_db_table.loaddate.day
import datetime as dt
datee = dt.datetime.strptime(clipper_db_table.loaddate, "%Y-%m-%d")
pd.to_datetime(clipper_db_table['loaddate'])
pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD'] = np.where(clipper_db_table.LD <= 10, 1, 
                                 np.where(10>clipper_db_table.LD <=20, 11, 21))
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD']
clipper_db_table['LD'] = np.where(clipper_db_table.LD <= 10, 1, 
                                 np.where(10<clipper_db_table.LD <=20, 11, 21))
clipper_db_table['LD'] = np.where(clipper_db_table.LD <= 10, 1,2)
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD'] = np.where(clipper_db_table.LD <= 10, 1, np.where(10<clipper_db_table.LD <=20, 11, 21))
np.where(10<clipper_db_table.LD <=20, 11, 21)
10<clipper_db_table.LD
clipper_db_table.L
clipper_db_table.LD
10<clipper_db_table.LD <=20
(10<clipper_db_table.LD)&(clipper_db_table.LD <=20)
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD'] = np.where(clipper_db_table.LD <= 10, 1, np.where((10<clipper_db_table.LD)&(clipper_db_table.LD <=20), 11, 21))
clipper_db_table['LD']
clipper_db_table['LD_day'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD_month'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['LD_year'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['LD_day'] = np.where(clipper_db_table.LD <= 10, 1, np.where((10<clipper_db_table.LD)&(clipper_db_table.LD <=20), 11, 21))
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_year','LD_month','LD_day']])
clipper_db_table['LD_month']
clipper_db_table['LD_year']
clipper_db_table['LD_day']
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_year','LD_month','LD_day']])
clipper_db_table[['LD_year','LD_month','LD_day']]
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_year','LD_month','LD_day']], yearfirst=True)
clipper_db_table['LD_day'] = np.where(clipper_db_table.LD <= 10, 1, np.where((10<clipper_db_table.LD)&(clipper_db_table.LD <=20), 11, 21)).astype(int)
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_year','LD_month','LD_day']], yearfirst=True)
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_day','LD_month','LD_year']])
clipper_db_table[['LD_day','LD_month','LD_year']]
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_day','LD_month','LD_year']])
clipper_db_table['LD']
clipper_db_table['LD_day'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD_month'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['LD_year'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['LD_day'] = np.where(clipper_db_table.LD <= 10, 1, np.where((10<clipper_db_table.LD)&(clipper_db_table.LD <=20), 11, 21)).astype(int)
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_day','LD_month','LD_year']])
pd.to_datetime(clipper_db_table[['LD_day','LD_month','LD_year']])
clipper_db_table['LD_day']
clipper_db_table['LD_year']
clipper_db_table['LD_month']
clipper_db_table['LD_day'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['LD_month'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['LD_year'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['LD_day'] = np.where(clipper_db_table.LD <= 10, 1, np.where((10<clipper_db_table.LD)&(clipper_db_table.LD <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_day','LD_month','LD_year']])
clipper_db_table['LD_day']
clipper_db_table[['LD_day','LD_month','LD_year']]
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['LD_day','LD_month','LD_year']])
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.LD <= 10, 1, np.where((10<clipper_db_table.LD)&(clipper_db_table.LD <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']])
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc
import datetime as dt

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection)
  
  
  
  ### convert the loaddate column to a pandas time series          
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.LD <= 10, 1, np.where((10<clipper_db_table.LD)&(clipper_db_table.LD <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']])
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']])
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
clipper_db_pivot.head()
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_pivot.columns[0]
type(clipper_db_pivot.columns[0])
clipper_db_pivot.columns[0][0]
clipper_db_pivot.columns[0][1]
clipper_db_pivot.columns=clipper_db_pivot.columns(x for x in clipper_db_pivot.columns[x][0]&clipper_db_pivot.columns[x][1])
clipper_db_pivot.columns[0]
clipper_db_pivot.columns
pd.DataFrame(clipper_db_pivot)
pd.DataFrame(clipper_db_pivot).head()
clipper_db_pivot.columns
type(clipper_db_pivot)
listCols = list(clipper_db_pivot.columns[x][0]&clipper_db_pivot.columns[x][1] for x in len(clipper_db_pivot.columns))
listCols = list(clipper_db_pivot.columns[x][0]&clipper_db_pivot.columns[x][1] for x in range(1, len(clipper_db_pivot.columns)))
listCols = list([clipper_db_pivot.columns[x][0]&clipper_db_pivot.columns[x][1] for x in len(clipper_db_pivot.columns)])
len(clipper_db_pivot.columns)
clipper_db_pivot.columns
range(0,len(clipper_db_pivot.columns))
listCols = list([clipper_db_pivot.columns[x][0]&clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])
listCols = list([clipper_db_pivot.columns[x][0]+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])
listCols = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])
clipper_db_pivot.columns=listCols
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc
import datetime as dt
import numpy as np

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection)



### get load dat6e column in same format as decades from balances
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']])

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']])

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns)+1)])
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
dt.date(2012,12,31)
date(2012,12,31)
dt.datetime(2012,12,31)
clipper_db_pivot = clipper_db_pivot[clipper_db_pivot.LD > dt.date(2012,12,31)]
clipper_db_pivot = clipper_db_pivot[clipper_db_pivot['LD'] > dt.date(2012,12,31)]
clipper_db_pivot = clipper_db_pivot.iloc[clipper_db_pivot.index > dt.date(2012,12,31)]
clipper_db_pivot = clipper_db_pivot.iloc[clipper_db_pivot.index > dt.date(2012,31,12)]
clipper_db_pivot.index
clipper_db_pivot = clipper_db_pivot.iloc[clipper_db_pivot.index > dt.datetime(2012,12,31)]
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']])

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[clipper_db_pivot.index > dt.datetime(2012,12,31)]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).date()
clipper_db_pivot.index
clipper_db_pivot.index.date()
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[clipper_db_pivot.index > dt.datetime(2012,12,31)]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[clipper_db_pivot.index > dt.date(2012,12,31)]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]] # first data point
		b = [Y[1] - Y[0]] # trend is first minus second, or change in level
		y = [a[0] + b[0]] # hence estimate is first value plus first trend estimate
  
		for i in range(len(Y)):
            # for each sequential point in our data series, update the level and trend calcs and add to estimate list
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)] # initial seasonal indicies
			y = [a[0] + b[0] + s[0]] # initial observation is level plus trend plus seasonality
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i])) # take actual minus seasonbal index weighed agfainst expected 
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i]) # current slope estimate based on levels estimate weighed against previous trend value
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i]) # seasonality is rearranged version same as above but sub in the levels equation above and bound results in this
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse


def linear(x, fc, alpha = None, beta = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None):
  
		initial_values = array([0.3, 0.1])
		boundaries = [(0, 1), (0, 1)]
		type = 'linear'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type), bounds = boundaries, approx_grad = True)
		alpha, beta = parameters[0]
 
	a = [Y[0]]
	b = [Y[1] - Y[0]]
	y = [a[0] + b[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(a[-1] + b[-1])
  
		a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		y.append(a[i + 1] + b[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, rmse


def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1] + s[-m]),0))
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max(((a[-1] + b[-1]) * s[-m]),0))
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse

forecast_group = pd.DataFrame()

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 36,12)[0]).rename(columns={0:i})], axis=1)

forecast_group = pd.DataFrame()

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 36,36)[0]).rename(columns={0:i})], axis=1)

dt.now()
dt.today()
dt.now()
Timestamp.now()
import datetime as dt
dt.now()
pd.to_datetime('today')
pd.to_datetime('today')-10
import pypyodbc
from datetime import datetime, timedelta as dt, td
from datetime import timedelta
import numpy as np
pd.to_datetime('today')-timedelta(days=10)
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > dt.date(2012,12,31))&(clipper_db_pivot.index < pd.to_datetime('today')-timedelta(days=10))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')

# =============================================================================
# data = pd.read_excel("M:\saud exports for holt test.xlsx")
# saud_exports = list(data.iloc[:,1].values)
# saud_exports[:]
# additive(list(kbd['CHINA']), 12,12)
# multiplicative(list(kbd['INDIA EC']), 12,12)
# =============================================================================

forecast_group = pd.DataFrame()

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 36,36)[0]).rename(columns={0:i})], axis=1)

from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc
from datetime import datetime
from datetime import timedelta
import numpy as np
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > datetime.date(2012,12,31))&(clipper_db_pivot.index < pd.to_datetime('today')-timedelta(days=10))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > datetime.dt.date(2012,12,31))&(clipper_db_pivot.index < pd.to_datetime('today')-timedelta(days=10))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc
from datetime import datetime as dt
from datetime import timedelta
import numpy as np

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection)



### get load dat6e column in same format as decades from balances
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > dt.date(2012,12,31))&(clipper_db_pivot.index < pd.to_datetime('today')-timedelta(days=10))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > dt.date(2012,12,31))&(clipper_db_pivot.index < pd.to_datetime('today')-timedelta(days=10))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < pd.to_datetime('today')-timedelta(days=10))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
import pypyodbc
from datetime import datetime as dt
from datetime import date
from datetime import timedelta
date(2012,12,31)
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < pd.to_datetime('today')-timedelta(days=10))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
clipper_db_pivot.index
date(2012,12,31)
pd.to_datetime('today')-timedelta(days=10)
pd.to_datetime('today')
date(pd.to_datetime('today'))
pd.to_datetime('today').dt.date
pd.to_datetime('today').date
clipper_db_pivot.index < pd.to_datetime('today').date-timedelta(days=10)
clipper_db_pivot.index < (pd.to_datetime('today').date-timedelta(days=10))
pd.to_datetime('today').date-timedelta(days=10)
pd.to_datetime('today').date
pd.to_datetime('today').date()
pd.to_datetime('today').date()-timedelta(days=10)
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=10)))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
forecast_group = pd.DataFrame()

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 36,36)[0]).rename(columns={0:i})], axis=1)

(pd.to_datetime('today').date()-timedelta(days=10))
pd.to_datetime('today').date()
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' )
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=10)))]
test = additive(list(clipper_db_pivot['Saudi Arabia_INDIA EC']), 36,36)
clipper_db_pivot['Saudi Arabia_INDIA EC']
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)
test = additive(list(clipper_db_pivot['Saudi Arabia_INDIA EC']), 36,36)
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=10)))]
path='M:/'
clipper_db_pivot.to_excel(path+'pivot.xlsx')
test = additive(list(clipper_db_pivot['Saudi Arabia_INDIA EC']), 36,36)
forecast_group = pd.DataFrame()

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 36,36)[0]).rename(columns={0:i})], axis=1)

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection)
type(clipper_db_table.loadmonth)
clipper_db_table.loadmonth
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[[1,'months','years']]).dt.date
clipper_db_table['days'] = pd.to_datetime(clipper_db_table['loaddate']).dt.day
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
#clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['days'] = 1
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date
clipper_db_table['LD']
pd.to_datetime('today').date()-timedelta(months=1)
pd.to_datetime('today').date()
df.today.dt.to_period('M') 
d.to_datetime('today').date().dt.to_period('M')
pd.to_datetime('today').date().dt.to_period('M') 
pd.to_datetime('today').date().to_period('M')
pd.to_datetime('today').date()
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()))]
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()))]
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=31)))]
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=32)))]
test = additive(list(clipper_db_pivot['Saudi Arabia_INDIA EC']), 12,12)
test = additive(list(clipper_db_pivot['Saudi Arabia_INDIA EC']), 12,12)[0]
clipper_db_pivot.dt.daysinmonth
clipper_db_pivot.index.dt.daysinmonth
clipper_db_pivot.LD.dt.daysinmonth
pd.to_datetime(clipper_db_pivot.index).dt.daysinmonth
clipper_db_pivot['DaysInMonth'] = pd.to_datetime(clipper_db_pivot.index)
clipper_db_pivot['DaysInMonth'] = clipper_db_pivot['DaysInMonth'].dt.daysinmonth
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
#clipper_db_table['days'] = np.where(clipper_db_table.days <= 10, 1, np.where((10<clipper_db_table.days)&(clipper_db_table.days <=20), 11, 21)).astype('int64')
clipper_db_table['days'] = 1
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)
# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=32)))]

# transform to kbd
clipper_db_pivot['DaysInMonth'] = pd.to_datetime(clipper_db_pivot.index)
clipper_db_pivot['DaysInMonth'] = clipper_db_pivot['DaysInMonth'].dt.daysinmonth
clipper_db_pivot = clipper_db_pivot.div(clipper_db_pivot['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)
forecast_group = pd.DataFrame()

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)

pd.date_range(pd.to_datetime('today').date(), periods=12, freq='MS')
pd.date_range(pd.to_datetime('today').date()+MonthBegin(n=-1), periods=12, freq='MS')
pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')
forecast_group = pd.DataFrame()
forecast_group.set_index(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
forecast_group = pd.DataFrame(index = (pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')))
for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)

forecast_group = pd.DataFrame(index = (pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')))
forecast_group = pd.DataFrame()
fc_index = pd.DataFrame(index = (pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')))
for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)

forecast_group.set_index(fc_index)
fc_index = pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')
forecast_group.set_index(fc_index)    
forecast_group = forecast_group.set_index(fc_index) 
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = 1
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)

# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=32)))]

# transform to kbd
clipper_db_pivot['DaysInMonth'] = pd.to_datetime(clipper_db_pivot.index)
clipper_db_pivot['DaysInMonth'] = clipper_db_pivot['DaysInMonth'].dt.daysinmonth
clipper_db_pivot = clipper_db_pivot.div(clipper_db_pivot['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)

forecast_group = pd.DataFrame()
fc_index = pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


forecast_group = forecast_group.set_index(fc_index)    
path='M:/'
forecast_group.to_excel(path+'FlowsForecasts.xlsx')
for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(multiplicative(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


forecast_group = forecast_group.set_index(fc_index)    
path='M:/'
forecast_group.to_excel(path+'FlowsForecasts2.xlsx')
forecast_group = pd.DataFrame()
fc_index = pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(multiplicative(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


forecast_group = forecast_group.set_index(fc_index)    
path='M:/'
forecast_group.to_excel(path+'FlowsForecasts2.xlsx')
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = 1
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)

# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=32)))]

# transform to kbd
clipper_db_pivot['DaysInMonth'] = pd.to_datetime(clipper_db_pivot.index)
clipper_db_pivot['DaysInMonth'] = clipper_db_pivot['DaysInMonth'].dt.daysinmonth
clipper_db_pivot = clipper_db_pivot.div(clipper_db_pivot['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)

forecast_group = pd.DataFrame()
fc_index = pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


forecast_group = forecast_group.set_index(fc_index)    
path='M:/'
forecast_group.to_excel(path+'FlowsForecasts.xlsx')

## ---(Thu Feb 22 15:33:37 2018)---
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc
from datetime import datetime as dt
from datetime import date
from datetime import timedelta
import numpy as np

clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection)

type(clipper_db_table.loadmonth)

### get load dat6e column in same format as decades from balances
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = 1
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity', index = clipper_db_table['LD'], columns = ['loadcountry','dischargesubregion'], aggfunc = 'sum' ).fillna(0)

# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2012,12,31))&(clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=32)))]

# transform to kbd
clipper_db_pivot['DaysInMonth'] = pd.to_datetime(clipper_db_pivot.index)
clipper_db_pivot['DaysInMonth'] = clipper_db_pivot['DaysInMonth'].dt.daysinmonth
clipper_db_pivot = clipper_db_pivot.div(clipper_db_pivot['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)

forecast_group = pd.DataFrame()
fc_index = pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')

for i in clipper_db_pivot.columns.values:
    forecast_group = pd.concat([forecast_group, pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})], axis=1)


forecast_group = forecast_group.set_index(fc_index)    
path='M:/'
forecast_group.to_excel(path+'FlowsForecasts.xlsx')
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE with months.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()
pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]
pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0])
pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})
pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[1]).rename(columns={0:i})
pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)[0]).rename(columns={0:i})
pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)).rename(columns={0:i})
(additive(list(clipper_db_pivot[i]), 12,12))
additive(list(clipper_db_pivot[i]), 12,12)[0]
additive(list(clipper_db_pivot[i]), 12,12)[1]
pd.DataFrame(additive(list(clipper_db_pivot[i]), 12,12)).rename(columns={0:i})
additive(list(clipper_db_pivot[i]), 12,12)
model_output = additive(list(clipper_db_pivot[i]), 12,12)
model_output
pd.DataFrame(model_output[0]).rename(columns={0:i})
model_output[1]
(model_output[4]
(model_output[4])
temp = pd.DataFrame(model_output[0]).rename(columns={0:i})
temp.append(model_output[4])
temp.append(pd.Series(model_output[4]))
temp.append(pd.Series(model_output[4]), ignore_index=True)
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = pd.DataFrame(model_output[0]).rename(columns={0:i})
temp.append(pd.Series(model_output[4]), ignore_index=True, axis=0)
additive(list(clipper_db_pivot[i]), 12,12)
list(model_output[0])
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])
temp
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])

temp = pd.DataFrame(temp).rename(columns={0:i})
temp
fc_index
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index = fc_index.append('RMSE')
fc_index
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index = fc_index.append('RMSE')
print(fc_index)
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
forecast_group = pd.DataFrame()
fc_index = pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])
temp = pd.DataFrame(temp).rename(columns={0:i})
temp
forecast_group = pd.concat([forecast_group, temp], axis=1) 
fc_index.append('RMSE')
fc_index.iloc[12] = 'RMSE'
pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')
fc_index.append(str('RMSE'))
forecast_group
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.append(str('RMSE'))
forecast_group = forecast_group.set_index(fc_index)  
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')).date()
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12))
list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index[0]
fc_index[0][0]
forecast_group = pd.DataFrame()
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index[0][0]
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.append(str('RMSE'))
forecast_group
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])
temp = pd.DataFrame(temp).rename(columns={0:i})
forecast_group = pd.concat([forecast_group, temp], axis=1)
forecast_group
forecast_group = forecast_group.set_index(fc_index) 
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')).apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index = list(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS').apply(lambda x: x.dt.strftime('%Y/%m/%d')))
fc_index = pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS')
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.append(str('RMSE'))
fc_index.iloc[12]=(str('RMSE'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
list(fc_index)
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
list(fc_index)
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index.loc[12]
fc_index.loc[11]
fc_index.loc[12] = [str('RMSE')]
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index.loc[12] = str('RMSE')
forecast_group = pd.DataFrame()
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])
temp = pd.DataFrame(temp).rename(columns={0:i})
forecast_group = pd.concat([forecast_group, temp], axis=1)
forecast_group = forecast_group.set_index(fc_index)   
forecast_group
fc_index
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])
temp = pd.DataFrame(temp).rename(columns={0:i})
fc_index = pd.concat([fc_index, temp], axis=1) 
fc_index = fc_index.set_index('0')    
fc_index
fc_index.columns.values  
fc_index = fc_index.set_index(0)  
fc_index.loc[13] = str('RMSE vs Mean')
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])
temp = pd.DataFrame(temp).rename(columns={0:i})
fc_index = pd.concat([fc_index, temp], axis=1)
model_output[4] / np.mean(model_output[0])
model_output[4]
model_output[0]
np.mean(model_output[0])
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')
model_output = additive(list(clipper_db_pivot[i]), 12,12)
temp = list(model_output[0])
temp.append(model_output[4])
temp.append(model_output[4] / np.mean(model_output[0]))
temp = pd.DataFrame(temp).rename(columns={0:i})
fc_index = pd.concat([fc_index, temp], axis=1)  
fc_index = fc_index.set_index(0) 
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')





for i in clipper_db_pivot.columns.values:
    model_output = additive(list(clipper_db_pivot[i]), 12,12)
    temp = list(model_output[0])
    temp.append(model_output[4])
    temp.append(model_output[4] / np.mean(model_output[0]))
    temp = pd.DataFrame(temp).rename(columns={0:i})
    fc_index = pd.concat([fc_index, temp], axis=1)    



fc_index = fc_index.set_index(0)    
path='M:/'
fc_index.to_excel(path+'FlowsForecasts.xlsx')
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)
visualisation = pd.merge(clipper_db_pivot, fc_index, how='left', on=clipper_db_pivot.columns.values)
clipper_db_pivot.columns.values
visualisation = pd.merge(clipper_db_pivot, fc_index, how='left', left_on=clipper_db_pivot.columns.values)
visualisation = clipper_db_pivot.join(fc_index, on=clipper_db_pivot.columns.values  )
clipper_db_pivot
len(clipper_db_pivot.columns.values)
len(fc_index)
len(fc_index.columns.values)
visualisation = pd.merge(clipper_db_pivot, fc_index, how='left', left_on=clipper_db_pivot.columns.values)
clipper_db_pivot.join(fc_index)
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)  
visualisation.iloc[:-2,0]
data = visualisation.iloc[:-2,0]
data.index.month
data.index
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d').date())
data.index
pd.to_datetime(data.index)
index=data.index.month
data.index
data.set_index(pd.to_datetime(data.index))
data = visualisation.iloc[:-2,0]
data
data.index = pd.to_datetime(data.index)
data
data.index.month
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum'
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
data.index.year
pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
columns=data.index.year
data
data.columns.values
data.index = pd.to_datetime(data.index)

seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
data = pd.DataFrame(visualisation.iloc[:-2,0])
data.index = pd.to_datetime(data.index)

seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
seasonal_grid
data = pd.DataFrame(visualisation.iloc[:,0])
data.index = pd.to_datetime(data.index)

seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
data = pd.DataFrame(visualisation.iloc[:-2,0])
data.index = pd.to_datetime(data.index)

seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
seasonal_grid
data = pd.DataFrame(visualisation.iloc[:-2,82])
data.index = pd.to_datetime(data.index)

seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
seasonal_grid
data = pd.DataFrame(visualisation.iloc[:-2,541])
data.index = pd.to_datetime(data.index)

seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
print(seasonal_grid)
data = pd.DataFrame(visualisation.iloc[:-2,682])
data.index = pd.to_datetime(data.index)

seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
print(seasonal_grid)
import seaborn
seaborn.tsplot(seasonal_grid)
seasonal_grid
seaborn.tsplot(seasonal_grid, time=seasonal_grid.index)
seasonal_grid
seasonal_grid[0]
seasonal_grid
seasonal_grid.index
y = np.cumsum(rng.randn(500, 6), 0)
seasonal_grid
seasonal_grid[0]
seasonal_grid.loc[0]
seasonal_grid.iloc[:,:]
seaborn.plot(seasonal_grid)
seaborn.tsplot(seasonal_grid)
seasonal_grid.plot()
import matpltlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
data = pd.DataFrame(visualisation.iloc[:-2,705])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
import seaborn as sns
import matplotlib.pyplot as plt
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)  
data = pd.DataFrame(visualisation.iloc[:-3,705])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)  
data = pd.DataFrame(visualisation.iloc[:-3,705])
data.index = pd.to_datetime(data.index)
fc_index
clipper_db_pivot
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')
for i in clipper_db_pivot.columns.values:
    model_output = additive(list(clipper_db_pivot[i]), 12,12)
    temp = list(model_output[0])
    temp.append(model_output[4])
    temp.append(model_output[4] / np.mean(model_output[0]))
    temp = pd.DataFrame(temp).rename(columns={0:i})
    fc_index = pd.concat([fc_index, temp], axis=1)    

visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)  
visualisation.iloc[:-3,705]
fc_index = fc_index.set_index(0) 
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0) 
visualisation.iloc[:-3,705]
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)  
visualisation.index = pd.to_datetime(visualisation.index)
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)  
data = pd.DataFrame(visualisation.iloc[:-3,705])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
data = pd.DataFrame(visualisation.iloc[:-3,910])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index.index = pd.to_datetime(fc_index.index)
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')
fc_index = pd.to_datetime(fc_index)
fc_index
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
fc_index = pd.to_datetime(fc_index)
pd.to_datetime(fc_index)
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index = pd.to_datetime(fc_index)
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index = pd.to_datetime(fc_index)
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
#fc_index = pd.to_datetime(fc_index)
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')
fc_index = fc_index.set_index(0)   
import seaborn as sns
import matplotlib.pyplot as plt
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)

# Slice the dataframe to cut off the RMSE and the RMSE:mean, convert    
data = pd.DataFrame(visualisation.iloc[:-3,910])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
seasonal_grid
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=32), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
#fc_index = pd.to_datetime(fc_index)
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')


for i in clipper_db_pivot.columns.values:
    model_output = additive(list(clipper_db_pivot[i]), 12,12)
    temp = list(model_output[0])
    temp.append(model_output[4])
    temp.append(model_output[4] / np.mean(model_output[0]))
    temp = pd.DataFrame(temp).rename(columns={0:i})
    fc_index = pd.concat([fc_index, temp], axis=1)    


# set the index of the forecast dataframe to the 0th column we created above  
fc_index = fc_index.set_index(0)    

# export as an excel file
path='M:/'
fc_index.to_excel(path+'FlowsForecasts.xlsx')

# Seaonality charts for visualisations
import seaborn as sns
import matplotlib.pyplot as plt
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)

# Slice the dataframe to cut off the RMSE and the RMSE:mean, convert    
data = pd.DataFrame(visualisation.iloc[:-3,910])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
import seaborn as sns
import matplotlib.pyplot as plt
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)

# Slice the dataframe to cut off the RMSE and the RMSE:mean, convert    
data = pd.DataFrame(visualisation.iloc[:-3,910])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
data = pd.DataFrame(visualisation.iloc[:-3,850])
data.index = pd.to_datetime(data.index)
seasonal_grid = pd.pivot_table(data, index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
data = pd.DataFrame(visualisation.iloc[:-3,:])
data.index = pd.to_datetime(data.index)
data
data['Saudi Arabia_UK CONT']
seasonal_grid = pd.pivot_table(data['Saudi Arabia_UK CONT'], index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
data.index.month
data.index.year
data['Saudi Arabia_UK CONT']
seasonal_grid = pd.pivot_table(pd.DataFrame(data['Saudi Arabia_UK CONT']), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
x = 'Saudi Arabia_UK CONT'

seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'UK CONT'
x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 5), loc=2, borderaxespad=0.)
x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(5, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'UK CONT'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'N ASIA (EXC CHINA)'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'SOUTH EAST ASIA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'INDIA WC'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
seasonal_grid
seasonal_grid['2013']
seasonal_grid[1]
seasonal_grid
seasonal_grid.columns.values
x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'INDIA EC'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'US GULF (padd 3)'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'US ATLANTIC (padd 1)'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
clipper_db_pivot
load_country = 'Saudi Arabia'
dest_region = 'CHINA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Iran'
dest_region = 'CHINA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Yemen'
dest_region = 'CHINA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Algeria'
dest_region = 'CHINA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Aruba'
dest_region = 'CHINA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'CHINA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'North Africa'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'NORTH AFRICA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Iran'
dest_region = 'CHINA'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Algeria'
dest_region = 'MED OTHER'

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'CHINA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Albania'
dest_region = 'US ATLANTIC (padd 1)'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
load_country = 'Saudi Arabia'
dest_region = 'US ATLANTIC (padd 1)'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
data = pd.DataFrame(visualisation.iloc[:-3,:])
data.index = pd.to_datetime(data.index)

# What do we want to look at?
load_country = 'Venezuela'
dest_region = 'China'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Venezuela'
dest_region = 'CHINA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
import seaborn as sns
import matplotlib.pyplot as plt
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)

# Slice the dataframe to cut off the RMSE and the RMSE:mean, convert datetime index    
data = pd.DataFrame(visualisation.iloc[:-3,:])
data.index = pd.to_datetime(data.index)

# What do we want to look at?
load_country = 'Brazil'
dest_region = 'S AMERICA - PACIFIC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Brazil'
dest_region = 'S AMERICA - ATLANTIC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Brazil'
dest_region = 'CHINA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
#####

## ---(Thu Mar  1 11:20:16 2018)---
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE with months.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GradesAssay.py', wdir='C:/Users/mima/.spyder-py3')
GradesNeedSulphur = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSuplur.txt")
import pandas as pd
import pypyodbc

GradesNeedSulphur = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")
import re
import pypyodbc
import pandas as pd
import re
import pypyodbc

GradesNeedSulphur = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''

SELECT TOP (10000) GradeLocationAssay.[Id]
      ,[IdRegionTree]
      ,Grade.Name
      ,[IdLocation]
      ,[IdAssay]
      ,[NumValue]
      ,[StartDate]
  FROM [STG_Targo].[dbo].[GradeLocationAssay]
  INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
  --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
  order by [Name]

'''
import pandas as pd
import re
import pypyodbc

GradesNeedSulphur = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''

data = pd.read_sql(query, cxn)
import pandas as pd
import re
import pypyodbc

# 
GradesNeedSulphur = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''

data = pd.read_sql(query, cxn)
GradesList = list(GradesNeedSulphur)
GradesList = GradesNeedSulphur.tolist()
GradesList = GradesNeedSulphur.CleanGrade.tolist()
string = 'ABO'

pattern = re.compile(string)
matches = [x for x in GradesList if pattern.match(x)]
GradesList = data.Name.tolist()

string = 'ABO'
pattern = re.compile(string)
matches = [x for x in GradesList if pattern.match(x)]
GradesList = data.name.tolist()

string = 'ABO'
pattern = re.compile(string)
matches = [x for x in GradesList if pattern.match(x)]
import pandas as pd
import re
import pypyodbc

# 
GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)

GradesWithSulphur = GradesWithSulphur.name.tolist()

#string = 'ABO'
#pattern = re.compile(string)

matches = []
for grade in GradesToCheck:
    print(type(grade))

GradesToCheck = GradesToCheck.CleanGrade.tolist()
GradesWithSulphur = GradesWithSulphur.name.tolist()
for grade in GradesToCheck:
    print(type(grade))

for grade in GradesToCheck:
    #print(type(grade))
    pattern = re.compile(grade)
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)

for grade in GradesToCheck:
    #print(type(grade))
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)

matches = []

for grade in GradesToCheck:
    #print(type(grade))
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)

matches = []

for grade in GradesToCheck:
    #print(type(grade))
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


cheking = zip(GradesToCheck, matches)
cheking
matches = []

for grade in GradesToCheck:
    #print(type(grade))
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
GradesToCheck = GradesToCheck.CleanGrade.tolist().str.upper()
import pandas as pd
import re
import pypyodbc

# 
GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)


GradesToCheck = GradesToCheck.CleanGrade.tolist().str.upper()
GradesWithSulphur = GradesWithSulphur.name.tolist().str.upper()
import pandas as pd
import re
import pypyodbc

# 
GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)


GradesToCheck = GradesToCheck.CleanGrade.str.upper()
GradesWithSulphur = GradesWithSulphur.name.str.upper()
import pandas as pd
import re
import pypyodbc

# 
GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()
import pandas as pd
import re
import pypyodbc

# 
GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)


GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()
GradesWithSulphur = GradesWithSulphur.name.str.upper().tolist()
import pandas as pd
import re
import pypyodbc

# 
GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)


GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()
GradesWithSulphur = GradesWithSulphur.name.str.upper().tolist()

#string = 'ABO'
#pattern = re.compile(string)

matches = []

for grade in GradesToCheck:
    #print(type(grade))
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
print([x[1] for x in checking])
ListForExcel = list([x[1] for x in checking])
GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)


GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()
GradesWithSulphur = GradesWithSulphur.name.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)


GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()
GradesWithSulphur = GradesWithSulphur.name.unique().str.upper().tolist()
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur.name.unique()
GradesWithSulphur = GradesWithSulphur.name.unique()
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.DataFrame(GradesWithSulphur.name.unique()).str.upper().tolist()
type(GradesToCheck.CleanGrade)
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()
matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade)
    temp_list = ['NAME IN ASSAY' for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade)
    temp_list = ['NAME IN ASSAY' for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade)
    temp_list = ['NAME IN ASSAY'x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade)
    temp_list = ['NAME IN ASSAY'+x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        --WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade)
    temp_list = ['NAME IN ASSAY - '+x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay = 49
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade)
    temp_list = ['NAME IN ASSAY - '+x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay not in (49)
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade)
    temp_list = ['NAME IN ASSAY - '+x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay not in (49)
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade[:3])
    temp_list = ['NAME IN ASSAY - '+x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay not in (49)
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade[:3])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
import pandas as pd
import re
import pypyodbc

GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay not in (49)
        --WHERE Grade.[Name] in ('MURBAN','FORTIES','alba condensate')
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
GradesWithSulphur = pd.Series(GradesWithSulphur.name.unique()).str.upper().tolist()
GradesToCheck = GradesToCheck.CleanGrade.str.upper().tolist()

matches = []

for grade in GradesToCheck:
    pattern = re.compile(grade[:4])
    temp_list = [x for x in GradesWithSulphur if pattern.match(x)]
    matches.append(temp_list)


checking = list(zip(GradesToCheck, matches))
ListForExcel = list([x[1] for x in checking])
load_country = 'Venezuela'
dest_region = 'US GULF (padd 3)'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE with months.py', wdir='C:/Users/mima/.spyder-py3')
load_country = 'Aruba'
dest_region = 'Asia'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Aruba'
dest_region = 'SOUTH EAST ASIA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
#####
load_country = 'Aruba'
dest_region = 'CHINA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Aruba'
dest_region = 'US GULF (padd 3)'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
#####
pd.to_datetime('today').date()-timedelta(days=32)
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE with months.py', wdir='C:/Users/mima/.spyder-py3')
load_country = 'Brazil'
dest_region = 'India EC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Brazil'
dest_region = 'INDIA EC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
#####
load_country = 'Brazil'
dest_region = 'INDIA WC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
#####
load_country = 'Brazil'
dest_region = 'INDIA EC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.
load_country = 'Brazil'
dest_region = 'INDIA EC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
#####
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE with months.py', wdir='C:/Users/mima/.spyder-py3')
load_country = 'Brazil'
dest_region = 'INDIA WC'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Venezuela'
dest_region = 'CHINA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
load_country = 'Uruguay'
dest_region = 'CHINA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
(pd.to_datetime('today').date()-timedelta(days=32)
(pd.to_datetime('today').date()-timedelta(days=32))
(pd.to_datetime('today').date()-timedelta(days=30))
runfile('C:/Users/mima/.spyder-py3/Holt Winter with RMSE with months.py', wdir='C:/Users/mima/.spyder-py3')
pd.to_datetime('today').date()-timedelta(days=30)
pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=1), periods=12, freq='MS'))
pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=30), periods=12, freq='MS'))
(pd.to_datetime('today').date()-timedelta(days=30))
clipper_db_table['months'] = pd.to_datetime(clipper_db_table['loaddate']).dt.month
clipper_db_table['years'] = pd.to_datetime(clipper_db_table['loaddate']).dt.year
clipper_db_table['days'] = 1
clipper_db_table['LD'] = pd.to_datetime(clipper_db_table[['days','months','years']]).dt.date

#create pivot table for to get input data
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity',
                                  index = clipper_db_table['LD'],
                                  columns = ['loadcountry','dischargesubregion'],
                                  aggfunc = 'sum' ).fillna(0)

# flatten the table with the new combined column headers and export
clipper_db_pivot.columns = list([clipper_db_pivot.columns[x][0]+'_'+clipper_db_pivot.columns[x][1] for x in range(0,len(clipper_db_pivot.columns))])

# slice so we only use complete data, i.e. form 2013 onwards
clipper_db_pivot = clipper_db_pivot.iloc[(clipper_db_pivot.index > date(2014,12,31))&
                                         (clipper_db_pivot.index < (pd.to_datetime('today').date()-timedelta(days=30)))]
clipper_db_pivot['DaysInMonth'] = pd.to_datetime(clipper_db_pivot.index)
clipper_db_pivot['DaysInMonth'] = clipper_db_pivot['DaysInMonth'].dt.daysinmonth
clipper_db_pivot = clipper_db_pivot.div(clipper_db_pivot['DaysInMonth']*1000, axis = 'index').fillna(0).drop(['DaysInMonth'], axis = 1)
fc_index = pd.DataFrame(pd.date_range(pd.to_datetime('today').date()-timedelta(days=30), periods=12, freq='MS'))
fc_index.apply(lambda x: x.dt.strftime('%Y/%m/%d'))
#fc_index = pd.to_datetime(fc_index)
fc_index.loc[12] = str('RMSE')
fc_index.loc[13] = str('RMSE vs Mean')
clipper_db_pivot.columns.values
for i in clipper_db_pivot.columns.values:
    model_output = additive(list(clipper_db_pivot[i]), 12,12)
    temp = list(model_output[0])
    temp.append(model_output[4])
    temp.append(model_output[4] / np.mean(model_output[0]))
    temp = pd.DataFrame(temp).rename(columns={0:i})
    fc_index = pd.concat([fc_index, temp], axis=1)    


# set the index of the forecast dataframe to the 0th column we created above  
fc_index = fc_index.set_index(0)    

# export as an excel file
path='M:/'
fc_index.to_excel(path+'FlowsForecasts.xlsx')

# Seaonality charts for visualisations
import seaborn as sns
import matplotlib.pyplot as plt
visualisation = pd.concat([clipper_db_pivot, fc_index], axis=0)

# Slice the dataframe to cut off the RMSE and the RMSE:mean, convert datetime index    
data = pd.DataFrame(visualisation.iloc[:-3,:])
data.index = pd.to_datetime(data.index)

# What do we want to look at?
load_country = 'Uruguay'
dest_region = 'CHINA'

#for dest_region in regwions

x = "{}_{}".format(load_country, dest_region)
seasonal_grid = pd.pivot_table(pd.DataFrame(data[x]), index=data.index.month, columns=data.index.year, aggfunc='sum')
sns.set()
seasonal_grid.plot()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)

seasonal_grid.columns.values
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')

## ---(Fri Mar  9 16:46:49 2018)---
from __future__ import division
from sys import exit
from math import sqrt
from numpy import array
from scipy.optimize import fmin_l_bfgs_b
import pandas as pd
import pypyodbc
from datetime import datetime as dt
from datetime import date
from datetime import timedelta
import numpy as np



# =============================================================================
# must define what our loss function is for the optimiser - here we are using the root mean squared error
# 1) Y is the data series we are looking at
# 2) the type is defined as 'linear', 'mulitiplicative' or 'additive' - will be using additive but others in here for good measure
# and takes the second positional argument
# 3) set rmse = 0 initially
# 4) a is the initial level
# 5) b is the initial trend
# 6) y is the expected value of Y which takes the inital values of a plus the initial values of b, i.e. first level with trend added
# 7) this will give a series of y which we then measure against the true value Y and calculate the rmse
# 8) s is the seaosnal index which when rearranged gives s equation below when you bound inital coefficient to be between 1 and 0
#  https://www.otexts.org/fpp/7/5
# 9) sum of the inidividual components are then added to list y
# 10) the arguments that are passed into the RMSE are given in the optimiser algo
# =============================================================================

def RMSE(params, *args):
 
	Y = args[0]
	type = args[1]
	rmse = 0
 
	if type == 'linear':
  
		alpha, beta = params
		a = [Y[0]] # first data point
		b = [Y[1] - Y[0]] # trend is first minus second, or change in level
		y = [a[0] + b[0]] # hence estimate is first value plus first trend estimate
  
		for i in range(len(Y)):
            # for each sequential point in our data series, update the level and trend calcs and add to estimate list
			a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
			b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
			y.append(a[i + 1] + b[i + 1])
 
	else:
  
		alpha, beta, gamma = params
		m = args[2]		
		a = [sum(Y[0:m]) / float(m)]
		b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
  
		if type == 'additive':
   
			s = [Y[i] - a[0] for i in range(m)] # initial seasonal indicies
			y = [a[0] + b[0] + s[0]] # initial observation is level plus trend plus seasonality
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i])) # take actual minus seasonbal index weighed agfainst expected 
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i]) # current slope estimate based on levels estimate weighed against previous trend value
				s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i]) # seasonality is rearranged version same as above but sub in the levels equation above and bound results in this
				y.append(a[i + 1] + b[i + 1] + s[i + 1])
  
		elif type == 'multiplicative':
   
			s = [Y[i] / a[0] for i in range(m)]
			y = [(a[0] + b[0]) * s[0]]
   
			for i in range(len(Y)):
    
				a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
				b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
				s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
				y.append((a[i + 1] + b[i + 1]) * s[i + 1])
  
		else:
   
			exit('Type must be either linear, additive or multiplicative')
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y, y[:-1])]) / len(Y))
 
	return rmse


def linear(x, fc, alpha = None, beta = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None):
  
		initial_values = array([0.3, 0.1])
		boundaries = [(0, 1), (0, 1)]
		type = 'linear'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type), bounds = boundaries, approx_grad = True)
		alpha, beta = parameters[0]
 
	a = [Y[0]]
	b = [Y[1] - Y[0]]
	y = [a[0] + b[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(a[-1] + b[-1])
  
		a.append(alpha * Y[i] + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		y.append(a[i + 1] + b[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, rmse


def additive(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.3, 0.1, 0.1])
		boundaries = [(0, 0.5), (0, 0.5), (0, 0.5)]
		type = 'additive'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] - a[0] for i in range(m)]
	y = [a[0] + b[0] + s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max((a[-1] + b[-1] + s[-m]),0))
  
		a.append(alpha * (Y[i] - s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] - a[i] - b[i]) + (1 - gamma) * s[i])
		y.append(a[i + 1] + b[i + 1] + s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


def multiplicative(x, m, fc, alpha = None, beta = None, gamma = None):
 
	Y = x[:]
 
	if (alpha == None or beta == None or gamma == None):
  
		initial_values = array([0.0, 1.0, 0.0])
		boundaries = [(0, 1), (0, 1), (0, 1)]
		type = 'multiplicative'
  
		parameters = fmin_l_bfgs_b(RMSE, x0 = initial_values, args = (Y, type, m), bounds = boundaries, approx_grad = True)
		alpha, beta, gamma = parameters[0]
 
	a = [sum(Y[0:m]) / float(m)]
	b = [(sum(Y[m:2 * m]) - sum(Y[0:m])) / m ** 2]
	s = [Y[i] / a[0] for i in range(m)]
	y = [(a[0] + b[0]) * s[0]]
	rmse = 0
 
	for i in range(len(Y) + fc):
  
		if i == len(Y):
			Y.append(max(((a[-1] + b[-1]) * s[-m]),0))
  
		a.append(alpha * (Y[i] / s[i]) + (1 - alpha) * (a[i] + b[i]))
		b.append(beta * (a[i + 1] - a[i]) + (1 - beta) * b[i])
		s.append(gamma * (Y[i] / (a[i] + b[i])) + (1 - gamma) * s[i])
		y.append((a[i + 1] + b[i + 1]) * s[i + 1])
 
	rmse = sqrt(sum([(m - n) ** 2 for m, n in zip(Y[:-fc], y[:-fc - 1])]) / len(Y[:-fc]))
 
	return Y[-fc:], alpha, beta, gamma, rmse


clipper_db_connection = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

clipper_db_query = '''exec sp_CurrentStems @datasource = 'clipper' '''
clipper_db_table = pd.read_sql(clipper_db_query, clipper_db_connection)
clipper_db_table = pd.read_excel('C:\Users\mima\Desktop\US Balance2.xlsb', sheet_name = 'CLIPPER_TARGO')
clipper_db_table = pd.read_excel('C:\\Users\\mima\\Desktop\\US Balance2.xlsb', sheet_name = 'CLIPPER_TARGO')
clipper_db_table = pd.read_excel('M:\CLIPPER_TARGO.xlsx', sheet_name = 'CLIPPER_TARGO')
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='quantity',
                                  index = clipper_db_table['Custom_Load_Month'],
                                  columns = ['loadcountry','dischargesubregion'],
                                  aggfunc = 'sum' ).fillna(0)
clipper_db_pivot = pd.pivot_table(clipper_db_table, values='Quantity',
                                  index = clipper_db_table['Custom_Load_Month'],
                                  columns = ['SweetSour','LoadCountry','DischargeSubRegion'],
                                  aggfunc = 'sum' ).fillna(0)
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')

## ---(Wed Mar 21 11:01:08 2018)---
runfile('C:/Users/mima/.spyder-py3/mango test.py', wdir='C:/Users/mima/.spyder-py3')
assay = {
        'LPG': {'yield': 0.0131},
       'LVN': {'yield': 0.0691},
       'HVN': {'yield': 0.0915},
       'KERO': {'yield': 0.1544},
       'LGO': {'yield': 0.1525, 'density': 0.8543},
       'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
       'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
       'Residue': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
       }

print(assay)
print(type(assay))
assay['LPG']
assay['LPG']['yield]
assay['LPG']['yield']
assay.hvn.yield
assay.hvn
assay['HVN']['yield']
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
utlised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12
utlised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utlised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - ultilised_fcc_cap, kero_input), 0)
surplus_for_naphtha = np.maxiumum(hvn_input - ultilised_fcc_cap, 0)
import numpy as np 
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utlised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utlised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - ultilised_fcc_cap, kero_input), 0)
surplus_for_naphtha = np.maxiumum(hvn_input - ultilised_fcc_cap, 0)
utilised_ref_cap = 43 * 0.97
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - ultilised_fcc_cap, kero_input), 0)
surplus_for_naphtha = np.maxiumum(hvn_input - ultilised_fcc_cap, 0)
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maxiumum(hvn_input - utilised_ref_cap, 0)
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
"""
import numpy as np 

class Asset():
    '''A standard refinery
    
    Attributes: 
        volume: integer representing the cpacity of the unit
        location: where the unit is located for freight costs
        name: name of the unit
        reformer: reformer function
        fcc: fcc function
        coker: coker function
        thermal cracker: thermal cracker function
    
    '''







refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
import numpy as np 

class Asset():
    '''A standard refinery
    
    Attributes: 
        volume: integer representing the cpacity of the unit
        location: where the unit is located for freight costs
        name: name of the unit
        reformer: reformer function
        fcc: fcc function
        coker: coker function
        thermal cracker: thermal cracker function
    
    '''







refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = assay['HVN']['yield'] * refinery_volume
kero_input = assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
import numpy as np 

#class Asset():
    '''A standard refinery
    
    Attributes: 
        volume: integer representing the cpacity of the unit
        location: where the unit is located for freight costs
        name: name of the unit
        reformer: reformer function
        fcc: fcc function
        coker: coker function
        thermal cracker: thermal cracker function
    
    '''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.15
kero_gasolinepool = 0.12

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER

utilised_ref_cap = 43 * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.0}
        }
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 48

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 48

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 48

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)


reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_bx = reformer_assay['gasoline']['yield'] * reformer_volume
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 48

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)


reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = 48 * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = assay_yield_vgo * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - ultilised_fcc_cap, 0)

c3_c4_yield = 0.02
c3__yield = 0.02
lco_yield = 0.20
clo_yield = 0.20
gasoline_yield = 0.56

fcc_c3_c4 = fcc_assay['c3_c4']['yield'] * fcc_volume
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - ultilised_fcc_cap, 0)

fcc_c3_c4 = fcc_assay['c3_c4']['yield'] * fcc_volume
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - ultilised_fcc_cap, 0)

fcc_c3_c4 = fcc_assay['c3_c4']['yield'] * fcc_volume
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_c3_c4 = fcc_assay['c3_c4']['yield'] * fcc_volume
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 2.62},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 3.23},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 5.84, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_propane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_butane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume
crude_assay['RESIDUE']['sulphur']
residue_to_blend = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.35:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor

crude_assay['RESIDUE']['density']
residue_density = crude_assay['RESIDUE']['density'] * 1000
residue_sulphur_weight = residue_to_blend * resid_sulphur * residue_density / 6.29
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 0.0262},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 0.0323},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 0.0584, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_propane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_butane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue_to_blend = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.35:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


#
residue_density = crude_assay['RESIDUE']['density'] * 1000
residue_sulphur_weight = residue_to_blend * resid_sulphur * residue_density / 6.29 
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 0.0262},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 0.0323},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 0.0584, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_propane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_butane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue_to_blend = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


#
residue_density = crude_assay['RESIDUE']['density'] * 1000
crude_assay['RESIDUE']['sulphur'] 
residue_to_blend = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor

residue_density = crude_assay['RESIDUE']['density'] * 1000
residue_sulphur_weight = residue_to_blend * resid_sulphur * residue_density / 6.29
residue_viscosity_index = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47
imported_vgo = 0  
available_diluent = surplus_vgo + fcc_clo + fcc_lco + imported_vgo
imported_vgo = 0  
available_diluent = surplus_vgo + fcc_clo + imported_vgo
imported_vgo = 0  
available_diluent = surplus_vgo + fcc_clo + imported_vgo

available_diluent_sulphur = 0.001
available_diluent_density = 0.897
available_diluent_sulphur_weight = available_diluent * available_diluent_sulphur * available_diluent_density / 6.29
available_diluent_viscosity = 3 # this is the base viscosity
available_diluent_viscosity_index = 23.1 + np.log10((np.log10(available_diluent_viscosity+0.8)) * 33.47
available_diluent_sulphur = 0.001
available_diluent_density = 0.897
available_diluent_sulphur_weight = available_diluent * available_diluent_sulphur * available_diluent_density / 6.29
available_diluent_viscosity = 3 # this is the base viscosity
available_diluent_viscosity_index = 23.1 + np.log10(np.log10(available_diluent_viscosity+0.8)) * 33.47
imported_vgo = 0  
available_diluent = surplus_vgo + fcc_clo + imported_vgo

available_diluent_sulphur = 0.001
available_diluent_density = 0.897
available_diluent_sulphur_weight = available_diluent * available_diluent_sulphur * available_diluent_density / 6.29
available_diluent_viscosity = 3 # this is the base viscosity
available_diluent_viscosity_index = 23.1 + np.log10(np.log10(available_diluent_viscosity+0.8)) * 33.47

diluent_used = 0
leftover_diluent = available_diluent - diluent_used '# need to have an optimizer function of some sort'
imported_vgo = 0  
available_diluent = surplus_vgo + fcc_clo + imported_vgo

available_diluent_sulphur = 0.001
available_diluent_density = 0.897
available_diluent_sulphur_weight = available_diluent * available_diluent_sulphur * available_diluent_density / 6.29
available_diluent_viscosity = 3 # this is the base viscosity
available_diluent_viscosity_index = 23.1 + np.log10(np.log10(available_diluent_viscosity+0.8)) * 33.47

diluent_used = 0
leftover_diluent = available_diluent - diluent_used # need to have an optimizer function of some sort
available_diluent_assay = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
available_diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
available_diluent['volume']
residue_to_blend = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    resid_sulphur = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


residue_density = crude_assay['RESIDUE']['density'] * 1000
residue_sulphur_weight = residue_to_blend * resid_sulphur * residue_density / 6.29 # convert the bbls volume to tonnes
residue_viscosity_index = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  

available_diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }


available_diluent_sulphur_weight = available_diluent['volume'] * available_diluent['sulphur'] * available_diluent['density'] / 6.29
imported_vgo = 0  

available_diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }


available_diluent_sulphur_weight = available_diluent['volume'] * available_diluent['sulphur'] * available_diluent['density'] * 1000 / 6.29
imported_vgo = 0  

available_diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }


available_diluent_sulphur_weight = available_diluent['volume'] * available_diluent['sulphur'] * available_diluent['density'] * 1000 / 6.29
available_diluent_viscosity_index = 23.1 + np.log10(np.log10(available_diluent['viscosity']+0.8)) * 33.47
diluent_used = 0
leftover_diluent = available_diluent['volume'] - diluent_used # need to have an optimizer function of some sort
diluent_fo_sulphur = diluent_used * available_diluent['density'] * 1000 / 6.29 * available_diluent['sulphur']
diluent_fo_sulphur_weight = diluent_used * available_diluent['density'] * 1000 / 6.29
residue_assay = {}
residue = {}
residue['volume'] = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


residue['density'] = crude_assay['RESIDUE']['density']
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity'] = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47 # this is a set conversion calculation
available_diluent['sulphurW'] = available_diluent['volume'] * available_diluent['sulphur'] * available_diluent['density'] * 1000 / 6.29
available_diluent['sulphurW'] = available_diluent['volume'] * available_diluent['sulphur'] * available_diluent['density'] * 1000 / 6.29
available_diluent['viscosity_index'] = 23.1 + np.log10(np.log10(available_diluent['viscosity'] + 0.8)) * 33.47
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 0.0262},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 0.0323},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 0.0584, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_propane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_butane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


residue['density'] = crude_assay['RESIDUE']['density']
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity'] = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  

available_diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }


available_diluent['sulphurW'] = available_diluent['volume'] * available_diluent['sulphur'] * available_diluent['density'] * 1000 / 6.29
available_diluent['viscosity_index'] = 23.1 + np.log10(np.log10(available_diluent['viscosity'] + 0.8)) * 33.47
imported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 0
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['sulphur_per_t'] = diluent_used * diluent['density'] * 1000 / 6.29
imported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 5
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['sulphur_per_t'] = diluent_used * diluent['density'] * 1000 / 6.29
residue
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 

#class Asset():
'''A standard refinery

Attributes: 
    volume: integer representing the cpacity of the unit
    location: where the unit is located for freight costs
    name: name of the unit
    reformer: reformer function
    fcc: fcc function
    coker: coker function
    thermal cracker: thermal cracker function

'''






refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 0.0262},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 0.0323},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 0.0584, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42

reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_c3_c4 = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_h2 = reformer_assay['h2']['yield'] * reformer_volume
reformer_bx = reformer_assay['bx']['yield'] * reformer_volume
reformer_gasoline = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48

fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_propane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_butane = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_c3__ = fcc_assay['c3__']['yield'] * fcc_volume
fcc_lco = fcc_assay['lco']['yield'] * fcc_volume
fcc_clo = fcc_assay['clo']['yield'] * fcc_volume
fcc_gasoline = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


residue['density'] = crude_assay['RESIDUE']['density']
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity'] = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 5
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
mported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 10
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
imported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 10
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']

target_viscosity = 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index']
                                    + residue['volume'] / (diluent_used + residue['volume']) * diluent['viscosity_index'])-23.1)/33.47)) -0.8
target_viscosity = 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index']
                                    + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
residue = {}
residue['volume'] = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


residue['density'] = crude_assay['RESIDUE']['density']
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 10
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']

target_viscosity = 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index']
                                    + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
import numpy as np 

refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 0.0262},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 0.0323},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 0.0584, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_output['c3_c4'] = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
reformer_output['bx'] = reformer_assay['bx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48
fcc_output = {}
fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


residue['density'] = crude_assay['RESIDUE']['density']
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 10
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']

target_viscosity = 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index']
                                    + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight







lpg = crude_assay['LPG']['yield'] * refinery_volume + reformer_output['c3_c4'] + fcc_output['c3_c4'] + fcc_output['c3__']
import numpy as np 

refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 0.0262},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 0.0323},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 0.0584, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']
### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'bx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_output['c3_c4'] = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
reformer_output['bx'] = reformer_assay['bx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48
fcc_output = {}
fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = crude_assay['VGO']['yield'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = crude_assay['RESIDUE']['yield'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if crude_assay['RESIDUE']['sulphur'] < 0.035:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = crude_assay['RESIDUE']['sulphur'] * low_HDS_factor


residue['density'] = crude_assay['RESIDUE']['density']
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(crude_assay['RESIDUE']['viscosity']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  

diluent = {
        'volume': surplus_vgo + fcc_clo + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 10
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']

target_viscosity = 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index']
                                    + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight







lpg = crude_assay['LPG']['yield'] * refinery_volume + reformer_output['c3_c4'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']
surplus_for_naphtha
eformer_output['btx']
reformer_output['btx']
reformer_capacity = 42
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = crude_assay['HVN']['yield'] * refinery_volume
kero_input = crude_assay['KERO']['yield'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_output['c3_c4'] = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
final_yields = {}

final_yields['lpg'] = crude_assay['LPG']['yield'] * refinery_volume + reformer_output['c3_c4'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']
final_yields['naphtha'] = crude_assay['LVN']['yield'] * refinery_volume * (1-lvn_gasolinepool) + surplus_for_naphtha
final_yields['btx'] = reformer_output['btx']
lco_for_blending
runfile('C:/Users/mima/.spyder-py3/refinery model.py', wdir='C:/Users/mima/.spyder-py3')
fcc_output['clo']
runfile('C:/Users/mima/.spyder-py3/refinery model.py', wdir='C:/Users/mima/.spyder-py3')
final_yields['ulsd'] = (crude_assay['LGO']['yield'] + crude_assay['HGO']['yield']) * refinery_volume
print(final_yields)                                       
runfile('C:/Users/mima/.spyder-py3/refinery model.py', wdir='C:/Users/mima/.spyder-py3')
crude_assay['KERO']['yield']
kero_gasolinepool
surplus_for_jet
runfile('C:/Users/mima/.spyder-py3/refinery model.py', wdir='C:/Users/mima/.spyder-py3')
final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 100) 
runfile('C:/Users/mima/.spyder-py3/refinery model.py', wdir='C:/Users/mima/.spyder-py3')
residue['viscosity_index']

## ---(Tue Mar 27 10:24:36 2018)---
import pandas as pd
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm' sheetname = 'Upload_Test', header = True)
import pandas as pd

raw_assay = pd.read_xlsm('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheet_name = 'Upload_Test', header = True)
import pandas as pd

raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheet_name = 'Upload_Test', header = True)
runfile('C:/Users/mima/.spyder-py3/Refinery.py', wdir='C:/Users/mima/.spyder-py3')
names=["lloyd", "alice", "tyler"]
keys=["homework", "quizzes", "tests"]
dic={ name.capitalize():{ key:[] for key in keys} for name in names}
dic
names=["lloyd", "alice", "tyler"]
keys=["homework", "quizzes", "tests"]
dic={ 
     name.capitalize():
         { 
                 key:'mike' for key in keys
         } for name in names
    }
dic
{'Tyler': {'quizzes': [], 'tests': [], 'homework': []}, 
 'Lloyd': {'quizzes': [], 'tests': [], 'homework': []},
 'Alice': {'quizzes': [], 'tests': [], 'homework': []}}
key:'mike' for key in keys
names=["lloyd", "alice", "tyler"]
keys=["homework", "quizzes", "tests"]
dic={ 
     name.capitalize():
         { 
                 key:['mike'] for key in keys
         } for name in names
    }
dic
names=["lloyd", "alice", "tyler"]
keys=["homework", "quizzes", "tests"]
dic={ 
     name.capitalize():
         { 
                 key:'mike' for key in keys
         } for name in names
    }
dic
crudes = []

things = [{'thing_id': row[0], 'thing_name': row[1]} for row in raw_assay
things = [{'thing_id': row[0], 'thing_name': row[1]} for row in raw_assay]
for row in raw_assay:
    print(row)

[print(row) for row in raw_assay]
mike = [print(row) for row in raw_assay]
mike = [print(row) for row in raw_assay.itertuples()]
mike.head()
mike = [row for row in raw_assay.itertuples()]
mike = [row[1] for row in raw_assay.itertuples()]
mike = [row for row in raw_assay[0:5].itertuples()]
[print(row) for row in raw_assay[0:5].itertuples()]
[print(row) for row in raw_assay[0].itertuples()]
[print(row) for row in raw_assay[0:1].itertuples()]
[print(row[0:5]) for row in raw_assay[0:1].itertuples()]
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')
crudes = raw_assay.to_dict('index')
crudes['Agbami']
crudes.Agbami
crudes.agbami
crudes['Agbami']
crude = 'Brent'

crudes[crude]['HVN']
import numpy as np 
import pandas as pd

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crudes = raw_assay.to_dict('index')


### INPUT CRUDE ###
crude = 'Brent'

crudes[crude]['HVN']
import numpy as np 
import pandas as pd

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')


### INPUT CRUDE ###
crude = 'Brent'

assay[crude]['HVN']
refinery_volume = 200
assay[crude]['HVN']
assay[crude]['VGO'] 
assay[crude]['RESIDUE']
assay[crude]['RESIDUE_sulphur'] 
assay[crude]['RESIDUE_density'] 
assay[crude]['RESIDUE_viscosity']
assay[crude]['RESIDUE_v50']
crude = 'Brent'

assay[crude]['HVN']
refinery_volume = 200

crude_assay = {
                'LPG': {'yield': 0.0131},
               'LVN': {'yield': 0.0691},
               'HVN': {'yield': 0.0915},
               'KERO': {'yield': 0.1544},
               'LGO': {'yield': 0.1525, 'density': 0.8543},
               'HGO': {'yield': 0.0574, 'density': 0.8973, 'sulphur': 0.0262},
               'VGO': {'yield': 0.2282, 'density': 0.9386, 'sulphur': 0.0323},
               'RESIDUE': {'yield': 0.2338, 'density': 1.0613, 'sulphur': 0.0584, 'viscosity': 35500000}
               }


crude_assay['HVN']['yield']





### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_output['c3_c4'] = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48
fcc_output = {}
fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = assay[crude]['VGO'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  
lco_for_blending = 0

diluent = {
        'volume': surplus_vgo + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent_used = 10
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']

target_viscosity = 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index']
                                    + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight



final_yields = {}

final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['c3_c4'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + surplus_for_naphtha) / refinery_volume
final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + surplus_for_jet) / refinery_volume
final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
final_yields['gasoil'] = (fcc_output['lco'] - lco_for_blending + leftover_diluent) / refinery_volume
final_yields['lsfo'] = (diluent_used + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 

print(final_yields)   
diluent_used = 0
x = diluent_used
x[0]
x = diluent_used
x[0]
x = [diluent_used]
x[0]
diluent['volume']
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent_used - diluent['volume'] })
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index']
                                    + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8

"""
Created on Tue Mar 27 16:46:57 2018

@author: mima
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 
import pandas as pd

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')


### INPUT CRUDE ###
crude = 'Brent'

refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_output['c3_c4'] = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48
fcc_output = {}
fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = assay[crude]['VGO'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  
lco_for_blending = 0
diluent_used = 0

diluent = {
        'volume': surplus_vgo + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }




diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

"""
Created on Tue Mar 27 16:46:57 2018

@author: mima
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 
import pandas as pd

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')


### INPUT CRUDE ###
crude = 'Brent'

refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_output['c3_c4'] = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48
fcc_output = {}
fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = assay[crude]['VGO'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume



# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  
lco_for_blending = 0
diluent_used = 0

diluent = {
        'volume': surplus_vgo + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }




diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47

diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 300})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) < 380})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) < 380})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'L-BFGS-B', constraints=cons))                         
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'L-BFGS-B', constraints=cons))    
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))  
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda target_viscosity(diluent_used): target_viscosity(diluent_used) - 380})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda target_viscosity: target_viscosity(diluent_used) - 380})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))    
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type':'ineq', 'fun': lambda target_viscosity(): target_viscosity(diluent_used) - 380})  

from scipy.optimize import minimize

print(minimize(lambda x: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))    
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))   
diluent_used
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used) - 380, [0], method = 'COBYLA', constraints=cons))     
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used,) [0], method = 'COBYLA', constraints=cons)) 
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))  
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))                         
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 370})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))          
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 350})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))   
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 300})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))   
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  

from scipy.optimize import minimize

print(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons))
from scipy.optimize import minimize
minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)) 
minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
type(minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)) 
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  

res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
res.x  
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
res.x[0] 
imported_vgo = 0  
lco_for_blending = 0
diluent_used = 0

diluent = {
        'volume': surplus_vgo + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29








def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  

res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent_used = res.x[0]   
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29



combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
surplus_for_jet
refinery_volume = 200
print(crude)
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
surplus_for_jet
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
crude_price = 100
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = '29/03/2018'
expiry_date = '20/03/2018'
basis = 'CMA'
if basis = 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude price + structure
    else:
        crude_diff_local_index = crude price

basis
if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude price + structure
    else:
        crude_diff_local_index = crude price

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
    else:
        crude_diff_local_index = crude_price

crude_price = 100
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = '29/02/2018'
expiry_date = '20/03/2018'
basis = 'CMA'
if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
    else:
        crude_diff_local_index = crude_price

crude_price = 100
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = '28/02/2018'
expiry_date = '20/03/2018'
basis = 'CMA'
if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
    else:
        crude_diff_local_index = crude_price

crude_price = 100
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = '28/02/2018'
expiry_date = '20/03/2018'
basis = 'CMA'

# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price

date < expiry_date
from datetime import datetime
date = datetime(2018,2,1)
date = datetime(2018,2,1)
expiry_date = datetime(2018,3,1)
from datetime import datetime

crude_price = 100
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = datetime(2018,4,1)
expiry_date = datetime(2018,3,1)
basis = 'CMA'



# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price

crude_price = 100
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = datetime(2018,2,1)
expiry_date = datetime(2018,3,1)
basis = 'CMA'



# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price

crude_diff_brent_futs = crude_diff_local_index - brent_wti_2m_spread
crude_diff_bfoe = crude_diff_brent_futs - brent_efp_M2 # this gets the value vs BFOE
crude_diff_dated = crude_diff_bfoe - cfd # this gives us value vs dated  *** this will need to be a function that looks at the wk x vs wk y cfd strructure and chooses the stronger 
from datetime import datetime

crude_price = 2.5
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = datetime(2018,2,1)
expiry_date = datetime(2018,3,1)
basis = 'CMA'



# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price


'US to Europe conversion' # convert to vs dated       
crude_diff_brent_futs = crude_diff_local_index - brent_wti_2m_spread
crude_diff_bfoe = crude_diff_brent_futs - brent_efp_M2 # this gets the value vs BFOE
crude_diff_dated = crude_diff_bfoe - cfd # this gives us value vs dated  *** this will need to be a function that looks at the wk x vs wk y cfd strructure and chooses the stronger 
port_costs = 0.05
other_costs = 0.5
port_costs = port_costs + other_costs
from datetime import datetime

crude_price = 2.5
structure = 0.5
brent_wti_2m_spread = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = datetime(2018,2,1)
expiry_date = datetime(2018,3,1)
basis = 'CMA'
port_costs = 0.05
other_costs = 0.5

'NEed to name/label world scale routes and attach to neccessray journies'

# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price


'US to Europe conversion' # convert to vs dated       
crude_diff_brent_futs = crude_diff_local_index - brent_wti_2m_spread
crude_diff_bfoe = crude_diff_brent_futs - brent_efp_M2 # this gets the value vs BFOE
crude_diff_dated = crude_diff_bfoe - cfd # this gives us value vs dated  *** this will need to be a function that looks at the wk x vs wk y cfd strructure and chooses the stronger 

'US to East'  # convert to vs dubai
crude_diff_brent_futs = crude_diff_local_index - brent_wti_2m_spread
crude_diff_dubai = crude_diff_brent_futs + brent_dubai_2m

# Port costs
port_costs = port_costs + other_costs 
from datetime import datetime

crude_price = 2.5
structure = 0.5
brent_wti_2m_spread = 3.50
brent_dubai_2m = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = datetime(2018,2,1)
expiry_date = datetime(2018,3,1)
basis = 'CMA'
port_costs = 0.05
other_costs = 0.5

'NEed to name/label world scale routes and attach to neccessray journies'

# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price


'US to Europe conversion' # convert to vs dated       
crude_diff_brent_futs = crude_diff_local_index - brent_wti_2m_spread
crude_diff_bfoe = crude_diff_brent_futs - brent_efp_M2 # this gets the value vs BFOE
crude_diff_dated = crude_diff_bfoe - cfd # this gives us value vs dated  *** this will need to be a function that looks at the wk x vs wk y cfd strructure and chooses the stronger 

'US to East'  # convert to vs dubai
crude_diff_brent_futs = crude_diff_local_index - brent_wti_2m_spread
crude_diff_dubai = crude_diff_brent_futs + brent_dubai_2m
import pandas as pd
import re
import pypyodbc

#GradesToCheck = pd.read_csv("L:\TRADING\ANALYSIS\Python\MIMA\ClipperTargoStuff\GradesNeedSulphur.txt")

cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS112;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''
        
        SELECT GradeLocationAssay.[Id]
              ,[IdRegionTree]
              ,Grade.Name
              ,[IdLocation]
              ,[IdAssay]
              ,[NumValue]
              ,[StartDate]
        FROM [STG_Targo].[dbo].[GradeLocationAssay]
          INNER JOIN [STG_Targo].[dbo].[Grade] as Grade on [STG_Targo].[dbo].[GradeLocationAssay].IdGrade = Grade.Id
        WHERE [STG_Targo].[dbo].[GradeLocationAssay].idassay not in (49)
        ORDER BY [Name]

'''
GradesWithSulphur = pd.read_sql(query, cxn)
crudes = raw_assay.to_dict('index')
crudes['Agbami']

crudes = []

things = [{'thing_id': row[0], 'thing_name': row[1]} for row in raw_assay]

mike = [print(row[0:5]) for row in raw_assay[0:1].itertuples()]
mike.head()
import pandas as pd

raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crudes = raw_assay.to_dict('index')
crudes['Agbami']

crudes = []

things = [{'thing_id': row[0], 'thing_name': row[1]} for row in raw_assay]

mike = [print(row[0:5]) for row in raw_assay[0:1].itertuples()]
mike.head()
import pandas as pd

raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crudes = raw_assay.to_dict('index')
crudes['Agbami']

crudes = []

things = [{'thing_id': row[0], 'thing_name': row[1]} for row in raw_assay]

mike = [print(row[0:5]) for row in raw_assay[0:1].itertuples()]
print(mike)
crudes
raw_assay.to_dict('index')
crudes['Agbami']
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crudes = raw_assay.to_dict('index')
crudes['Agbami']
crudes['Agbami']
crudes[0]
crudes
crudes[0,0]
things = [{'thing_id': row[0], 'thing_name': row[1]} for row in raw_assay]
crudes.keys()
crudes.viewkeys()
crudes = raw_assay.to_dict('index')
crudes.keys()
k = crudes.viewkeys()
k = crudes.values()
crudes.values()
crudes.items()
crudes.keys()
imported_vgo = 0  
lco_for_blending = 0
diluent_used = 0

diluent = {
        'volume': surplus_vgo + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29

# create the 'cost' function - what do we want to minimise?
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent_used = res.x[0]   
print(diluent_used)
"""
Created on Tue Mar 27 16:46:57 2018

@author: mima
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')


### INPUT CRUDE ###
crude = 'Forties'
print(crude)

refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 42
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)

reformer_output['c3_c4'] = reformer_assay['c3_c4']['yield'] * reformer_volume
reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume



# FCC    
fcc_capacity = 48
fcc_output = {}
fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
vgo_available = assay[crude]['VGO'] * refinery_volume
fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)

fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume





# FO BLENDING

### RESIDUE INPUT ###
residue = {}
residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  
lco_for_blending = 0
diluent_used = 0

diluent = {
        'volume': surplus_vgo + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29

# create the 'cost' function - what do we want to minimise?
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent_used = res.x[0]   
print(diluent_used)
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29



combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 5})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent_used = res.x[0]   
diluent_used
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 5000})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent_used = res.x[0]   
print(diluent_used)
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8

# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used)+ 5000})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent_used = res.x[0]   
print(diluent_used)
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used)+ 5000})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent_used = res.x[0]   
print(diluent_used)
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
target_viscosity(diluent_used)
leftover_diluent = diluent['volume'] - diluent_used # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29



combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight




final_yields = {}

final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['c3_c4'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + surplus_for_naphtha) / refinery_volume
final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + surplus_for_jet) / refinery_volume
final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
final_yields['gasoil'] = (fcc_output['lco'] - lco_for_blending + leftover_diluent) / refinery_volume
final_yields['lsfo'] = (diluent_used + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 

print(final_yields)                                        
fcc_output['clo'] 
surplus_vgo
print(crude)
assay
print(assay[crude])
reformer_output['h2']
reformer_assay['h2']['yield']
reformer_output['c3_c4']
reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
surplus_for_jet
surplus_for_naphtha
refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

# REFORMER
reformer_capacity = 0
reformer_output = {}
reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }

utilised_ref_cap = reformer_capacity * 0.97
hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)

#
surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
surplus_for_jet
surplus_for_naphtha
kero_input
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)


#
    surplus_for_jet = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    surplus_for_naphtha = np.maximum(hvn_input - utilised_ref_cap, 0)
    
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output['propane'], reformer_output['butane'], reformer_output['h2'], reformer_output['btx'], reformer_output['gasoline'], surplus_for_jet, surplus_for_naphtha 
reformer()
reformer(reformer_capacity=10)
def fcc(fcc_cpacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)
    
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output['propane'], fcc_output['butane'], fcc_output['c3__'], fcc_output['lco'], fcc_output['clo'], fcc_output['gasoline'], surplus_vgo

fcc()  
def fcc(fcc_cpacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)
    
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output, fcc_output['propane'], fcc_output['butane'], fcc_output['c3__'], fcc_output['lco'], fcc_output['clo'], fcc_output['gasoline'], surplus_vgo

fcc()    
def fcc(fcc_cpacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    surplus_vgo = np.maximum(vgo_available - utilised_fcc_cap, 0)
    
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output, surplus_vgo



fcc()
fcc_cpacity = refinery_volume*0.21
fcc_capacity
fcc()    
def fcc(fcc_cpacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)


#
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output
final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
final_yields
diluent
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')



### INPUT CRUDE ###
crude = 'Forties'
print(assay[crude])

refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')



### INPUT CRUDE ###
crude = 'Forties'
print(assay[crude])

refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15
def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)


#
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output
def fcc(fcc_cpacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    lco_for_blending = 0
    diluent_used = 0
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent, lco_for_blending, target_sulphur

reformer()
fcc()
fo_blend()


final_yields = {}

final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
final_yields['gasoil'] = (fcc_output['lco'] - lco_for_blending + diluent['surplus_after_blend']) / refinery_volume
final_yields['lsfo'] = (diluent_used + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 

print(final_yields)           
def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)


#
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output
#reformer(reformer_capacity=10)

def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

#fcc()    

def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    lco_for_blending = 0
    diluent_used = 0
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent, lco_for_blending, target_sulphur

reformer()
fcc()
fo_blend()
fcc_output
def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume

def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

reformer()
fcc()
fo_blend()
def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

#fcc()    

def fo_blend(fcc_output=fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    lco_for_blending = 0
    diluent_used = 0
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent, lco_for_blending, target_sulphur

def fo_blend(fcc_output=fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    lco_for_blending = 0
    diluent_used = 0
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent, lco_for_blending, target_sulphur

reformer()
fcc()
fo_blend()
def fo_blend(fcc_output=fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    lco_for_blending = 0
    diluent_used = 0
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent, lco_for_blending, target_sulphur

fcc_output
fcc()
def fcc_output(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

fcc_output()
fcc_output['clo']
fcc_output()
fcc_output
def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

#fcc()    

fcc()
fcc_output
type(fcc())
fcc_output = fcc() 
def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    lco_for_blending = 0
    diluent_used = 0
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent, lco_for_blending, target_sulphur

'volume': fcc_output['surplus_vgo'] + fcc_output['clo']
fcc_output['clo']
def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    lco_for_blending = 0
    diluent_used = 0
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent_used * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent_used * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    target_sulphur = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent, lco_for_blending, target_sulphur

reformer_output = reformer()
fcc_output = fcc()
reformer_output = reformer()
fcc_output = fcc()
fo_blend = fo_blend()
fo_blend
reformer_output
fcc_output = fcc() 

def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

fo_blend = fo_blend()
fo_blend
def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent['volume_used']: diluent['volume'] - diluent['volume_used']},
             {'type': 'ineq', 'fun': lambda diluent['volume_used']: target_viscosity(diluent['volume_used']) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent['volume_used']: target_viscosity(diluent['volume_used']), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight

diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'volume_used': 0,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': 0
        }
imported_vgo = 0  

diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'volume_used': 0,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': 0
        }
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29

# create the 'cost' function - what do we want to minimise?
def target_viscosity(diluent['volume_used']):
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8

diluent
def target_viscosity(diluent['volume_used']):
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8

diluent['volume_used']
def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': 0
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent['volume_used']):
        return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent['volume_used']: diluent['volume'] - diluent['volume_used']},
             {'type': 'ineq', 'fun': lambda diluent['volume_used']: target_viscosity(diluent['volume_used']) - 375})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent['volume_used']: target_viscosity(diluent['volume_used']), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

def target_viscosity(diluent['volume_used']):

diluent['volume_used']
diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'volume_used': 0,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': 0
        }
diluent
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
ons = ({'type': 'ineq', 'fun': lambda diluent['volume_used']: diluent['volume'] - diluent['volume_used']},
         {'type': 'ineq', 'fun': lambda diluent['volume_used']: target_viscosity(diluent['volume_used']) - 375})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent['volume_used']: target_viscosity(diluent['volume_used']), [0], method = 'COBYLA', constraints=cons)     
diluent['volume_used'] = res.x[0]

# =============================================================================
# Left blank for the addition of if vgo is needed to be imported
# Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
# 
# if target_viscosity(diluent_used) > 380:
#     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
#     
# =============================================================================

diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
cons = ({'type': 'ineq', 'fun': lambda diluent['volume_used']: diluent['volume'] - diluent['volume_used']},
         {'type': 'ineq', 'fun': lambda diluent['volume_used']: target_viscosity(diluent['volume_used']) - 375}) 
res = minimize(lambda diluent['volume_used']: target_viscosity(diluent['volume_used']), [0], method = 'COBYLA', constraints=cons)     
diluent['volume_used'] = res.x[0]
diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
def target_viscosity(diluent['volume_used']):
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   

diluent[0]
(diluent['volume_used']
diluent['volume_used']
def target_viscosity():
    diluent['volume_used']= 5
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   

cons = ({'type': 'ineq', 'fun': lambda diluent['volume_used']: diluent['volume'] - diluent['volume_used']},
         {'type': 'ineq', 'fun': lambda diluent['volume_used']: target_viscosity(diluent['volume_used']) - 375})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent['volume_used']: target_viscosity(diluent['volume_used']), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
"""
Created on Tue Mar 27 16:46:57 2018

@author: mima
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')



### INPUT CRUDE ###
crude = 'Forties'
print(assay[crude])

refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)


#
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output


#reformer(reformer_capacity=10)

def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output

#fcc()    

fcc_output = fcc() 
residue = {}
residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid

imported_vgo = 0  

diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'volume_used': 0,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': 0
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
diluent['volume_used']
def target_viscosity(diluent['volume_used']):
    #diluent['volume_used']= 5
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   

a = diluent['volume_used']
def target_viscosity():
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   

cons = ({'type': 'ineq', 'fun': lambda diluent['volume_used']: diluent['volume'] - diluent['volume_used']},{'type': 'ineq', 'fun': lambda diluent['volume_used']: target_viscosity(diluent['volume_used']) - 375})
{'type': 'ineq', 'fun': lambda diluent['volume_used']: diluent['volume'] - diluent['volume_used']}
diluent['volume_used']
{'type': 'ineq', 'fun': lambda (diluent['volume_used']): diluent['volume'] - diluent['volume_used']}
def target_viscosity():
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda a: diluent['volume'] - diluent['volume_used']},
         {'type': 'ineq', 'fun': lambda a: target_viscosity(diluent['volume_used']) - 375})

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda a: target_viscosity(diluent['volume_used']), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'volume_used': 0,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': 0
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29

# create the 'cost' function - what do we want to minimise?
a = diluent['volume_used']
def target_viscosity():
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda a: diluent['volume'] - diluent['volume_used']},
         {'type': 'ineq', 'fun': lambda a: target_viscosity(diluent['volume_used']) - 375})

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda a: target_viscosity(), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
a = diluent['volume_used']
def target_viscosity():
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda a: diluent['volume'] - diluent['volume_used']},
         {'type': 'ineq', 'fun': lambda a: target_viscosity() - 375})

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda a: target_viscosity(), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
diluent['volume_used']
res.x[0]
res.x
res
a = diluent['volume_used']
def target_viscosity():
    return 10 ** (10 ** (((diluent['volume_used'] / (diluent['volume_used'] + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent['volume_used'] + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda a: diluent['volume'] - a},
         {'type': 'ineq', 'fun': lambda a: target_viscosity() - 375})

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda a: target_viscosity(), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
res
a = diluent['volume_used']
def target_viscosity():
    return 10 ** (10 ** (((a / (a + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (a + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda a: diluent['volume'] - a},
         {'type': 'ineq', 'fun': lambda a: target_viscosity() - 375})

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda a: target_viscosity(), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
def target_viscosity(a = diluent['volume_used']):
    return 10 ** (10 ** (((a / (a + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (a + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda a: diluent['volume'] - a},
         {'type': 'ineq', 'fun': lambda a: target_viscosity() - 375})

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda a: target_viscosity(), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
res
target_viscosity()
def target_viscosity(a = diluent['volume_used']):
    return 10 ** (10 ** (((a / (a + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (a + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8   


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda a: diluent['volume'] - a},
         {'type': 'ineq', 'fun': lambda a: 375 - target_viscosity()})

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda a: target_viscosity(), [0], method = 'COBYLA', constraints=cons)
diluent['volume_used'] = res.x[0]
res
type(diluent['volume_used'])
diluent_used
diluent_used = 0
diluent_used
type(diluent['volume_used'])
type(diluent_used)
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 375})  
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons) 
res
diluent['volume_used'] = res.x[0]
diluent
def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': 0
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    diluent_used = 0
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

diluent
res
diluent['cst'] = res.fun[0]
res.fun
diluent['cst'] = res.fun
diluent
reformer_output
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
diluent
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
diluent['surplus_after_blend']
diluent
def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    
    imported_vgo = 0  
    
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': 0
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    diluent_used = 0
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used']
diluent['volume']
diluent['volume_used']
diluent['surplus_after_blend']
diluent
fo_blend = fo_blend()
fo_blend
final_yields = {}

final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 

print(final_yields)         
runfile('C:/Users/mima/.spyder-py3/GPW Model V2.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW Model V5.py', wdir='C:/Users/mima/.spyder-py3')
"""
Created on Tue Mar 27 16:46:57 2018

@author: mima
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Mar 21 11:49:51 2018

@author: mima
"""
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')



### INPUT CRUDE ###
crude = 'Forties'
print(assay[crude])

refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')

### INPUT CRUDE ###
crude = 'Forties'
refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

reformer_output = reformer()
print(reformer_output)
fcc_output = fcc()
print(fcc_output)
diluent = fo_blend()
print(diluent)
def final_yields(reformer_output, fcc_output, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 
    
    return final_yields

def yields(reformer_output, fcc_output, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 
    
    return final_yields

final_yields = yields(reformer_output, fcc_output, diluent)
print(final_yields) 
assay
"""
from datetime import datetime

crude_price = 2.5
structure = 0.5
brent_wti_2m_spread = 3.50
brent_dubai_2m = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = datetime(2018,2,1)
expiry_date = datetime(2018,3,1)
basis = 'CMA'
port_costs = 0.05
other_costs = 0.5

'NEed to name/label world scale routes and attach to neccessray journies'

# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price

from datetime import datetime

crude_price = 2.5
structure = 0.5
brent_wti_2m_spread = 3.50
brent_dubai_2m = 3.50
brent_efp_M2 = 0.02
cfd = -1.00
date = datetime(2018,2,1)
expiry_date = datetime(2018,3,1)
basis = 'CMA'
port_costs = 0.05
other_costs = 0.5

'NEed to name/label world scale routes and attach to neccessray journies'

# =============================================================================
# ### LANDED COSTS ### tules to get back to benhcmarks
# 
# ### Need 
#         1) expiry dates table
# 
# 
# import dataframe with time series values
# 
# need expiry month table
# =============================================================================

'this gives vs 2nd month WTI, need an expiry dates table'

if basis == 'CMA': 
    if date < expiry_date:
        crude_diff_local_index = crude_price + structure
else:
    crude_diff_local_index = crude_price

bt_factor = assay[crude]['Conversion']
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm', sheetname = 'WS Daily', header = 0, index_col = 'DATE', use_cols "B:F")
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm', sheetname = 'WS Daily', header = 0, index_col = 'DATE', usecols "B:")
from datetime import datetime
import pandas as pd
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm', sheetname = 'WS Daily', header = 0, index_col = 'DATE', usecols "B:")
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm', sheetname = 'WS Daily', header = 0, index_col = 'DATE')
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS')
prices = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Price')
ws.columns
ws[1:2,:]
ws.iloc[1:2,:]
ws.iloc[1,:]
ws.iloc[1]
ws.loc[1]
ws.loc[1:2,:]
ws.loc[1,:]
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [1,2])
prices = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Price', header = [1,2])
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [1,2])
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [0,1])
prices = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Price', header = [0,1])
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [0,1], index_col = [0])
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [1], index_col = [0])
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [[0,1]], index_col = [0])
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [0,1], index_col = [0])
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [0], index_col = [0])
ws.loc[1]
ws.loc[1,:]
ws.iloc[1,:]
ws.iloc[0,:]
ws.columns = pd.MultiIndex.from_product([ws.columns, ws.iloc[0,:]])
s.iloc[0,:].values
ws.iloc[0,:].values
([ws.columns.values
ws.columns.values
arrays = [ws.columns.values,ws.iloc[0,:].values]
arrays = [ws.columns.values,ws.iloc[0,:].values]
tuples = list(zip(*arrays))
index = pd.MultiIndex.from_tuples(tuples, names=['name', 'code'])
index
ws = pd.DataFrame(ws, columns = index)
ws.head()
ws
ws = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'WS', header = [0], index_col = [0])
ws.head()
arrays = [ws.columns.values,ws.iloc[0,:].values]
tuples = list(zip(*arrays))
index = pd.MultiIndex.from_tuples(tuples, names=['name', 'code'])
ws.head()
ws.rename(columns=index)
index
ws.iloc[0,:].values
ws.columns.values
arrays
zip(*arrays
zip(*arrays)
tuples
index
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
ws
arrays = [ws.columns.values,ws.iloc[0,:].values]
tuples
assay.agbami
assay.Agbami
assay['Agbami']
flat_rate = pd.Dataframe([(1,'BP - Sullom Voe Terminal','Houston',datetime(2018,1,1), 1.5)], columns=['Id','Load Port','Discharge Port','Year','Rate'])
flat_rate = pd.DataFrame([(1,'BP - Sullom Voe Terminal','Houston',datetime(2018,1,1), 1.5)], columns=['Id','Load Port','Discharge Port','Year','Rate'])
assay['Agbami']
flat_rate = pd.DataFrame([(1,'BP - Sullom Voe Terminal','Houston',datetime(2018,1,1), 1.5)], columns=['Id','Load Port','Discharge Port','Year','Rate'])

ws_table = pd.DataFrame([(1,'NWE','USGC',1000,'$/T','ABCDEFG'),
                         (1,'NWE','USGC',1000,'$/T','poiuytr'),
                         (1,'NWE','USGC',2000,'lumpsum','1234567')
                         ], columns=['Id','Origin','Destination','Size','Terms','Code'])

prices = pd.DataFrame([(2, 15, 10, 3.3)
                         ], columns=['mike','ABCDEFG','poiuytr','1234567'])
crude = 'LLS'
assay[crude]['Index'] = 'CMA'
def convert_dtd(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        else:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]
    
    elif assay[crude]['Index'] in index_dtd: # This will have differing cfd values dependign on location / journey time
        if prices.index < datetime(2018,7,28):
            if prices[first_cfd] > prices[second_cfd]:              
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[second_cfd]
            else:
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[first_cfd]
        else:
            diff_vs_dtd =  prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        diff_vs_dtd = prices[assay[crude]['Code']] - prices[dated_brent]
    
    return diff_vs_dtd

def convert_wti(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date: 
            diff_vs_wti = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) 
        else:
            diff_vs_wti = prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if prices[first_cfd] > prices[second_cfd]: 
            diff_vs_wti = prices[assay[crude]['Code']] + prices[second_cfd] +  prices[efpm2] + (prices[brentm2] - prices[wtim2])
        else: 
            diff_vs_wti = prices[assay[crude]['Code']] + prices[first_cfd] +  prices[efpm2] + (prices[brentm2] - prices[wtim2])
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        diff_vs_wti = prices[assay[crude]['Code']] - prices[wtim2]
    
    return diff_vs_wti

ef convert_dtd(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        else:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]
    
    """ convert the dtd related crude into an equivalent landing period vs other crudes - could do prompt but have talked to Andrea about this"""
    elif assay[crude]['Index'] in index_dtd: # This will have differing cfd values dependign on location / journey time
        if prices.index < datetime(2018,7,28):
            if prices[first_cfd] > prices[second_cfd]:              
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[second_cfd]
            else:
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[first_cfd]
        else:
            diff_vs_dtd =  trader_assessed_prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        diff_vs_dtd = prices[assay[crude]['Code']] - prices[dated_brent]
    
    return diff_vs_dtd
def convert_dtd(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        else:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]
    
    """ convert the dtd related crude into an equivalent landing period vs other crudes - could do prompt but have talked to Andrea about this"""
    elif assay[crude]['Index'] in index_dtd: # This will have differing cfd values dependign on location / journey time
        if prices.index < datetime(2018,7,28):
            if prices[first_cfd] > prices[second_cfd]:              
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[second_cfd]
            else:
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[first_cfd]
        else:
            diff_vs_dtd =  trader_assessed_prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        diff_vs_dtd = prices[assay[crude]['Code']] - prices[dated_brent]
    
    return diff_vs_dtd

def convert_dtd(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        else:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]

def convert_dtd(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        else:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]
    
    """ convert the dtd related crude into an equivalent landing period vs other crudes - could do prompt but have talked to Andrea about this"""
    elif assay[crude]['Index'] in index_dtd:
        if prices.index < datetime(2018,7,28):
            if prices[first_cfd] > prices[second_cfd]:              
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[second_cfd]
            else:
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[first_cfd]
        else:
            diff_vs_dtd =  trader_assessed_prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        diff_vs_dtd = prices[assay[crude]['Code']] - prices[dated_brent]
    
    return diff_vs_dtd

def convert_dtd(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        else:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]
    
    
    """ convert the dtd related crude into an equivalent landing period vs other crudes - could do prompt but have talked to Andrea about this"""
    elif assay[crude]['Index'] in index_dtd: # This will have differing cfd values dependign on location / journey time
        if prices.index < datetime(2018,7,28):
            if prices[first_cfd] > prices[second_cfd]:              
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[second_cfd]
            else:
                diff_vs_dtd =  prices[assay[crude]['Code']] + prices[first_cfd]
        else:
            diff_vs_dtd =  trader_assessed_prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        diff_vs_dtd = prices[assay[crude]['Code']] - prices[dated_brent]
    
    return diff_vs_dtd

if assay[crude]['Index'] in index_wti:
    if prices.index < prices.wti_expiry_date:
        if prices[first_cfd] > prices[second_cfd]:
            diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
        else:
            diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
    else:
        if prices[first_cfd] > prices[second_cfd]:
            diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
        else:
            diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]

def convert_dtd(crude):
    
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        else:
            if prices[first_cfd] > prices[second_cfd]:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
            else:
                diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]

def convert_wti(crude):
    if assay[crude]['Index'] in index_wti:
        if prices.index < prices.wti_expiry_date: 
            diff_vs_wti = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) 
        else:
            diff_vs_wti = prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if prices[first_cfd] > prices[second_cfd]: 
            diff_vs_wti = prices[assay[crude]['Code']] + prices[second_cfd] +  prices[efpm2] + (prices[brentm2] - prices[wtim2])
        else: 
            diff_vs_wti = prices[assay[crude]['Code']] + prices[first_cfd] +  prices[efpm2] + (prices[brentm2] - prices[wtim2])

assay[crude]['Index']
elif assay[crude]['Index'] in index_dtd
elif assay[crude]['Index'] in index_dtd:
elif assay[crude]['Index'] in index_dub:
def pricing_adjustment(crude):
    index_wti = ['CMA','WTI Forward','WTI Front Line']
    index_dtd = ['CMA','WTI Forward','WTI Front Line']
    index_dubai = ['CMA','WTI Forward','WTI Front Line']
    
    def convert_wti(crude):
        if assay[crude]['Index'] in index_wti:
            if prices.index < prices.wti_expiry_date: 
                diff_vs_wti = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) 
            else:
                diff_vs_wti = prices[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dtd:
            if prices[first_cfd] > prices[second_cfd]: 
                diff_vs_wti = prices[assay[crude]['Code']] + prices[second_cfd] +  prices[efpm2] + (prices[brentm2] - prices[wtim2])
            else: 
                diff_vs_wti = prices[assay[crude]['Code']] + prices[first_cfd] +  prices[efpm2] + (prices[brentm2] - prices[wtim2])
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            diff_vs_wti = prices[assay[crude]['Code']] - prices[wtim2]
        
        return diff_vs_wti
    
    def convert_dtd(crude):
        
        if assay[crude]['Index'] in index_wti:
            if prices.index < prices.wti_expiry_date:
                if prices[first_cfd] > prices[second_cfd]:
                    diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[second_cfd]
                else:
                    diff_vs_dtd = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] - prices[efpm2] - prices[first_cfd]
            else:
                if prices[first_cfd] > prices[second_cfd]:
                    diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[second_cfd]
                else:
                    diff_vs_dtd = prices[assay[crude]['Code']] - prices[brentm2] - prices[efpm2] - prices[first_cfd]
        
        elif assay[crude]['Index'] in index_dtd:
            
            if prices.index < datetime(2018,7,28):
                if prices[first_cfd] > prices[second_cfd]:              
                    diff_vs_dtd =  prices[assay[crude]['Code']] + prices[second_cfd]
                else:
                    diff_vs_dtd =  prices[assay[crude]['Code']] + prices[first_cfd]
            else:
                diff_vs_dtd =  trader_assessed_prices[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            diff_vs_dtd = prices[assay[crude]['Code']] - prices[dated_brent]
        
        return diff_vs_dtd
    
    def convert_dub(crude):
        if assay[crude]['Index'] in index_wti:
            if prices.index < prices.wti_expiry_date:
                diff_vs_dubai = prices[assay[crude]['Code']] +  (prices[wtim1] -  prices[wtim2]) - prices[brentm2] + prices[efs2]
            else:
                diff_vs_dubai = prices[assay[crude]['Code']] - prices[brentm2] + prices[efs2]
        
        elif assay[crude]['Index'] in index_dtd:
            if prices[first_cfd] > prices[second_cfd]:  
                diff_vs_dubai = prices[assay[crude]['Code']] + prices[second_cfd] +  prices[efpm2] + prices[efs2]
            else:
                diff_vs_dubai = prices[assay[crude]['Code']] + prices[first_cfd] +  prices[efpm2] + prices[efs2]
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return diff_vs_dub


import pandas as pd

data = pd.read_excel('C://Users//mima//Downloads//toydata.xlsx')
data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay')
data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table')
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table')
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table')
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
crude = 'Forties'
assay[crude]['LoadPort']
def flat_rate(crude, discharge):
    
    rate = flat_rate[(flat_rate['Load Port'] == assay[crude]['LoadPort'])&
                     (flat_rate['Discharge Port'] == discharge)&
                     (price.index.year == flat_rate['Date'])]['Rate'].values

assay[crude]['LoadPort']
assay = pd.read_excel(data, 'assay').to_dict('index')
assay[crude]['LoadPort']
assay
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table')
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
crude = 'Forties'
assay[crude]['LoadPort']
data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table')
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
flat_rate
def flat_rate(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     (price.index.year == flat_rate['Date'])]['Rate'].values
    
    return rate

def flat_rate(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     ('2018' == flat_rate['Year'])]['Rate'].values
    
    return rate

flat_rate(crude, 'Houston')
def fr(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     ('2018' == flat_rate['Year'])]['Rate'].values
    
    return rate




fr(crude, 'Houston')
def fr(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     ('2018' == flat_rate['Year'])]['Rate'].values
    
    return rate

import pandas as pd

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table')
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')

crude = 'Forties'


def fr(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     ('2018' == flat_rate['Year'])]['Rate'].values
    
    return rate




fr(crude, 'Houston')
flat_rate['DischargePort']
def fr(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     (2018 == flat_rate['Year'])]['Rate'].values
    
    return rate




fr(crude, 'Houston')
flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                 (flat_rate['DischargePort'] == discharge)&
                 (2018 == flat_rate['Year'])]['Rate'].values
fr(crude, 'Houston')
flat_rate
crude = 'Azeri'


def fr(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     (2018 == flat_rate['Year'])]['Rate'].values
    
    return rate




fr(crude, 'Houston')
prices
ws_table
ws_table.head()
ws_table = pd.read_excel(data, 'ws_table', header = 1)
ws_table
ws_table.head()
prices = pd.read_excel(data, 'prices', header = 1)
ws_table.head()
total = pd.merge(prices, ws, how='left')
prices
total = pd.merge(prices, ws, how='left', left_on = prices.index)
prices
prices.index
total = prices.merge(ws, how='inner', left_on = prices.index)
total = prices.merge(ws, left_on = prices.index)
total = pd.merge(prices, ws, how = 'left', on = prices.index)
total = pd.merge(prices, ws, how = 'left', on = prices.index, left_index = True)
total = pd.merge(prices, ws, how = 'inner', on = prices.index, left_index = True)
total = pd.merge(prices, ws, how = 'inner', left_index = True, right_index = True)
total
total = prices.merge(ws, how = 'inner', left_index = True, right_index = True)
total
ws
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total
ports
ws_codes = ws[(ws['Origin'] == ports[ports['Name'] == assay[crude]['LoadPort']]) &
                  (ws['Destination'] == 'USGC')]
ports
ws_codes = ws[(ws['Origin'] == ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']) &
                  (ws['Destination'] == 'USGC')]
ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
ws_codes = ws[(ws['Origin'] == ports[ports['Name'] == assay[crude]['LoadPort']]['Region']) &
                  (ws['Destination'] == 'USGC')]
ports[ports['Name'] == assay[crude]['LoadPort']]['Region']
ws['Origin']
ws
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')




crude = 'Azeri'
discharge = 'Houston'

def fr(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     (2018 == flat_rate['Year'])]['Rate'].values
    
    return rate




fr(crude, 'Houston')




ws_table.head()

total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
ws_codes = ws[(ws['Origin'] == ports[ports['Name'] == assay[crude]['LoadPort']]['Region']) &
                  (ws['Destination'] == ports[ports['Name'] == discharge]['Region'])]
discharge
ports[ports['Name'] == discharge]['Region']
ws
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
flat_rate = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')




crude = 'Azeri'
discharge = 'Houston'

def fr(crude, discharge):
    
    rate = flat_rate[(flat_rate['LoadPort'] == assay[crude]['LoadPort'])&
                     (flat_rate['DischargePort'] == discharge)&
                     (2018 == flat_rate['Year'])]['Rate'].values
    
    return rate




fr(crude, 'Houston')




ws_table.head()

total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)



""" Need to have regions etc sorted - remember to cha nge the codse below"""

def world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    ws_codes = ws[(ws['Origin'] == ports[ports['Name'] == assay[crude]['LoadPort']]['Region']) &
                      (ws['Destination'] == ports[ports['Name'] == discharge]['subregion'])]

ports[ports['Name'] == discharge]['subregion']
ports[ports['Name'] == discharge]['Subregion']
ws
ws_codes
s_codes = ws[(ws['Origin'] == ports[ports['Name'] == assay[crude]['LoadPort']]['Region']) &
                  (ws['Destination'] == ports[ports['Name'] == discharge]['Subregion'])]
(ws['Destination'] == ports[ports['Name'] == discharge]['Subregion'])
ws['Destination']
ports[ports['Name'] == discharge]['Subregion']
flat_rate['LoadPort']
ws['Destination']
ports[ports['Name'] == discharge]['Subregion'
ports[ports['Name'] == discharge]['Subregion']
assay[crude]['LoadPort']
ws['Destination'] == ports[ports['Name'] == discharge]['Subregion'].values
ports[ports['Name'] == discharge]['Subregion'].values
ports[ports['Name'] == discharge]
ws['Destination']
ws['Destination'] == 'MED'
ports[ports['Name'] == discharge]['Subregion'].values
ws['Destination'] == ports[ports['Name'] == discharge]['Subregion'].values
ports[ports['Name'] == discharge]['Subregion'].values
ports[ports['Name'] == discharge]['Subregion'][0].values
ports[ports['Name'] == discharge]['Subregion']
ports.loc[discharge]
"""
Created on Tue Jan  9 12:11:55 2018

@author: mima
"""

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta 

DOEAPIDATA = pd.read_excel("L://TRADING//ANALYSIS//BALANCES//US Balance.xlsm",
                        sheetname="DOE_API_W", header =0).drop([0])

DOEAPIDATA = DOEAPIDATA.set_index('Date')
DOEAPIDATA = DOEAPIDATA.drop(DOEAPIDATA.columns[0], axis=1)
DOEAPIDATA.index = pd.to_datetime(DOEAPIDATA.index)
DOEAPIDATA = pd.DataFrame(DOEAPIDATA).astype(float)
mike = DOEAPIDATA.resample('d').bfill()
d = mike.index.day -1 - np.clip((mike.index.day-1) // 10, 0, 2)*10
date1 = mike.index.values - np.array(d, dtype="timedelta64[D]")
decade_mike = mike.groupby(date1).mean()
writer = pd.ExcelWriter("M://DOE TEST SHEET.xlsx")
decade_mike.to_excel(writer, 'DOE_API_Dec')
writer.save()

import pandas as pd
import numpy as np
from datetime import datetime
import random

##data = pd.ExcelFile('C://Users//mike_//Downloads//toydata (3).xlsx')

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
total_f = total.dropna(how='any')
total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2))
total
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
total_f = total.dropna(how='any')
total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2))
total_f['wtim2'] = total_f['wtim1'].apply(lambda row: generate_price(row,-1,1))
total_f['wti_expiry_date'] = total_f.index.shift(11, freq='D') 
total_f['first_cfd'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2)) - total_f['PCAAS00']
total_f['second_cfd'] = total_f['first_cfd'].apply(lambda row: generate_price(row,-5,-2))
total_f['efpm2'] = total_f['first_cfd'].apply(lambda row: generate_price(row,-5,-2))
total_f['brentm2'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-1,-1))
total_f['efs2'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2)) - total_f['PCAAS00']
index_wti
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC DATED	MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
total
import pandas as pd
import numpy as np
from datetime import datetime
import random

##data = pd.ExcelFile('C://Users//mike_//Downloads//toydata (3).xlsx')

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]


crude = 'Azeri'
discharge = 'Houston'

df = pd.DataFrame(index=total.index)
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]


crude = 'Azeri'
discharge = 'Houston'

df = pd.DataFrame(index=total.index)
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC DATED	MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
#TEST PRICES
base = total['PCAAS00']
minp = -1
maxp = 1
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)



generate_price(base, minp, maxp)
total_f = total.dropna(how='any')
total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2))
total_f['wtim2'] = total_f['wtim1'].apply(lambda row: generate_price(row,-1,1))
total_f['wti_expiry_date'] = total_f.index.shift(11, freq='D') 
total_f['first_cfd'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2)) - total_f['PCAAS00']
total_f['second_cfd'] = total_f['first_cfd'].apply(lambda row: generate_price(row,-5,-2))
total_f['efpm2'] = total_f['first_cfd'].apply(lambda row: generate_price(row,-5,-2))
total_f['brentm2'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-1,-1))
total_f['efs2'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2)) - total_f['PCAAS00']
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC DATED	MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
#TEST PRICES
base = total['PCAAS00']
minp = -1
maxp = 1
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)



generate_price(base, minp, maxp)
total_f = total.dropna(how='any')
total_f = total.dropna(how='any')
generate_price(base, minp, maxp)
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)

ow = total['PCAAS00']
minp = -1
maxp = 1
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)

generate_price(row, minp, maxp)
row = total['PCAAS00']
generate_price(row=row, minp, maxp)
total_f = total.dropna(how='any')
total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2))
total_f['wtim2'] = total_f['wtim1'].apply(lambda row: generate_price(row,-1,1))
total_f['wti_expiry_date'] = total_f.index.shift(11, freq='D') 
total_f['first_cfd'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2)) - total_f['PCAAS00']
total_f['second_cfd'] = total_f['first_cfd'].apply(lambda row: generate_price(row,-5,-2))
total_f['efpm2'] = total_f['first_cfd'].apply(lambda row: generate_price(row,-5,-2))
total_f['brentm2'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-1,-1))
total_f['efs2'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2)) - total_f['PCAAS00']
assay[crude]['Index']
total_f.index
total_f['wti_expiry_date']
total_f.index < total_f['wti_expiry_date']
if assay[crude]['Index'] in index_wti:
    if total_f.index < total_f['wti_expiry_date']: 
        total_f['diff_vs_wti'] = total_f[assay[crude]['Code']] +  (total_f['wtim1'] -  total_f['wtim2']) 
    else:
        total_f['diff_vs_wti'] = total_f[assay[crude]['Code']]

total_f['diff_vs_wti']
(total_f['wtim1'] -  total_f['wtim2']) 
total_f[assay[crude]['Code']] +  (total_f['wtim1'] -  total_f['wtim2']) 
total_f['diff_vs_wti']
if assay[crude]['Index'] in index_wti:
    if total_f.index < total_f['wti_expiry_date']: 
        total_f['diff_vs_wti'] = total_f[assay[crude]['Code']] +  (total_f['wtim1'] -  total_f['wtim2']) 
    else:
        total_f['diff_vs_wti'] = total_f[assay[crude]['Code']]

total_f['diff_vs_wti']
total_f
import pandas as pd
import numpy as np
from datetime import datetime
import random

##data = pd.ExcelFile('C://Users//mike_//Downloads//toydata (3).xlsx')

data = pd.ExcelFile('C://Users//mima//Downloads//toydata.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
crude = 'Azeri'
discharge = 'Houston'
ports
assay
crude = 'Azeri'
ports['Name']
ws_table
flat_rate_table["Year"]
flat_rate_table
flat_rate_table = calculate_flat_rate(crude, discharge)
crude = 'Azeri'
discharge = 'Houston'

df = pd.DataFrame(index=total.index)

def flat_rate(crude, discharge):
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table

flat_rate_table
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])

crude = 'Azeri'
discharge = 'Houston'
crude = 'Azeri'
discharge = 'Houston'
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])

calculate_flat_rate(crude, discharge)
calculate_flat_rates(crude, discharge)
assay[crude]['Index']
import pandas as pd
import numpy as np
from datetime import datetime
import random

##data = pd.ExcelFile('C://Users//mike_//Downloads//toydata (3).xlsx')

data = pd.ExcelFile('C://Users//mima//Downloads//toydata11042018.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
import pandas as pd
import numpy as np
from datetime import datetime
import random

data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
import pandas as pd
import numpy as np
from datetime import datetime
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
import pandas as pd
import numpy as np
from datetime import datetime
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]


crude = 'Azeri'
discharge = 'Houston'
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
total = total.dropna(how='any')
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
def calculate_flat_rate(crude, discharge):
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table

calculate_flat_rate()
flat_rate_row["Rate"]
flat_rate_table = calculate_flat_rate()
df['Rate'] = total.apply(lambda x: calculate_rate(flat_rate_table, x), axis =1)
df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])

df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
import pandas as pd
import numpy as np
from datetime import dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()


"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
total = total.dropna(how='any')


"""Initialise the crude and discharge location - this will be driven by user input or looped over a list. Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight"""
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


round(random.uniform(-1,2), 2)
df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()


"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
total = total.dropna(how='any')


"""Initialise the crude and discharge location - this will be driven by user input or looped over a list. Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight"""
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


round(random.uniform(-1,2), 2)
df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
flat_rate_table["Year"]
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)

sub_region_2 = ports[ports['Name'] == discharge]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)

ws_codes = ws[(ws['Origin'] == sub_region) &
                  (ws['Destination'] == sub_region_2)]
ws_codes
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)

sub_region_2 = ports[ports['Name'] == discharge]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)

ws_codes = ws[(ws['Origin'] == sub_region) &
                  (ws['Destination'] == sub_region_2)]
ws_codes['Code']
sub_to_ws
ports
for i in list(ws_codes['Code']):
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            dollars_barrel = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        
        else:
            dollars_barrel = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']

dollars_barrel
ws_codes['Code']
ws_codes[ws_codes['Code'] == i]['Name']
name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
for i in list(ws_codes['Code']):
    name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']

for i in list(ws_codes['Code']):
    name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
    if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
        df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
    else:
        df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']

df
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == discharge]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    #freight_frame = pd.DataFrame()
    counter = 0
    total.columns.values
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df

calculate_world_scale()
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC DATED	MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2))
total_f = total.dropna(how='any')
total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2)
total_f = total.dropna(how='any')
total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2))
runfile('C:/Users/mima/.spyder-py3/arb_econ_070418v2.py', wdir='C:/Users/mima/.spyder-py3')
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)

total_f['wtim1'] = total_f['PCAAS00'].apply(lambda row: generate_price(row,-5,-2))
assay[crude]['Index']
prices_reference = pd.read_excel(data, 'prices', header = 0)[0:2]
prices_reference = pd.read_excel(data, 'prices', header = 0)[0:1]
prices_reference = pd.read_excel(data, 'prices', header = None)[0:1]
prices_reference = pd.read_excel(data, 'prices', header = None)[0:2]
prices_reference = pd.read_excel(data, 'prices', header = 1)[0:2]
prices_reference = pd.read_excel(data, 'prices', header = 0)[0:2]
prices_reference = pd.read_excel(data, 'prices', header = 0)[0:1]
prices_reference = pd.read_excel(data, 'prices', header = 0).reset_index(drop=True)[0:1]
prices_reference = pd.read_excel(data, 'prices', header = 0).to_dict('index')
prices_reference = pd.read_excel(data, 'prices', header = 0).reset_index(drop=True)[0:1]
prices_reference = pd.read_excel(data, 'prices', header = 0).reset_index(drop=True)[0:1]
prices_reference = prices_reference.to_dict('index')
prices_reference = pd.read_excel(data, 'prices', header = 0).reset_index(drop=True)[0:1]
prices_reference[0]
prices_reference['0']
prices_reference = pd.read_excel(data, 'prices', header = 0).reset_index(drop=True)[0:1]
prices_reference = prices_reference.to_dict('index')
prices_reference
prices_reference = pd.read_excel(data, 'prices', header = 0).reset_index(drop=True)[0:1]
prices_reference
prices_reference = pd.read_excel(data, 'prices', header = 0).reset_index(drop=True)[0:1]
prices_reference = pd.read_excel(data, 'prices', header = None).reset_index(drop=True)[0:1]
prices_reference = pd.read_excel(data, 'prices', header = None).reset_index(drop=True)[0:2]
prices_reference = pd.read_excel(data, 'prices', header = None)[0:2]
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2]
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.set_index(['Name','Code'])
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.rename(idnex={0:'Name',1:'Code'})
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.rename(index={0:'Name',1:'Code'})
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.rename(index={0:'Name',1:'Code'}).to_dict('Index')
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.rename(index={0:'Name',1:'Code'}).to_dict('list')
prices_reference = prices_reference.rename(index={0:'Name',1:'Code'}).to_dict('dict')
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.rename(index={0:'Name',1:'Code'}).to_dict('dict')
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.rename(index={0:'Name',1:'Code'}).to_dict('records')
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.rename(index={0:'Name',1:'Code'}).to_dict('list')
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.transpose()
prices_reference = prices_reference.transpose().to_dict()
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.transpose().to_dict()
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.transpose().to_dict('index')
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.transpose()
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.transpose()
prices_reference = prices_reference.rename({0:'Name'})
prices_reference = pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]
prices_reference = prices_reference.transpose().rename(columns={0:'Name'})
prices_reference = (pd.read_excel(data, 'prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
paper_prices = pd.read_excel(data, 'paper_prices', header = 1)
expiry = pd.read_excel(data, 'paper_prices', header = 1)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry = pd.read_excel(data, 'paper_prices', header = 1)
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
prices_reference
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
total
wtim1 = total['CLc1']
wtim1
expiry
expiry = pd.read_excel(data, 'expiry', header = 1)
expiry
expiry_table = pd.read_excel(data, 'expiry', header = 1)
ports = pd.read_excel(data, 'ports')
flat_rate_table
expiry_table = pd.read_excel(data, 'expiry')
expiry_table
expiry_table["Month"].month
expiry_table
expiry_table["Month"].month
expiry_table = pd.to_datetime(pd.read_excel(data, 'expiry'))
pd.read_excel(data, 'expiry').iloc[0::]
expiry_table = pd.to_datetime(pd.read_excel(data, 'expiry').iloc[0::])
expiry_table = pd.read_excel(data, 'expiry')
expiry['Month'], expiry['expiry'] = pd.to_datetime(expiry['Month']), pd.to_datetime(expiry['expiry'])
expiry['Month']
expiry_table['Month'], expiry_table['expiry'] = pd.to_datetime(expiry_table['Month']), pd.to_datetime(expiry_table['expiry'])
expiry_table['Month'], expiry_table['Expiry'] = pd.to_datetime(expiry_table['Month']), pd.to_datetime(expiry_table['Expiry'])
expiry_table["Month"].month
expiry_table = pd.read_excel(data, 'expiry')
expiry_table["Month"].dt.month
df['Expiry'] = total.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])

df['Expiry'] = total.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(expiry_row["Expiry"].iat[0])

df['Expiry'] = total.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]

df['Expiry'] = total.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
df['Expiry']
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = total.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = df.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
df
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_expiry = df['Expiry']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_expiry = df['Expiry']
cfd1 = total['PCAKG00']
cfd2 = ['AAGLV00']
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_expiry = df['Expiry']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()


"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
"""
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]

df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
df = pd.DataFrame(index=total.index)
df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]

df
df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = df.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
#total = total.dropna(how='any')


"""
Initialise the crude and discharge location - this will be driven by user input or looped over a list. 
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()



"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == discharge]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    #freight_frame = pd.DataFrame()
    counter = 0
    total.columns.values
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_expiry = df['Expiry']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
runfile('C:/Users/mima/.spyder-py3/arb_econ_070418v2.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]
#total = total.dropna(how='any')


"""
Initialise the crude and discharge location - this will be driven by user input or looped over a list. 
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()





"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)



"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == discharge]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    #freight_frame = pd.DataFrame()
    counter = 0
    total.columns.values
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
ef apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]

df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC DATED	MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_expiry = df['Expiry']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
total
total.columns.values
total
cleaned_column_headers = [i.strip() for i in total.columns.values]
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_expiry = df['Expiry']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
index_wti
assay[crude]['Index'] = 'WTI CMA'
df
df.index < df.expiry
df.index < df['expiry']
df
df.index < df.Expiry
df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
if assay[crude]['Index'] in index_wti:
    if df.index < df.Expiry: 
        df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]

if assay[crude]['Index'] in index_wti:
    if (df.index < df.Expiry): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]

if (df.index < df.Expiry): 
    df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
else:
    df['diff_vs_wti'] = total[assay[crude]['Code']]

if (df.index < df.Expiry).any(): 
    df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
else:
    df['diff_vs_wti'] = total[assay[crude]['Code']]

df['diff_vs_wti']
if assay[crude]['Index'] in index_wti:
    if (df.index < df.Expiry).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]


elif assay[crude]['Index'] in index_dtd:
    if (cfd1 > cfd2).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
    else: 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)

elif assay[crude]['Index'] in index_dub:
    pass

else:
   df['diff_vs_wti'] = total[assay[crude]['Code']] - wtim2
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > datetime(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

#total = total.dropna(how='any')


"""
Initialise the crude and discharge location - this will be driven by user input or looped over a list. 
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
crude = 'Azeri'
discharge = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == discharge)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()





"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)



"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == discharge]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    #freight_frame = pd.DataFrame()
    counter = 0
    total.columns.values
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC DATED	MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_expiry = df['Expiry']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
if assay[crude]['Index'] in index_wti:
    if (df.index < df.Expiry).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]


elif assay[crude]['Index'] in index_dtd:
    if (cfd1 > cfd2).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
    else: 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)

elif assay[crude]['Index'] in index_dub:
    pass

else:
   df['diff_vs_wti'] = total[assay[crude]['Code']] - wtim2
df
cfd2
efpm2
brentm2
wtim2
total[assay[crude]['Code']
total[assay[crude]['Code']]
cfd2 +  efpm2 + (brentm2 - wtim2)
total[assay[crude]['Code']]
total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
total[assay[crude]['Code']] +  (wtim1 -  wtim2)
total[assay[crude]['Code']]
assay[crude]['Index'] 
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC DATED',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
if assay[crude]['Index'] in index_wti:
    if (df.index < df.Expiry).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]


elif assay[crude]['Index'] in index_dtd:
    if (cfd1 > cfd2).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
    else: 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)

elif assay[crude]['Index'] in index_dub:
    pass

else:
   df['diff_vs_wti'] = total[assay[crude]['Code']] - wtim2
df
total[assay[crude]['Code']] - wtim2
assay[crude]['Index']
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
if assay[crude]['Index'] in index_wti:
    if (df.index < df.Expiry).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]


elif assay[crude]['Index'] in index_dtd:
    if (cfd1 > cfd2).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
    else: 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)

elif assay[crude]['Index'] in index_dub:
    pass

else:
   df['diff_vs_wti'] = total[assay[crude]['Code']] - wtim2
df['diff_vs_wti'] 
if assay[crude]['Index'] in index_wti:
    if (df.index < df.Expiry).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]


elif assay[crude]['Index'] in index_dtd:
    if (cfd1 > cfd2).any(): 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
    else: 
        df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)

elif assay[crude]['Index'] in index_dub:
    pass

else:
   df['diff_vs_wti'] = total[assay[crude]['Code']]
total.index < dt(2018,7,28)
(total.index < dt(2018,7,28)).any()
dtd = total['PCAAS00']
def convert_wti():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
        else: 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
       df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    return df['diff_vs_wti']


def convert_dtd():
    
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any():
            if (cfd1 > cfd2).any():
                df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd2
            else:
                df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd1
        else:
            if (cfd1 > cfd2).any():
                df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd2
            else:
                df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd1
    
    elif assay[crude]['Index'] in index_dtd:
        
        if (total.index < dt(2018,7,28)).any():
            if cfd1 > cfd2:              
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd2
            else:
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd1
        else:
            df['diff_vs_dtd'] =  trader_assessed_prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = total[assay[crude]['Code']] - dtd
    
    return df['diff_vs_dtd']

convert_wti()
convert_dtd()
def convert_wti():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
        else: 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
       df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    return df['diff_vs_wti']


def convert_dtd():
    
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any():
            if (cfd1 > cfd2).any():
                df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd2
            else:
                df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd1
        else:
            if (cfd1 > cfd2).any():
                df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd2
            else:
                df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd1
    
    elif assay[crude]['Index'] in index_dtd:
        
        if (total.index < dt(2018,7,28)).any():
            if (cfd1 > cfd2).any():              
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd2
            else:
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd1
        else:
            df['diff_vs_dtd'] =  trader_assessed_prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = total[assay[crude]['Code']] - dtd
    
    return df['diff_vs_dtd']

convert_wti()
convert_dtd()
convert_wti()
convert_dtd()
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')

### INPUT CRUDE ###
crude = 'Forties'
refinery_volume = 200

### Fractions for gaosline pool
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15

def reformer(reformer_capacity=refinery_volume*0.21):
    #reformer_capacity = 42
    reformer_output = {}
    reformer_assay = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    
    utilised_ref_cap = reformer_capacity * 0.97
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(fcc_capacity = refinery_volume*0.21):
    #fcc_capacity = 48
    fcc_output = {}
    fcc_assay = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    vgo_available = assay[crude]['VGO'] * refinery_volume
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_available)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_available - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'volume_used': 0,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def yields(reformer_output, fcc_output, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 
    
    return final_yields

runfile('C:/Users/mima/.spyder-py3/arb_econ_070418v2.py', wdir='C:/Users/mima/.spyder-py3')
def convert_dub():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any():
            df['diff_vs_dub']= prices[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
        
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] - brentm2 + efs2
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any():  
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 

convert_dub()
def convert_dub():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any():
            df['diff_vs_dub']= prices[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
        
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] - brentm2 + efs2
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any():  
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 

convert_dub()  
def convert_dub():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any():
            df['diff_vs_dub']= prices[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
        
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] - brentm2 + efs2
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any():  
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 

convert_dub() 
index_wti
f assay[crude]['Index']
assay[crude]['Index']
assay[crude]['Index'] in index_wti
if assay[crude]['Index'] in index_dtd

assay[crude]['Index'] in index_dtd
(cfd1 > cfd2).any(
(cfd1 > cfd2).any()
cfd1
(cfd1 > cfd2).any()
dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
cfd1
(cfd1 > cfd2).any()
(df.index < df.Expiry).any()
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
"""
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']

def convert_wti():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
        else: 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
       df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    return df['diff_vs_wti']

def convert_dtd():
    
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any():
            if (cfd1 > cfd2).any():
                df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd2
            else:
                df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd1
        else:
            if (cfd1 > cfd2).any():
                df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd2
            else:
                df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd1
    
    elif assay[crude]['Index'] in index_dtd:
        
        if (total.index < dt(2018,7,28)).any():
            if (cfd1 > cfd2).any():              
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd2
            else:
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd1
        else:
            df['diff_vs_dtd'] =  trader_assessed_prices[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = total[assay[crude]['Code']] - dtd
    
    return df['diff_vs_dtd']


def convert_dub():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any():
            df['diff_vs_dub']= prices[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
        
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] - brentm2 + efs2
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any():  
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
        else:
            df['diff_vs_dub'] = prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 

convert_dub()
convert_dtd()
prices[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
prices[assay[crude]['Code']] - brentm2 + efs2
prices[assay[crude]['Code']] + cfd2 +  prices[efpm2] + efs2
index_dtd
assay[crude]['Index'] in index_dtd
(cfd1 > cfd2).any()
prices[assay[crude]['Code']
prices[assay[crude]['Code']]
cfd2
efpm2
convert_dtd()
prices[assay[crude]['Code']] + cfd2 +  efpm2 + efs2
prices[assay[crude]['Code']] + cfd1 +  efpm2 + efs2
prices[assay[crude]['Code']] + cfd2 +  efpm2 + efs2
total[assay[crude]['Code']] - dtd
dtd
total[assay[crude]['Code']]
df['diff_vs_dtd'] = total[assay[crude]['Code']]
total[assay[crude]['Code']] + cfd1
total[assay[crude]['Code']] + cfd2
total[assay[crude]['Code']] - brentm2 - efpm2 - cfd1
def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
destination = 'Houston'
def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df

df
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Initialise the crude and destination - this will be driven by user input or looped over a list. 
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
#crude = 'Azeri'
destination = 'Houston'
df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()





"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)


crude = 'Azeri'
destination = 'Houston'
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()





"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)



"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df

calculate_world_scale()
crude = 'Azeri'
destination = 'Houston'
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()





"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)



"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
df
def arb(crude,destination):
    
    import pandas as pd
    import numpy as np
    from datetime import datetime as dt
    import random
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    
    df = pd.DataFrame(index=total.index)
    
    def calculate_flat_rate():
        
        flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                         (rate_data['DischargePort'] == destination)])
        
        return flat_rate_table
    
    flat_rate_table = calculate_flat_rate()
    
    
    
    
    
    """
    THIS IS ESSENTIALLY A VLOOKUP
    
    inputs are the table to look up values from for each row of another table
    
    in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
    and return the value in the rate row.
    
    use the .iat function to return the inger values - this speeds up operations
    
    x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
    Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year
    
    """
    
    def calculate_flat_rates(flat_rate_table, x):
        flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
        return int(flat_rate_row["Rate"].iat[0])
    
    df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
    
    
    
    """
    Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
    We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
    As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
    We then convert the result to a string and we set index = false as we dont want the header row returned
    Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
    This gives us the neccessary world scale codes 
    
    We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
    If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
    If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor
    
    Final values will be in $/bbl
    
    """
    
    def calculate_world_scale():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
        
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        
        ws_codes = ws[(ws['Origin'] == sub_region) &
                          (ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
        
        return df
    
    df = calculate_world_scale()
    
    def pricing_adjustment():
        index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
        index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
        index_dub = ['DUBAI','DUBAI M2']
        #TEST PRICES
        def generate_price(row, minp, maxp):
            return row + random.uniform(minp,maxp)
        
        # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
        
        """
        This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
        """
        def apply_expiry(expiry_table, x):
            expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                                      (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
            return expiry_row["Expiry"].iat[0]
        
        df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
        
        dtd = total['PCAAS00']
        wtim1 = total['CLc1'] 
        wtim2 = total['CLc2']
        cfd1 = total['PCAKG00']
        cfd2 = total['AAGLV00']
        efpm2 = total['AAGVX00']
        brentm2 = total['LCOc2']
        efs2 = total['AAEBS00']
        
        def convert_wti():
            if assay[crude]['Index'] in index_wti:
                if (df.index < df.Expiry).any(): 
                    df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
                else:
                    df['diff_vs_wti'] = total[assay[crude]['Code']]
            
            elif assay[crude]['Index'] in index_dtd:
                if (cfd1 > cfd2).any(): 
                    df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
                else: 
                    df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
            
            elif assay[crude]['Index'] in index_dub:
                pass
            
            else:
               df['diff_vs_wti'] = total[assay[crude]['Code']]
            
            return df
        
        df = convert_wti()
        
        def convert_dtd():
            
            if assay[crude]['Index'] in index_wti:
                if (df.index < df.Expiry).any():
                    if (cfd1 > cfd2).any():
                        df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd2
                    else:
                        df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd1
                else:
                    if (cfd1 > cfd2).any():
                        df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd2
                    else:
                        df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd1
            
            elif assay[crude]['Index'] in index_dtd:
                
                if (total.index < dt(2018,7,28)).any():
                    if (cfd1 > cfd2).any():              
                        df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd2
                    else:
                        df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd1
                else:
                    df['diff_vs_dtd'] =  trader_assessed_prices[assay[crude]['Code']]
            
            elif assay[crude]['Index'] in index_dub:
                pass
            
            else:
                df['diff_vs_dtd'] = total[assay[crude]['Code']]
            
            return df
        
        df = convert_dtd()
        
        def convert_dub():
            if assay[crude]['Index'] in index_wti:
                if (df.index < df.Expiry).any():
                    df['diff_vs_dub']= total[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
                else:
                    df['diff_vs_dub'] = total[assay[crude]['Code']] - brentm2 + efs2
            
            elif assay[crude]['Index'] in index_dtd:
                if (cfd1 > cfd2).any():  
                    df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2
                else:
                    df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2
            
            elif assay[crude]['Index'] in index_dub:
                pass
            
            return df 
        
        df = convert_dub()
    
    return df
    
    df = pricing_adjustment()
    
    return df

arb(crude, destination)
crude = 'Forties'
destination = 'Augusta'

arb(crude, destination)
import ArbEcon
import ArbEcons
df
import ArbEcons
ArbEcons.arb('Azeri','Houston')
import ArbEcons

ArbEcons.arb('Forties','Houston')
ArbEcons.arb('Azeri','Houston')
import ArbEcons

ArbEcons.arb('Forties','Houston')
ArbEcons.arb('Azeri','Houston')
df = pricing_adjustment()
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
print("df['Expiry'] created successfully")
import ArbEcons

ArbEcons.arb('Azeri','Houston')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")
crude = 'Forties'
destination = 'Augusta'
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
crude = 'Forties'
destination = 'Augusta'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")
f['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
calculate_flat_rates
flat_rate_table
def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_tabl

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table

flat_rate_table = calculate_flat_rate()
flat_rate_table
rate_data
rate_data['LoadPort']
assay[crude]['LoadPort']
rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort']
rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                 (rate_data['DischargePort'] == destination)]
rate_data['DischargePort']
destination
crude = 'Forties'
destination = 'Houston'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment():
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    def apply_expiry(expiry_table, x):
        expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                                  (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
        return expiry_row["Expiry"].iat[0]
    
    df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
    print("df['Expiry'] created successfully")
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    
    def convert_wti():
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any(): 
                df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
            else:
                df['diff_vs_wti'] = total[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dtd:
            if (cfd1 > cfd2).any(): 
                df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
            else: 
                df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
           df['diff_vs_wti'] = total[assay[crude]['Code']]
        
        return df
    
    df = convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any():
                if (cfd1 > cfd2).any():
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd2
                else:
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd1
            else:
                if (cfd1 > cfd2).any():
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd2
                else:
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd1
        
        elif assay[crude]['Index'] in index_dtd:
            
            if (total.index < dt(2018,7,28)).any():
                if (cfd1 > cfd2).any():              
                    df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd2
                else:
                    df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd1
            else:
                df['diff_vs_dtd'] =  trader_assessed_prices[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = total[assay[crude]['Code']]
        
        return df
    
    df = convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any():
                df['diff_vs_dub']= total[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
            else:
                df['diff_vs_dub'] = total[assay[crude]['Code']] - brentm2 + efs2
        
        elif assay[crude]['Index'] in index_dtd:
            if (cfd1 > cfd2).any():  
                df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2
            else:
                df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return df 
    
    df = convert_dub()
    print("convert_dub() created successfully")


return df

df = pricing_adjustment()
print("pricing_adjustment() created successfully")

return df

import ArbEcons

ArbEcons.arb('Azeri','Houston')
import ArbEcons

ArbEcons.arb('Forties','Augusta')
import ArbEcons

ArbEcons.arb('Forties','Houston')
flat_rate_table
total
flat_rate_table
total
paper_prices
prices_reference
import ArbEcons

ArbEcons.arb('Forties','Houston')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
ArbEcons.arb('Azeri','Houston')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import ArbEcons

ArbEcons.arb('Azeri','Houston')
import ArbEcons

ArbEcons.arb('Azeri','Rotterdam')
rate_data
crude = 'Azeri'
destination = 'Rotterdam'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment(df=df):
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    def apply_expiry(expiry_table, x):
        expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                                  (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
        return expiry_row["Expiry"].iat[0]
    
    df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
    print("df['Expiry'] created successfully")
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    
    def convert_wti():
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any(): 
                df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
            else:
                df['diff_vs_wti'] = total[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dtd:
            if (cfd1 > cfd2).any(): 
                df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
            else: 
                df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
           df['diff_vs_wti'] = total[assay[crude]['Code']]
        
        return df
    
    df = convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any():
                if (cfd1 > cfd2).any():
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd2
                else:
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - brentm2 - efpm2 - cfd1
            else:
                if (cfd1 > cfd2).any():
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd2
                else:
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] - brentm2 - efpm2 - cfd1
        
        elif assay[crude]['Index'] in index_dtd:
            
            if (total.index < dt(2018,7,28)).any():
                if (cfd1 > cfd2).any():              
                    df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd2
                else:
                    df['diff_vs_dtd'] =  total[assay[crude]['Code']] + cfd1
            else:
                df['diff_vs_dtd'] =  trader_assessed_prices[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = total[assay[crude]['Code']]
        
        return df
    
    df = convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any():
                df['diff_vs_dub']= total[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
            else:
                df['diff_vs_dub'] = total[assay[crude]['Code']] - brentm2 + efs2
        
        elif assay[crude]['Index'] in index_dtd:
            if (cfd1 > cfd2).any():  
                df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2
            else:
                df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return df 
    
    df = convert_dub()
    print("convert_dub() created successfully")
    
    return df


df = pricing_adjustment()
print("pricing_adjustment() created successfully")

return df
rate_data
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRateComplete.xlsx')
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
r = pd.read_excel(data, sheet_name=0)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
data.sheet_names
data.sheet_names[0]
str = data.sheet_names[0]
str.split()
data.sheet_names[0]
str = data.sheet_names[0].split()
mike = data.sheet_names[0].split()
years = [i[3] for data.sheet_names[i].split()]
years = [i[3] for i in data.sheet_names[i].split()]
mike = data.sheet_names[0].split()
mike
r = pd.read_excel(data, sheet_name=0)
years = [i[3] for i in data.sheet_names[i].split()]
years = [i[3] for i in data.sheet_names[i].split() in range(len(data.sheet_names))]
data.sheet_names
years = []

for name in data.sheet_names:
    x = name.split()[2]
    years.append(x)

years = []

for name in data.sheet_names:
    years.append(name.split()[2])

[years.append(name.split()[2]) for name in data.sheet_names]
for name in data.sheet_names:
    years.append(name.split()[2])

years = [years.append(name.split()[2]) for name in data.sheet_names]
years = [name.split()[2] for name in data.sheet_names]
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

years = [name.split()[2] for name in data.sheet_names]
fr_matrix = pd.DataFrame(data, shet_name = 1)
fr_matrix = pd.DataFrame(data, sheet_name = 1)
fr_matrix = pd.DataFrame(data, sheet_names = 1)
fr_matrix = pd.read_excel(data, sheet_names = 1)
fr_matrix
fr_matrix.columns
len(fr_matrix.columns)
fr_matrix.head()
fr_matrix = pd.read_excel(data, sheet_names = 1, header = None)
len(fr_matrix.columns)
fr_matrix.head()
fr_matrix = pd.read_excel(data, sheet_names = 1, header = None).iloc[2:,1:]
len(fr_matrix.columns)
fr_matrix.head()
fr_matrix = pd.read_excel(data, sheet_names = 1, header = None).iloc[1:,1:]
len(fr_matrix.columns)
fr_matrix.head()
len(fr_matrix.iloc[1,1])
len(fr_matrix.iloc[1])
len(fr_matrix.iloc[:,1])
fr_matrix.head(-1)
fr_matrix.tail()
fr_matrix.tail(15)
fr_matrix = pd.read_excel(data, sheet_names = 1, header = None).iloc[1:46,1:]
fr_matrix
fr_matrix.tail(15)
len(fr_matrix.iloc[:,1])
len(fr_matrix.iloc[,1])
len(fr_matrix.iloc[:,1])
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

years = [name.split()[2] for name in data.sheet_names]

f  = pd.read_excel(data, sheet_names = 1, header = None).iloc[1:46,1:]
len(f.columns)
f.tail(15)

len(f.iloc[:,1])


r.iloc[1][0]
f.iloc[1][0]
f  = pd.read_excel(data, sheet_names = 1, header = None).iloc[1:46,1:]
f.iloc[1,0]
flen = len(f.iloc[:,1])+1
flen
flen = len(f.iloc[:,1])
f
for i in range(1,flen+1):
    f.iloc[i]
    print(f.iloc[i])

range(1,flen+1)
for i in range(1,flen+1):
    f.iloc[i,0]
    print(f.iloc[i,0])

f.iloc[i,0]
f
flen+1
range(1,flen+1)
print(range(1,flen+1))
for i in range(1,flen+1):
    f.iloc[i,0]
    print(i)

for i in range(1,flen+1):
    print(i)

f
f  = pd.read_excel(data, sheet_names = 1, header = None).iloc[1:46,1:]
f
f.iloc[:,:]
f.iloc[0,0]
for i in range(1,flen+1):
    print(f.iloc[i,0])
    print(i)

f.shape
flen+1
for i in range(1,flen):
    print(f.iloc[i,0])
    print(i)

flen
my_list = [1,2,3,4,5]
len(my_list)
for i in range(1,flen):
    print(f.iloc[i,0])
    print(i)

dplen = len(f.iloc[1,:])
years = [name.split()[2] for name in data.sheet_names]
[name.split()[2] for name in data.sheet_names]
for k in [name.split()[2] for name in data.sheet_names]:
    print(k)

flen
for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]

lplen = len(f.iloc[:,1])
dplen = len(f.iloc[1,:])
for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]

rates = []

for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]
            rates.append(zip(LoadPort,DischargePort,Year,Rate ))

rates = []

for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]
            rates.append(zip(LoadPort,DischargePort,Year,Rate))

Rate = f.iloc[i,j]
for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]
            rates.append(pd.DataFrame({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate}))

rates = []

for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]
            rates.append(pd.DataFrame({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate}, index = [0], ignore_index=True))

for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]
            rates.append(pd.DataFrame({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate}, index = [0]), ignore_index=True)

for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})

rates = []

for k in [name.split()[2] for name in data.sheet_names]:
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = k
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})

for k in enumerate([name.split()[2] for name in data.sheet_names]):
    print(k)

rates = []

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(data, sheet_names = x, header = None).iloc[1:46,1:]
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


pd.Dataframe(rates)
rates = []

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(data, sheet_names = x, header = None).iloc[1:46,1:]
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


pd.DataFrame(rates)
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []
lplen = len(f.iloc[:,1])
dplen = len(f.iloc[1,:])
for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(data, sheet_names = x, header = None).iloc[1:46,1:]
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


pd.DataFrame(rates)
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []
lplen = len(f.iloc[:,1])
dplen = len(f.iloc[1,:])
for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(data, sheet_names = x, header = None).iloc[1:46,1:]
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


flat_rate_table = pd.DataFrame(rates)
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []
lplen = len(f.iloc[:,1])
dplen = len(f.iloc[1,:])
for x,y in enumerate([name.split()[2] for name in data.sheet_names]):

f  = pd.read_excel(raw_rates, sheet_names = x, header = None).iloc[1:46,1:]
for j in range(1, dplen):
    for i in range(1,lplen):
        LoadPort = f.iloc[i,0]
        DischargePort = f.iloc[0,j]
        Year = y
        Rate = f.iloc[i,j]
        rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)


def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(raw_rates, sheet_names = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)


def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
flat_rate_table
enumerate([name.split()[2] for name in data.sheet_names])
f  = pd.read_excel(data, sheet_names = 2, header = None).iloc[1:46,1:]
f
import pandas as pd 

data = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(data, sheet_names = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


flat_rate_table = pd.DataFrame(rates)
flat_rate_table
f
for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(data, sheet_names = x, header = None).iloc[1:46,1:]
    print(f.iloc[5,5])

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(data, sheet_names = x, header = None).iloc[1:46,1:]
    print(f.iloc[5,5])
    print(x)
    print(y)

data
f  = pd.read_excel(data, sheet_names = 1, header = None).iloc[1:46,1:]
f
f  = pd.read_excel(data, sheet_names = 2, header = None).iloc[1:46,1:]
f
f.sheet_names
data.sheet_names
for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
rate_data
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
rate_data
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []

for x,y in enumerate([name.split()[2] for name in data.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
rate_data
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
crude = 'Azeri'
destination = 'Rotterdam'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    

raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
calculate_flat_rate()
flat_rate_table["Year"] 
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")
flat_rate_table
total
df
df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return int(flat_rate_row["Rate"].iat[0])


df['Rate'] = df.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return float(flat_rate_row["Rate"].iat[0])


df['Rate'] = df.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
print("df['Rate'] created successfully")
df
df[0]
df
df.iloc[0]
dt.strptime(str((df.iloc[0]).name), '%Y-%m-%d %H:%M:%S').year
df.iloc[0]).name
(df.iloc[0]).name
df['Empty'] = 0
def calculate_flat_rates(flat_rate_table, x):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return float(flat_rate_row["Rate"].iat[0])


df['Rate'] = df.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, x), axis =1)
df
flat_rate_row["Rate"].iat[0]
flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
x
df = pd.DataFrame(index=total.index)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
df
def calculate_flat_rates(flat_rate_table, dfrow):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(dfrow.name), '%Y-%m-%d %H:%M:%S').year)]
    return float(flat_rate_row["Rate"].iat[0])


df['Rate'] = df.apply(lambda x: calculate_flat_rates(flat_rate_table, dfrow), axis =1)
def calculate_flat_rates(flat_rate_table, dfrow):
    flat_rate_row = flat_rate_table[(flat_rate_table["Year"] == dt.strptime(str(dfrow.name), '%Y-%m-%d %H:%M:%S').year)]
    return float(flat_rate_row["Rate"].iat[0])


df['Rate'] = total.apply(lambda x: calculate_flat_rates(flat_rate_table, dfrow), axis =1)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import ArbEcons

arbs = ArbEcons.arb('Azeri','Houston')
print(arbs)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
(4 * (cfd2 - cfd1) / 14)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)


def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 

def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)


#calculate_flat_rates('2017-12-06 00:00:00')

df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment(df=df):
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    def apply_expiry(expiry_table, x):
        expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                                  (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
        return expiry_row["Expiry"].iat[0]
    
    df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
    print("df['Expiry'] created successfully")

df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
crude = 'Azeri'
destination = 'Rotterdam'
destination = 'Houston'
df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
ndex_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
"""
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
print("df['Expiry'] created successfully")

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
df['diff_vs_wti']
df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
df['diff_vs_wti']
convert_wti()
def convert_wti():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
        else: 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
       df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    return df

df['diff_vs_wti'] = convert_wti()
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]

df
total[assay[crude]['Code']]
total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
total[assay[crude]['Code']]
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
assay[crude]['Index']
crude = 'Azeri'
destination = 'Houston'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)


def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 

def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)


#calculate_flat_rates('2017-12-06 00:00:00')

df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment(df=df):
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    def apply_expiry(expiry_table, x):
        expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                                  (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
        return expiry_row["Expiry"].iat[0]
    
    df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
    print("df['Expiry'] created successfully")
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    
    def convert_wti():
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any(): 
                df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
            else:
                df['diff_vs_wti'] = total[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dtd:
            if (cfd1 > cfd2).any(): 
                df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
            else: 
                df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
           df['diff_vs_wti'] = total[assay[crude]['Code']]
        
        return df
    
    df['diff_vs_wti'] = convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any():
                if (cfd1 > cfd2).any():
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2
                else:
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1
            else:
                if (cfd1 > cfd2).any():
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2
                else:
                    df['diff_vs_dtd'] = total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1
        
        elif assay[crude]['Index'] in index_dtd:
            
            #if (total.index < dt(2018,7,28)).any():
            if (cfd1 > cfd2).any():              
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14) # the 4 will be replaces by the journey time between thge two ports
            else:
                df['diff_vs_dtd'] =  total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14) # the 4 will be replaces by the journey time between thge two ports
            #else:
            #    df['diff_vs_dtd'] =  trader_assessed_prices[assay[crude]['Code']]
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = total[assay[crude]['Code']]
        
        return df['diff_vs_dtd']
    
    df['diff_vs_dtd'] = convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if assay[crude]['Index'] in index_wti:
            if (df.index < df.Expiry).any():
                df['diff_vs_dub']= total[assay[crude]['Code']] +  (wtim1 -  wtim2)  - brentm2 + efs2
            else:
                df['diff_vs_dub'] = total[assay[crude]['Code']] - brentm2 + efs2
        
        elif assay[crude]['Index'] in index_dtd:
            if (cfd1 > cfd2).any():  
                df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2
            else:
                df['diff_vs_dub'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    df['diff_vs_dub'] = convert_dub()
    print("convert_dub() created successfully")
    
    return df


df = pricing_adjustment()
print("pricing_adjustment() created successfully")

return df
df['diff_vs_wti']
df
crude = 'Azeri'
destination = 'Houston'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)


def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 

def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)


#calculate_flat_rates('2017-12-06 00:00:00')

df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")
df
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
convert_wti()
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
"""
def apply_expiry(expiry_table, x):
    expiry_row = expiry_table[(expiry_table["Month"].dt.month == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').month)&
                              (expiry_table["Month"].dt.year == dt.strptime(str(x.name), '%Y-%m-%d %H:%M:%S').year)]
    return expiry_row["Expiry"].iat[0]


df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
print("df['Expiry'] created successfully")

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']

def convert_wti():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
        else: 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
       df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    return df


df['diff_vs_wti'] = convert_wti()
td = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']

def convert_wti():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
        else: 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
       df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    return df


df['diff_vs_wti'] = convert_wti()
df
assay[crude]['Index']
index_dtd
cfd1
cfd2
total[assay[crude]['Code']]
cfd2
efpm2
brentm2
wtim2
total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
df['diff_vs_wti']
df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
df['diff_vs_wti'] 
expiry_table
float(expiry_table[(expiry_table["Month"].dt.month == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                          (expiry_table["Month"].dt.year == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))].values)
x = '2015-04-01'
float(expiry_table[(expiry_table["Month"].dt.month == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                          (expiry_table["Month"].dt.year == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].values)
float(expiry_table[(expiry_table["Month"].dt.month == str(dt.strptime(str(x), '%Y-%m-%d').month))&
                          (expiry_table["Month"].dt.year == str(dt.strptime(str(x), '%Y-%m-%d').year))]["Expiry"].values)
str(dt.strptime(str(x), '%Y-%m-%d').month
str(dt.strptime(str(x), '%Y-%m-%d').month)
(expiry_table["Month"].dt.month 
expiry_table["Month"].dt.month
str(dt.strptime(str(x), '%Y-%m-%d').year)
expiry_table["Month"].dt.year
expiry_table[(expiry_table["Month"].dt.month == str(dt.strptime(str(x), '%Y-%m-%d').month))&
                          (expiry_table["Month"].dt.year == str(dt.strptime(str(x), '%Y-%m-%d').year))]["Expiry"]
(expiry_table["Month"].dt.month == str(dt.strptime(str(x), '%Y-%m-%d').month))
type(expiry_table["Month"].dt.month)
expiry_table["Month"].dt.month
(expiry_table["Month"].dt.year == str(dt.strptime(str(x), '%Y-%m-%d').year))
(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d').month))
float(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d').month))&
                          (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d').year))]["Expiry"].values)
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d').month))&
                          (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d').year))]["Expiry"]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d').year))]["Expiry"]
def apply_expiry(x):
    #x = '2015-04-01'            
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d').year))]["Expiry"]

index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)

df
df['Expiry'] = df.apply(lambda x: apply_expiry(expiry_table, x), axis =1)
df['Expiry'] = df.apply(lambda x: apply_expiry(x), axis =1)
def apply_expiry(x):
    #x = '2015-04-01'            
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(x, '%Y-%m-%d').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(x, '%Y-%m-%d').year))]["Expiry"]


df['Expiry'] = df.apply(lambda x: apply_expiry(x), axis =1)
def apply_expiry(x):
    #x = '2015-04-01'            
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(x, '%Y-%m-%d').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(x, '%Y-%m-%d').year))]["Expiry"]


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x), axis =1)
def apply_expiry(x):
    #x = '2015-04-01'            
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(x, '%Y-%m-%d').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(x, '%Y-%m-%d').year))]["Expiry"]


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
def apply_expiry(x):
    #x = '2015-04-01'            
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d').year))]["Expiry"]


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
(expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
f apply_expiry(x):
    #x = '2015-04-01'            
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
def apply_expiry(x):
    #x = '2015-04-01'            
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
df['Expiry']
expiry_table
expiry_table["Month"].dt.month
int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month
int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month)
df.index.to_series()
x = '2015-04-01'
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
x = df.index.to_series()   
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x[0]), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x[0]), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
def apply_expiry(x):
    #x = df.index.to_series()           
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
df['Expiry'] 
expiry_table
expiry_table["Month"].dt.month
int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month)
expiry_table
def apply_expiry(x):
    #x = df.index.to_series()           
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
df
df.index.to_series()
def apply_expiry(x):
    #x = df.index.to_series()           
    return str(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
df['Expiry']
def apply_expiry(x):
    #x = df.index.to_series()           
    return str(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])[0]


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print("df['Expiry'] created successfully")
df['Expiry']
def apply_expiry(x):
    #x = df.index.to_series()           
    return str(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])[1]


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
def apply_expiry(x):
    #x = df.index.to_series()           
    return str(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])[1]


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
def apply_expiry(x):
    #x = df.index.to_series()           
    return str(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
def apply_expiry(x):
    #x = df.index.to_series()           
    return int(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
x = df.index.to_series()[0]   
x
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
dt.strftime(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].values
def apply_expiry(x):
    #x = df.index.to_series()[0]           
    return expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].values

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
def apply_expiry(x):
    #x = df.index.to_series()[0]           
    y =  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].values
    
    return dt.strptime(str(y), '%Y-%m-%d %H:%M:%S')

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
def apply_expiry(x):
    #x = df.index.to_series()[0]           
    y =  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].values
    
    return dt.strptime(str(y[0]), '%Y-%m-%d %H:%M:%S')

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].values
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
x = df.index.to_series()[0]           
y =  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
y.strftime'(%Y-%m-%d')
y.strftime('%Y-%m-%d')
y.dt.strftime('%Y-%m-%d')
y =  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
y
y.dt.strftime('%Y-%m-%d')
float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)
str(dt.strptime(str(y[0]), '%Y-%m-%d %H:%M:%S'))
dt.strptime(str(y[0]), '%Y-%m-%d %H:%M:%S')
str(y.dt.strftime('%Y-%m-%d'))
int(y.dt.strftime('%Y-%m-%d'))
float(y.dt.strftime('%Y-%m-%d'))
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].dt.strftime('%Y-%m-%d')
def apply_expiry(x):
    #x = df.index.to_series()[0]           
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].dt.strftime('%Y-%m-%d')

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
expiry_table["Month"]
df.index.to_series()
df.index.to_series()[0] 
x = datetime(2016,02,12)
x = dt(2016,2,12)
x
expiry_table
expiry_table["Month"].dt.month
int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month
x = df.index.to_series()[0] 
int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month
int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month)
xpiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].date()
type(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"])
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iloc[0][0] 
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].date
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].dt.date
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
type(expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].dt.date) 
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]
flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate']
flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values
float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
expiry_table
df.index.to_series()
(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].dt.date
)  .date() dt.strftime('%Y-%m-%d')
x = df.index.to_series()[0] 
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].dt.date
flat_rate_table
flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate']
float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
def apply_expiry(x):
    #x = df.index.to_series()[0] 
    
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
df['Expiry'] 
df
expiry_table
df.index.to_series()
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0]
def apply_expiry(x):
    #x = df.index.to_series()[0] 
    
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0]

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"]
expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                    (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iloc[0]
def apply_expiry(x):
    #x = df.index.to_series()[0] 
    
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iloc[0]

def apply_expiry(x):
    #x = df.index.to_series()[0] 
    
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iloc[0]

df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
print(df['Expiry'])
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
crude = 'Azeri'
destination = 'Houston'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")

"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)


def calculate_flat_rate():
    
    flat_rate_table = pd.DataFrame(rate_data[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                     (rate_data['DischargePort'] == destination)])
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")




"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 

def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)


#calculate_flat_rates('2017-12-06 00:00:00')

df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


df = calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment():
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BRENT DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    def apply_expiry(x):
        #x = df.index.to_series()[0] 
        return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                            (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date
    
    df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
    print(df['Expiry'])
    
    
    print("df['Expiry'] created successfully")
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']

def convert_wti():
    if assay[crude]['Index'] in index_wti:
        if (df.index < df.Expiry).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    elif assay[crude]['Index'] in index_dtd:
        if (cfd1 > cfd2).any(): 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
        else: 
            df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
       df['diff_vs_wti'] = total[assay[crude]['Code']]
    
    return df

df['diff_vs_wti'] = convert_wti()
df
df.Expiry
df.index
df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
df['diff_vs_wti'] = convert_wti()
df
df.Expiry
df.index
df.index < df.Expiry
if (df.index < df.Expiry).any(): 
    df['diff_vs_wti'] = total[assay[crude]['Code']] +  (wtim1 -  wtim2) 
else:
    df['diff_vs_wti'] = total[assay[crude]['Code']]

(cfd1 > cfd2).any()
cfd1
cfd2
df.index < df.Expiry
cfd1 > cfd2
(cfd1 > cfd2).any()
(df.index < df.Expiry).any()
cfd1 > cfd2
if cfd1 > cfd2: 
    df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2)
else: 
    df['diff_vs_wti'] = total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2)

(df.index < df.Expiry)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
df[(df.index < df.Expiry)]
df[(df.index < df.Expiry)]['mike'] = 10
df
mask = df.index < df.Expiry
mask
df[mask]
df['newcol'] = np.where(mask,1,0)
df
df['diff_vs_wti'] = np.where(df.index < df.Expiry, total[assay[crude]['Code']] + (wtim1 -  wtim2), total[assay[crude]['Code']])
df
cfd1 > cfd2
df['diff_vs_wti']  = np.where(cfd1 > cfd2,
                      total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                      total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
def convert_wti():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_wti'] = np.where(df.index < df.Expiry,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                      total[assay[crude]['Code']])
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_wti']  = np.where(cfd1 > cfd2,
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif assay[crude]['Index'] in index_dub:
        pass
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]
    return df

df['diff_vs_wti'] = convert_wti()
df
df['diff_vs_wti'] = np.where(df.index < df.Expiry,
                              total[assay[crude]['Code']] + (wtim1 -  wtim2),
                              total[assay[crude]['Code']])
df['diff_vs_wti']
] in index_dtd:
    df['diff_vs_wti']  = np.where(cfd1 > cfd2,
                                  total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                  total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
df['diff_vs_wti']  = np.where(cfd1 > cfd2,
                              total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                              total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
df['diff_vs_wti']
df['diff_vs_wti'] = total[assay[crude]['Code']]
df['diff_vs_wti']
convert_wti()
df['diff_vs_wti'] = convert_wti()
df['diff_vs_wti']
convert_wti()
df
convert_wti()
assay[crude]['Index'] in index_wti
assay[crude]['Index']
index_wti
assay[crude]['Index'] in index_wti
indicies = [index_wti,index_dtd,index_dub]
indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
indicies
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
cfd1 > cfd2
assay[crude]['Index'] in indcies
assay[crude]['Index'] in indicies
assay[crude]['Index']
indicies[1]
indicies
assay[crude]['Index']
indicies
expiry_condition = df.index < df.Expiry
expiry_condition
expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
cfd_condition
np.invert(cfd_condition)
def convert_wti():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                      total[assay[crude]['Code']])
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif assay[crude]['Index'] in index_dub:
        pass
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]
    return df

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
np.invert(expiry_condition)
np.invert(cfd_condition)
convert_wti()
conditions = [
            (expiry_condition & cfd_condition),
            (expiry_condition & np.invert(cfd_condition)),
            (np.invert(expiry_condition) & cfd_condition),
            (np.invert(expiry_condition) & np.invert(cfd_condition))               
            ]
df['diff_vs_dtd'] = np.select(conditions, choices)
choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
           (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
           (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
           (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)
        ]
df['diff_vs_dtd'] = np.select(conditions, choices)
df['diff_vs_dtd']
def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                     total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = total[assay[crude]['Code']]
    
    return df['diff_vs_dtd']

convert_dtd()
total[assay[crude]['Code']
total[assay[crude]['Code']]
df['diff_vs_dub'] = np.where(expiry_condition,
                              total[assay[crude]['Code']] + (wtim1 -  wtim2)  - brentm2 + efs2),
                              total[assay[crude]['Code']] - brentm2 + efs2)
df['diff_vs_dub'] = np.where(expiry_condition,
                              total[assay[crude]['Code']] + (wtim1 -  wtim2)  - brentm2 + efs2,
                              total[assay[crude]['Code']] - brentm2 + efs2)
df['diff_vs_dub']
df['diff_vs_dub'] = np.where(expiry_condition,
                              total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                              total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
df['diff_vs_dub']
total[assay[crude]['Code']]
df['diff_vs_dub'] = total[assay[crude]['Code']] - brentm2 + efs2
df['diff_vs_dub']
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
flat_rate_table['Year']
df.index.year
df.index.year == flat_rate_table['Year']
df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))
df['NewRate'] = np.apply_along_axis(calculate_flat_rates(x))
def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == str(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)]['Rate'].values)

df['NewRate'] = np.apply_along_axis(calculate_flat_rates(x),1,df)
df['NewRate'] = np.apply_along_axis(calculate_flat_rates(x),0,df)
flat_rate_table[flat_rate_table["Year"]
flat_rate_table["Year"] 
df
df['NewRate'] = np.apply_along_axis(calculate_flat_rates(x),0,df.index.to_series())
df['NewRate'] = np.apply_along_axis(calculate_flat_rates(x),1,df.index.to_series())
df['NewRate'] = np.apply_along_axis(calculate_flat_rates(x),1,arr = df.index.to_series())
df.index.to_series()
['Rate']
df['NewRate'] = np.apply_along_axis(calculate_flat_rates(x),1,arr = df.index.to_series())
x
df['NewRate'] = np.apply_along_axis(calculate_flat_rates,1,df.index.to_series())
df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,df.index.to_series())
df.index.to_series()
df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.to_series()))
np.array(df.index.to_series())
np.asarray(df.index.to_series())
np.array(df.index)
np.array(df.index).year
np.array(df.index.year)
df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == x)]['Rate'].values)


#calculate_flat_rates('2017-12-06 00:00:00')


df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == x]['Rate'].values)


#calculate_flat_rates('2017-12-06 00:00:00')


df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"] == x]['Rate'].values)

np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
np.array(df.index.year)
np.array(df.index.year)[0]
flat_rate_table[flat_rate_table["Year"]
flat_rate_table["Year"]
x = np.array(df.index.year)[0]
float(flat_rate_table[flat_rate_table["Year"] == x]['Rate'].values)
x = int(np.array(df.index.year)[0])
float(flat_rate_table[flat_rate_table["Year"] == x]['Rate'].values)
x
flat_rate_table["Year"]
flat_rate_table["Year"] == x
x = int(np.array(df.index.year)[0])
flat_rate_table["Year"] == x
x = np.array(df.index.year)[0]
flat_rate_table[flat_rate_table["Year"] == x.astype(int)
flat_rate_table["Year"] == x.astype(int)
x.astype(int)
flat_rate_table["Year"]
flat_rate_table["Year"].iloc[0]
type(flat_rate_table["Year"].iloc[0])
flat_rate_table["Year"].astype(int)
flat_rate_table["Year"].astype(int) == x.astype(int)
def calculate_flat_rates(x):
    
    return float(flat_rate_table[flat_rate_table["Year"].astype(int) == x.astype(int)]['Rate'].values)

df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
calculate_flat_rates
np.array(df.index.year)
df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,arr=np.array(df.index.year))
np.apply_along_axis(calculate_flat_rates,0,arr=np.array(df.index.year))
flat_rate_table[flat_rate_table["Year"].astype(int) == x.astype(int)]['Rate'].values
def calculate_flat_rates(x):
    
    return flat_rate_table[flat_rate_table["Year"].astype(int) == x.astype(int)]['Rate'].values

df['NewRate'] = np.apply_along_axis(calculate_flat_rates,0,arr=np.array(df.index.year))
df['NewRate'] = np.apply_along_axis(calculate_flat_rates,1,arr=np.array(df.index.year))
np.array(df.index.year)
df['NewRate'] = np.where(df.index.year == flat_rate_table['Year'],flat_rate_table['Rate'],0 )
df['NewRate'] = np.where((df.index.year == flat_rate_table['Year']),flat_rate_table['Rate'],0 )
.index.year == flat_rate_table['Year']
df.index.year == flat_rate_table['Year']
x = np.array(df.index.year)[0]
flat_rate_table[flat_rate_table["Year"].astype(int) == x.astype(int)]['Rate'].values
df['NewRate'] = np.apply_along_axis(lambda x: calculate_flat_rates(x))
df['NewRate'] = np.apply_along_axis(lambda x: calculate_flat_rates(x), 0, np.array(df.index.year))
np.array(df.index.year)
flat_rate_table.iloc[x]
x = np.array(df.index.year)[0]
flat_rate_table['Year'] == x
x
np.array(df.index.year)[0].astype(int)
x = np.array(df.index.year)[0].astype(int)
flat_rate_table['Year'] == x
flat_rate_table['Year'].astype(int) == x
np.where(flat_rate_table['Year'].astype(int) == x, flat_rate_table['Rate'],0)
np.where(flat_rate_table['Year'].astype(int) == x, flat_rate_table['Rate'])
x.year == flat_rate_table['Year'].astype(int)
np.array(df.index.year)[0]
x = np.array(df.index.year)[0]
flat_rate_table[flat_rate_table["Year"].astype(int) == x.astype(int)]['Rate'].values
np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
flat_rate_table.loc[flat_rate_table['Year'] = 2018]
flat_rate_table.loc[flat_rate_table['Year'] == 2018]
flat_rate_table.loc[flat_rate_table['Year'] == '2018']
flat_rate_table.iloc[flat_rate_table['Year'] == '2018']
flat_rate_table.loc[flat_rate_table['Year'] == '2018', flat_rate_table['Rate']]
flat_rate_table.loc[flat_rate_table['Year'] == '2018', ['Rate']]
flat_rate_table.loc[flat_rate_table['Year'] == '2018', 'Rate']
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x.astype(int), 'Rate'])

np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))
np.apply_along_axis(calculate_flat_rates,0,np.array(df.index.year))
x
flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x.astype(int), 'Rate']
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x.astype(int), 'Rate'])

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
flat_rate_table
df['NewRate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
df['NewRate']
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
rate_data
rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
np.array(rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)])
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
expiry_table['Month'].astype(int)
expiry_table['Month']
expiry_table['Month'].dt.month
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&
                                  (expiry_table['Month'].dt.year == x.year), 'Expiry']).iat[0]


v_apply_expiry(x) =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.datetime64(df.index.year))
ef apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&
                                  (expiry_table['Month'].dt.year == x.year), 'Expiry']).iat[0]

v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.datetime64(df.index.year))
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&
                                  (expiry_table['Month'].dt.year == x.year), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.datetime64(df.index.year))
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&
                                  (expiry_table['Month'].dt.year == x.year), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.datetime64(df.index))
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&
                                  (expiry_table['Month'].dt.year == x.year), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.array(df.index))
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&
                                  (expiry_table['Month'].dt.year == x.year), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,df.index)
df.index
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,df.index)
df.index
[df.index.year,df.index.month]
(df.index.year,df.index.month)
list(df.index.year,df.index.month)
list([df.index.year,df.index.month])
np.array(df.index.year,df.index.month)
np.array([df.index.year,df.index.month])
[df.index.year,df.index.month for i,j in df.index]
[i,j for df.index.year,df.index.month in df.index]
expiry_table['Month'].dt.month
[df.index.year,df.index.month]
[df.index.year,df.index.month][0,0
[df.index.year,df.index.month][0,0]
[df.index.year,df.index.month][0]
np.array(df.index.year,df.index.month)
df.index.year
np.array[df.index.year,df.index.month]
np.array([df.index.year],[df.index.month])
[df.index.year]
np.array([[df.index.year],[df.index.month]])
list(df.index.year)
np.array([list(df.index.year),list(df.index.month)])
np.array((list(df.index.year),list(df.index.month)))
y = np.array((list(df.index.year),list(df.index.month)))
y[0,0]
y[0,1]
y = np.array((list(df.index.year),list(df.index.month))).transpose()
y[0,1]
y[0]
np.array((list(df.index.year),list(df.index.month)))
f = np.array((list(df.index.year),list(df.index.month))).transpose()
f[0]
f = np.array((list(df.index.year),list(df.index.month)))
f[0]
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.array((list(df.index.year),list(df.index.month))))
np.array((list(df.index.year),list(df.index.month)))
expiry_table['Month'].dt.month
f[0]
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.array((list(df.index.year),list(df.index.month))))
y = np.array((list(df.index.year),list(df.index.month))).transpose()
y[0]
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[0][1])&
                                  (expiry_table['Month'].dt.year == x[0][0]), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.array((list(df.index.year),list(df.index.month))).transpose())
np.array((list(df.index.year),list(df.index.month))).transpose()
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.array((list(df.index.year),list(df.index.month))).transpose())
np.array((list(df.index.year),list(df.index.month))).transpose()
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


v_apply_expiry =  np.vectorize(apply_expiry)
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,np.array((list(df.index.year),list(df.index.month))).transpose())
index_for_func = p.array((list(df.index.year),list(df.index.month))).transpose()
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,index_for_func)
index_for_func[0]
np.apply_along_axis(v_apply_expiry,0,index_for_func)
index_for_func = [np.array((list(df.index.year),list(df.index.month))).transpose()]
index_for_func
df['Expiry'] = np.apply_along_axis(v_apply_expiry,0,index_for_func)
index_for_func[0]
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,0,index_for_func)
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
df['Expiry']
def apply_expiry(x):
    #x = df.index.to_series()[0] 
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
#index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
def apply_expiry(x):
    #x = df.index.to_series()[0] 
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
import time
import time

t = time.process_time()
#do some stuff


def apply_expiry(x):
    #x = df.index.to_series()[0] 
    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
                        (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date


df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))

elapsed_time = time.process_time() - t
print(elapsed_time)
 t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
#index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
elapsed_time = time.process_time() - t
print(elapsed_time)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
debugfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))
t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))
def pricing_adjustment():
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    
    #import time
    
    #t = time.process_time()
    #do some stuff
    
    
    #def apply_expiry(x):
        #x = df.index.to_series()[0] 
    #    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
     #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date
    
    #df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
    
    #elapsed_time = time.process_time() - t
    #print(elapsed_time)
    
    """
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                          total[assay[crude]['Code']])
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif assay[crude]['Index'] in index_dub:
            pass
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                         total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = total[assay[crude]['Code']]
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    
    return df

t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))
pricing_adjustment()
ws_table
ws
sub_to_ws
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
total[assay[crude]['Code']]
raw_rates
total
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
prices_reference
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
flat_rate_table
rate_data
flat_rate_table
df.index.year
df
df.index
df.index.year
assay[crude]['LoadPort']
ws_codes
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)

sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)

ws_codes = ws[(ws['Origin'] == sub_region) &
                  (ws['Destination'] == sub_region_2)]
ws
ws_codes
ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
ws_codes['Code']
ws_codes
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']

ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
name
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
import ArbEcons
import ArbEcons

arbs = ArbEcons.arb('Azeri','Augusta')
print(arbs)
import ArbEcons

arbs = ArbEcons.arb('Forties','Augusta')
print(arbs)
assay[crude]['Index']
index_dtd
cfd_condition
f['diff_vs_dub']
df['diff_vs_dub'] 
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
df['diff_vs_dub'] = np.where(cfd_condition, 
                              total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                              total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
df['diff_vs_dub']
def convert_dub():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 

convert_dub()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
convert_dub()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
df
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")


"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment():
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    
    #import time
    
    #t = time.process_time()
    #do some stuff
    
    
    #def apply_expiry(x):
        #x = df.index.to_series()[0] 
    #    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
     #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date
    
    #df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
    
    #elapsed_time = time.process_time() - t
    #print(elapsed_time)
    
    """
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                          total[assay[crude]['Code']])
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif assay[crude]['Index'] in index_dub:
            pass
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                         total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = total[assay[crude]['Code']]
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    
    return df


t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))

return df
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")


"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment():
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    
    #import time
    
    #t = time.process_time()
    #do some stuff
    
    
    #def apply_expiry(x):
        #x = df.index.to_series()[0] 
    #    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
     #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date
    
    #df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
    
    #elapsed_time = time.process_time() - t
    #print(elapsed_time)
    
    """
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                          total[assay[crude]['Code']])
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif assay[crude]['Index'] in index_dub:
            pass
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                         total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = total[assay[crude]['Code']]
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    
    return df


t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))

return df
crude = 'Azeri'
destination = 'Houston'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")


"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

def pricing_adjustment():
    index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
    index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
    index_dub = ['DUBAI','DUBAI M2']
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    
    #import time
    
    #t = time.process_time()
    #do some stuff
    
    
    #def apply_expiry(x):
        #x = df.index.to_series()[0] 
    #    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
     #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date
    
    #df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
    
    #elapsed_time = time.process_time() - t
    #print(elapsed_time)
    
    """
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                          total[assay[crude]['Code']])
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif assay[crude]['Index'] in index_dub:
            pass
        else:
            df['diff_vs_wti'] = total[assay[crude]['Code']]
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
                   (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                         total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = total[assay[crude]['Code']]
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if assay[crude]['Index'] in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
        
        elif assay[crude]['Index'] in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                          total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
        
        elif assay[crude]['Index'] in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    
    return df


t = time.process_time()
pricing_adjustment()
print("pricing_adjustment() created successfully: Time was {}".format(time.process_time() - t))

return df
crude = 'Azeri'
destination = 'Houston'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")


"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

#def pricing_adjustment():
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
"""

#import time

#t = time.process_time()
#do some stuff


#def apply_expiry(x):
    #x = df.index.to_series()[0] 
#    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
 #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date

#df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))

#elapsed_time = time.process_time() - t
#print(elapsed_time)

"""
Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                      total[assay[crude]['Code']])
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif assay[crude]['Index'] in index_dub:
        pass
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]
    return df


convert_wti()
print("convert_wti() created successfully")
def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                     total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = total[assay[crude]['Code']]
    
    return df['diff_vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")
def convert_dub():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 


convert_dub()
print("convert_dub() created successfully")
df
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
debugfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
cfd_condition
debugfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
assay[crude]['Index']
df['diff_vs_dub']
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")


"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

#def pricing_adjustment():
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
"""

#import time

#t = time.process_time()
#do some stuff


#def apply_expiry(x):
    #x = df.index.to_series()[0] 
#    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
 #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date

#df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))

#elapsed_time = time.process_time() - t
#print(elapsed_time)

"""
Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                      total[assay[crude]['Code']])
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif assay[crude]['Index'] in index_dub:
        pass
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]
    return df


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                     total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = total[assay[crude]['Code']]
    
    return df['diff_vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

return df
crude = 'Forties'
destination = 'Houston'

import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""

df = pd.DataFrame(index=total.index)
print("Temp DataFrame created successfully")


"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

#def pricing_adjustment():
index_wti = ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']
index_dtd = ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']
index_dub = ['DUBAI','DUBAI M2']
indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
"""

#import time

#t = time.process_time()
#do some stuff


#def apply_expiry(x):
    #x = df.index.to_series()[0] 
#    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
 #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date

#df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))

#elapsed_time = time.process_time() - t
#print(elapsed_time)

"""
Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2),
                                      total[assay[crude]['Code']])
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif assay[crude]['Index'] in index_dub:
        pass
    else:
        df['diff_vs_wti'] = total[assay[crude]['Code']]
    return df


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd2),
               (total[assay[crude]['Code']] - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     total[assay[crude]['Code']] + (4 * (cfd2 - cfd1) / 14),
                                     total[assay[crude]['Code']] + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = total[assay[crude]['Code']]
    
    return df['diff_vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if assay[crude]['Index'] in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      total[assay[crude]['Code']] + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      total[assay[crude]['Code']] - (brentm2-wtim2) + efs2)
    
    elif assay[crude]['Index'] in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      total[assay[crude]['Code']] + cfd2 +  efpm2 + efs2, 
                                      total[assay[crude]['Code']] + cfd1 +  efpm2 + efs2)
    
    elif assay[crude]['Index'] in index_dub:
        pass
    
    return df['diff_vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

return df
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED	BREN, DATED','N.SEA DATED','BTC Dated',	'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','	GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
assay[crude]['Index']
crude_vs = assay[crude]['Index']
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
def import_data():
    crude = 'Forties'
    destination = 'Houston'
    
    import pandas as pd
    import numpy as np
    from datetime import datetime as dt
    import random
    import time
    
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    return assay, ws, expiry_table, ports, total, rate_data
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """

def arb(crude,destination):
    df = pd.DataFrame(index=total.index)
    print("Temp DataFrame created successfully")
    
    
    """
    create the flat rates table for the rates calculations and column creation
    """
    
    def calculate_flat_rate():
        
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        return flat_rate_table
    
    flat_rate_table = calculate_flat_rate()
    print("flat_rate_table created successfully")
    
    
    """
    THIS IS ESSENTIALLY A VLOOKUP
    
    inputs are the table to look up values from for each row of another table
    
    in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
    and return the value in the rate row.
    
    use the .iat function to return the inger values - this speeds up operations
    
    x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
    Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year
    
    """
    
    #x = '2017-12-06 00:00:00'
    #type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
    #flat_rate_table = 
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """ 
    Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
    """
    
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    #df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))
    
    print("df['Rate'] created successfully")
    
    
    """
    Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
    We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
    As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
    We then convert the result to a string and we set index = false as we dont want the header row returned
    Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
    This gives us the neccessary world scale codes 
    
    We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
    If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
    If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor
    
    Final values will be in $/bbl
    
    """
    
    def calculate_world_scale():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
        
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        
        ws_codes = ws[(ws['Origin'] == sub_region) &
                          (ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
        
        return df
    
    calculate_world_scale()
    print("calculate_world_scale() created successfully")
    
    #def pricing_adjustment():
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    
    
    
    
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    """
    
    #import time
    
    #t = time.process_time()
    #do some stuff
    
    
    #def apply_expiry(x):
        #x = df.index.to_series()[0] 
    #    return  expiry_table[(expiry_table["Month"].dt.month == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').month))&
     #                       (expiry_table["Month"].dt.year == int(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year))]["Expiry"].iat[0] #.dt.date
    
    #df['Expiry'] = df.index.to_series().apply(lambda x: apply_expiry(x))
    
    #elapsed_time = time.process_time() - t
    #print(elapsed_time)
    
    """
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if crude_vs in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2),
                                          diff)
        elif crude_vs in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df['diff_vs_wti'] = diff
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif crude_vs in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = diff
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if crude_vs in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        
        elif crude_vs in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    
    return df

runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
import ArbEcons

import_data()
arbs = ArbEcons.arb('Agbami','Rotterdam')
print(arbs)
import ArbEcons

ArbEcons.import_data()
arbs = ArbEcons.arb('Agbami','Rotterdam')
print(arbs)
ArbEcons.import_data()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
assay, ws, expiry_table, ports, total, rate_data = ArbEcons.import_data()
import ArbEcons

assay, ws, expiry_table, ports, total, rate_data = ArbEcons.import_data()
arbs = ArbEcons.arb('Agbami','Rotterdam')
dataframes = ArbEcons.import_data()
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import ArbEcons

assay, ws, expiry_table, ports, total, rate_data = ArbEcons.import_data()
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
import ArbEcons

assay, ws, expiry_table, ports, total, rate_data = ArbEcons.import_data()
ArbEcons.import_data()
import ArbEcons
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]
import ArbEcons,import_data()
import ArbEcons,import_data
import ArbEcons.import_data
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons import impor_data
from ArbEcons import import_data
assay, ws, expiry_table, ports, total, rate_data = import_data()
arbs = arb('Agbami','Rotterdam', total)
print(arbs)
arbs = arb('Agbami','Rotterdam')
print(arbs)
arbs = arb('Azeri','Rotterdam')
print(arbs)
from ArbEcons import import_data
from ArbExons import arb
from ArbEcons import import_data
from ArbEcons import arb
import_data()
arbs = arb('Azeri','Rotterdam')
print(arbs)
assay, ws, expiry_table, ports, total, rate_data = import_data()
arbs = arb('Azeri','Rotterdam')
print(arbs)
arbs = arb('Azeri','Rotterdam')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
def import_data():
    #crude = 'Forties'
    #destination = 'Houston'
    
    
    
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    return assay, ws, expiry_table, ports, total, rate_data, sub_to_ws

import_data()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arbs = arb('Azeri','Rotterdam', assay, ws, expiry_table, ports, total, rate_data, sub_to_ws)
print(arbs)
from ArbEcons import import_data
from ArbEcons import arb
arbs = arb('Azeri','Rotterdam', import_data())
print(arbs)
from ArbEcons import import_data
from ArbEcons import arb
arbs = arb('Azeri','Rotterdam', **kwargs)
print(arbs)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons import import_data
from ArbEcons import arb
import_data()
arbs = arb('Azeri','Rotterdam', **kwargs)
print(arbs)
arbs = arb('Azeri','Rotterdam')
print(arbs)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arbs = arb('Azeri','Rotterdam', assay, ws, expiry_table, ports, total, rate_data, sub_to_ws)
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arbs = arb('Azeri','Rotterdam', assay, ws, expiry_table, ports, total, rate_data, sub_to_ws)
expiry_table
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

class import_data():
    #crude = 'Forties'
    #destination = 'Houston'
    
    def __init__ (self):
        self.assay = assay
        self.ws = ws
        self.ports = ports
        self.total = total
        self.rate_data = rate_data
        self.sub_to_ws = sub_to_ws
        self.df = df    
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    #return assay, ws, ports, total, rate_data, sub_to_ws, df

import_data.assay
import_data.total
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEconsClasses import import_data
from ArbEconsClasses import arb
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import_data
import_data()
import_data
from ArbEconsClasses import import_data
from ArbEconsClasses import arb
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
expiry_table = pd.read_excel(data, 'expiry')
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
self.data
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time



class import_data():
    #crude = 'Forties'
    #destination = 'Houston'
    
    
    def __init__ (self):
        self.assay = assay
        self.ws = ws
        self.ports = ports
        self.total = total
        self.rate_data = rate_data
        self.sub_to_ws = sub_to_ws
        self.df = df 

# =============================================================================
#         self.expiry_table = expiry_table
#         self.data = data
#         self.raw_rates = raw_rates
#         self.ws_table = ws_table
# =============================================================================
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    #return assay, ws, ports, total, rate_data, sub_to_ws, df
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
def apply_expiry(self, x):
    expiry_table = self.expiry_table
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]

runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
def apply_expiry(x):
    global expiry_table
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]

runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
import_data()   
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time



class import_data():
    #crude = 'Forties'
    #destination = 'Houston'
    
    
    def __init__ (self):
        
        self.assay = assay
        self.ws = ws
        self.ports = ports
        self.total = total
        self.rate_data = rate_data
        self.sub_to_ws = sub_to_ws
        self.df = df 


#         self.expiry_table = expiry_table
#         self.data = data
#         self.raw_rates = raw_rates
#         self.ws_table = ws_table
# =============================================================================
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        expiry_table = self.expiry_table
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
def apply_expiry(x):
    expiry_table=41
    
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]

runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time



class import_data:
    #crude = 'Forties'
    #destination = 'Houston'
    
    
    def __init__ (self, assay, ws, ports, total, rate_data, sub_to_ws, df):
        #self.get_data = assay, ws, ports, total, rate_data, sub_to_ws, df
        self.assay = assay
        self.ws = ws
        self.ports = ports
        self.total = total
        self.rate_data = rate_data
        self.sub_to_ws = sub_to_ws
        self.df = df 


#         self.data = data
#         self.raw_rates = raw_rates
#         self.ws_table = ws_table
# =============================================================================
    @staticmethod  
    def get_data():
        t2 = time.process_time()
        #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004 (1).xlsx')
        #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete (1).xlsx')
        
        data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
        raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
        
        assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
        ws = pd.read_excel(data, 'ws')
        ws_table = pd.read_excel(data, 'ws_table', header = 1)
        #rate_data = pd.read_excel(data, 'flat_rate')
        prices = pd.read_excel(data, 'prices', header = 1)
        paper_prices = pd.read_excel(data, 'paper prices', header = 1)
        expiry_table = pd.read_excel(data, 'expiry')
        ports = pd.read_excel(data, 'ports')
        products = pd.read_excel(data, 'rott products')
        sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
        sub_to_ws = sub_to_ws.set_index([0]).to_dict()
        
        
        
        """
        Take in the crude prices and codes and convert to a dataframe.
        We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
        Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
        """
        prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
        
        
        """
        Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
        We don't drop rows now as dropping would be dependent on any nans in any column
        """
        total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
        total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
        total = total.iloc[total.index > dt(2015,12,31)]
        
        
        
        
        
        """
        Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
        """
        cleaned_column_headers = [i.strip() for i in total.columns.values]
        total.columns = cleaned_column_headers
        
        
        
        """
        This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
        """    
        
        
        rates = []
        
        for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
            f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
            lplen = len(f.iloc[:,1])
            dplen = len(f.iloc[1,:])
            for j in range(1, dplen):
                for i in range(1,lplen):
                    LoadPort = f.iloc[i,0]
                    DischargePort = f.iloc[0,j]
                    Year = y
                    Rate = f.iloc[i,j]
                    rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
        
        rate_data = pd.DataFrame(rates)
        
        """
        Also initialise the temp df with index of total.
        Temp df is tol hold the dataseries needed to calculate the freight
        """
        df = pd.DataFrame(index=total.index)
        
        """
        This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
        
        Have tried timing and slight improvment with the blow of 0.2seconds....
        """
        
        t = time.process_time()
        
        def apply_expiry(x):
            return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                          (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
        
        #v_apply_expiry =  np.vectorize(apply_expiry)
        index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
        df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
        print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
        print("Temp DataFrame created successfully")
        print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
        
        return assay, ws, ports, total, rate_data, sub_to_ws, df

# =============================================================================
#     assay, ws, ports, total, rate_data, sub_to_ws, df = self.get_data()
#     
# =============================================================================


print(import_data.get_data)
    #return assay, ws, ports, total, rate_data, sub_to_ws, df

import_data()   

def arb(crude,destination,):
    assay = import_data.assay
    ws = import_data.ws
    ports = import_data.ports
    total = import_data.total
    rate_data = import_data.rate_data
    sub_to_ws = import_data.sub_to_ws
    df = import_data.df
    
    
    """
    create the flat rates table for the rates calculations and column creation
    """
    
    def calculate_flat_rate():
        
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        return flat_rate_table
    
    flat_rate_table = calculate_flat_rate()
    print("flat_rate_table created successfully")
    
    
    """
    THIS IS ESSENTIALLY A VLOOKUP
    
    inputs are the table to look up values from for each row of another table
    
    in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
    and return the value in the rate row.
    
    use the .iat function to return the inger values - this speeds up operations
    
    x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
    Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year
    
    """
    
    #x = '2017-12-06 00:00:00'
    #type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
    #flat_rate_table = 
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """ 
    Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
    """
    
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    #df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))
    
    print("df['Rate'] created successfully")
    
    
    """
    Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
    We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
    As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
    We then convert the result to a string and we set index = false as we dont want the header row returned
    Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
    This gives us the neccessary world scale codes 
    
    We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
    If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
    If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor
    
    Final values will be in $/bbl
    
    """
    
    def calculate_world_scale():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
        
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        
        ws_codes = ws[(ws['Origin'] == sub_region) &
                          (ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
        
        return df
    
    calculate_world_scale()
    print("calculate_world_scale() created successfully")
    
    #def pricing_adjustment():
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    
    
    
    
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if crude_vs in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2),
                                          diff)
        elif crude_vs in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df['diff_vs_wti'] = diff
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif crude_vs in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = diff
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if crude_vs in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        
        elif crude_vs in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    print(df.head())
    
    return df
    
    #t = time.process_time()

runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time



class import_data:
    
    
    #crude = 'Forties'
    #destination = 'Houston'


# =============================================================================
    @staticmethod  
    def get_data():
        t2 = time.process_time()
        #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004 (1).xlsx')
        #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete (1).xlsx')
        
        data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
        raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
        
        assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
        ws = pd.read_excel(data, 'ws')
        ws_table = pd.read_excel(data, 'ws_table', header = 1)
        #rate_data = pd.read_excel(data, 'flat_rate')
        prices = pd.read_excel(data, 'prices', header = 1)
        paper_prices = pd.read_excel(data, 'paper prices', header = 1)
        expiry_table = pd.read_excel(data, 'expiry')
        ports = pd.read_excel(data, 'ports')
        products = pd.read_excel(data, 'rott products')
        sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
        sub_to_ws = sub_to_ws.set_index([0]).to_dict()
        
        
        
        """
        Take in the crude prices and codes and convert to a dataframe.
        We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
        Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
        """
        prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
        
        
        """
        Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
        We don't drop rows now as dropping would be dependent on any nans in any column
        """
        total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
        total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
        total = total.iloc[total.index > dt(2015,12,31)]
        
        
        
        
        
        """
        Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
        """
        cleaned_column_headers = [i.strip() for i in total.columns.values]
        total.columns = cleaned_column_headers
        
        
        
        """
        This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
        """    
        
        
        rates = []
        
        for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
            f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
            lplen = len(f.iloc[:,1])
            dplen = len(f.iloc[1,:])
            for j in range(1, dplen):
                for i in range(1,lplen):
                    LoadPort = f.iloc[i,0]
                    DischargePort = f.iloc[0,j]
                    Year = y
                    Rate = f.iloc[i,j]
                    rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
        
        rate_data = pd.DataFrame(rates)
        
        """
        Also initialise the temp df with index of total.
        Temp df is tol hold the dataseries needed to calculate the freight
        """
        df = pd.DataFrame(index=total.index)
        
        """
        This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
        
        Have tried timing and slight improvment with the blow of 0.2seconds....
        """
        
        t = time.process_time()
        
        def apply_expiry(x):
            return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                          (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
        
        #v_apply_expiry =  np.vectorize(apply_expiry)
        index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
        df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
        print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
        print("Temp DataFrame created successfully")
        print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
        
        return assay, ws, ports, total, rate_data, sub_to_ws, df
    
    get_data()
    
    
    def __init__ (self, assay, ws, ports, total, rate_data, sub_to_ws, df):
        #self.get_data = assay, ws, ports, total, rate_data, sub_to_ws, df
        self.assay = assay
        self.ws = ws
        self.ports = ports
        self.total = total
        self.rate_data = rate_data
        self.sub_to_ws = sub_to_ws
        self.df = df 
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time



class import_data:
    
    
    #crude = 'Forties'
    #destination = 'Houston'


# =============================================================================
    @staticmethod  
    def get_data():
        t2 = time.process_time()
        #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004 (1).xlsx')
        #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete (1).xlsx')
        
        data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
        raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
        
        assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
        ws = pd.read_excel(data, 'ws')
        ws_table = pd.read_excel(data, 'ws_table', header = 1)
        #rate_data = pd.read_excel(data, 'flat_rate')
        prices = pd.read_excel(data, 'prices', header = 1)
        paper_prices = pd.read_excel(data, 'paper prices', header = 1)
        expiry_table = pd.read_excel(data, 'expiry')
        ports = pd.read_excel(data, 'ports')
        products = pd.read_excel(data, 'rott products')
        sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
        sub_to_ws = sub_to_ws.set_index([0]).to_dict()
        
        
        
        """
        Take in the crude prices and codes and convert to a dataframe.
        We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
        Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
        """
        prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
        
        
        """
        Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
        We don't drop rows now as dropping would be dependent on any nans in any column
        """
        total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
        total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
        total = total.iloc[total.index > dt(2015,12,31)]
        
        
        
        
        
        """
        Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
        """
        cleaned_column_headers = [i.strip() for i in total.columns.values]
        total.columns = cleaned_column_headers
        
        
        
        """
        This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
        """    
        
        
        rates = []
        
        for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
            f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
            lplen = len(f.iloc[:,1])
            dplen = len(f.iloc[1,:])
            for j in range(1, dplen):
                for i in range(1,lplen):
                    LoadPort = f.iloc[i,0]
                    DischargePort = f.iloc[0,j]
                    Year = y
                    Rate = f.iloc[i,j]
                    rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
        
        rate_data = pd.DataFrame(rates)
        
        """
        Also initialise the temp df with index of total.
        Temp df is tol hold the dataseries needed to calculate the freight
        """
        df = pd.DataFrame(index=total.index)
        
        """
        This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
        
        Have tried timing and slight improvment with the blow of 0.2seconds....
        """
        
        t = time.process_time()
        
        def apply_expiry(x):
            return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                          (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
        
        #v_apply_expiry =  np.vectorize(apply_expiry)
        index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
        df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
        print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
        print("Temp DataFrame created successfully")
        print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
        
        return assay, ws, ports, total, rate_data, sub_to_ws, df
    
    #get_data()
    
    
    def __init__ (self, assay, ws, ports, total, rate_data, sub_to_ws, df):
        #self.get_data = assay, ws, ports, total, rate_data, sub_to_ws, df
        self.assay = assay
        self.ws = ws
        self.ports = ports
        self.total = total
        self.rate_data = rate_data
        self.sub_to_ws = sub_to_ws
        self.df = df 
import_data
import_data()
runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
class import_data:
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004 (1).xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete (1).xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))

runfile('C:/Users/mima/.spyder-py3/ArbEconsClasses.py', wdir='C:/Users/mima/.spyder-py3')
import_data.ws
import_data.expiry_table
expiry_table = pd.read_excel(data, 'expiry')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
import_data = import_data()
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
ArbEcons.arb
import_data()
import_data.assay
from ArbEcons import import_data
from ArbEcons import arb
import_data().assay
var = import_data()
var[1]
var.ws
arb('Azeri,','Housotn', *var)
arb('Azeri','Houston', *var)
var = import_data()
arb('Azeri','Houston', *var)
arb('Azeri','Houston', *var)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arb('Azeri','Houston', *var)
arb('Forties','Houston', *var)
var = import_data()
arb('Forties','Houston', *var).head()
ws_codes
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

def import_data():
    #crude = 'Forties'
    #destination = 'Houston'
    
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df




def arb(crude,destination,assay, ws, ports, total, rate_data, sub_to_ws, df):
    
    
    
    
    
    
    """
    create the flat rates table for the rates calculations and column creation
    """
    
    def calculate_flat_rate():
        
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        return flat_rate_table
    
    flat_rate_table = calculate_flat_rate()
    print("flat_rate_table created successfully")
    
    
    """
    THIS IS ESSENTIALLY A VLOOKUP
    
    inputs are the table to look up values from for each row of another table
    
    in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
    and return the value in the rate row.
    
    use the .iat function to return the inger values - this speeds up operations
    
    x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
    Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year
    
    """
    
    #x = '2017-12-06 00:00:00'
    #type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
    #flat_rate_table = 
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """ 
    Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
    """
    
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    #df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))
    
    print("df['Rate'] created successfully")
    
    
    """
    Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
    We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
    As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
    We then convert the result to a string and we set index = false as we dont want the header row returned
    Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
    This gives us the neccessary world scale codes 
    
    We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
    If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
    If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor
    
    Final values will be in $/bbl
    
    """
    
    def calculate_world_scale():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
        
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        
        ws_codes = ws[(ws['Origin'] == sub_region) &
                          (ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
        
        return df
    
    calculate_world_scale()
    print("calculate_world_scale() created successfully")
    
    #def pricing_adjustment():
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    
    
    
    
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if crude_vs in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2),
                                          diff)
        elif crude_vs in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df['diff_vs_wti'] = diff
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif crude_vs in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = diff
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if crude_vs in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        
        elif crude_vs in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    print(df.head())

    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    #v_apply_expiry =  np.vectorize(apply_expiry)
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



def arb(crude,destination,assay, ws, ports, total, rate_data, sub_to_ws, df):
    
    
    
    
    
    
    """
    create the flat rates table for the rates calculations and column creation
    """
    
    def calculate_flat_rate():
        
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        return flat_rate_table
    
    flat_rate_table = calculate_flat_rate()
    print("flat_rate_table created successfully")
    
    
    """
    THIS IS ESSENTIALLY A VLOOKUP
    
    inputs are the table to look up values from for each row of another table
    
    in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
    and return the value in the rate row.
    
    use the .iat function to return the inger values - this speeds up operations
    
    x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
    Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year
    
    """
    
    #x = '2017-12-06 00:00:00'
    #type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
    #flat_rate_table = 
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """ 
    Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
    """
    
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    #df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))
    
    print("df['Rate'] created successfully")
    
    
    """
    Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
    We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
    As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
    We then convert the result to a string and we set index = false as we dont want the header row returned
    Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
    This gives us the neccessary world scale codes 
    
    We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
    If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
    If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor
    
    Final values will be in $/bbl
    
    """
    
    def calculate_world_scale():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
        
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        
        ws_codes = ws[(ws['Origin'] == sub_region) &
                          (ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
        
        return df
    
    calculate_world_scale()
    print("calculate_world_scale() created successfully")
    
    #def pricing_adjustment():
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    
    
    
    
    indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
    #TEST PRICES
    def generate_price(row, minp, maxp):
        return row + random.uniform(minp,maxp)
    
    # will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4
    
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if crude_vs in index_wti:
            df['diff_vs_wti'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2),
                                          diff)
        elif crude_vs in index_dtd:
            df['diff_vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df['diff_vs_wti'] = diff
        return df
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df['diff_vs_dtd'] = np.select(conditions, choices)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif crude_vs in index_dub:
            pass
        
        else:
            df['diff_vs_dtd'] = diff
        
        return df['diff_vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if crude_vs in index_wti:
            df['diff_vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        
        elif crude_vs in index_dtd:
            df['diff_vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        
        elif crude_vs in index_dub:
            pass
        
        return df['diff_vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    print(df.head())

t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]





"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

#def pricing_adjustment():
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]





indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4


dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df['diff_vs_wti'] = diff
    return df


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = diff
    
    return df['diff_vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df['diff_vs_dub'] 


convert_dub()
print("convert_dub() created successfully")
print(df.head())
crude = 'Forties'
destination = 'Houston'




"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[name] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
print("calculate_world_scale() created successfully")

#def pricing_adjustment():
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]





indicies = {'index_wti': index_wti, 'index_dtd':index_dtd,'index_dub':index_dub}
#TEST PRICES
def generate_price(row, minp, maxp):
    return row + random.uniform(minp,maxp)


# will need something to determine which cfds to choose - for now thw ones to choose are going to be the 3/4


dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df['diff_vs_wti'] = diff
    return df


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = diff
    
    return df['diff_vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df['diff_vs_dub'] 


convert_dub()
print("convert_dub() created successfully")
print(df.head())
destination
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]+"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df[size] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df


calculate_world_scale()
ws_codes
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)

sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)

ws_codes = ws[(ws['Origin'] == sub_region) &
                  (ws['Destination'] == sub_region_2)]
ws_codes
destination
ws_codes['Destination']
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)

sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)

ws_codes = ws[(ws['Origin'] == sub_region) &
                  (ws['Destination'] == sub_region_2)]

for i in list(ws_codes['Code']):
    size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
    if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
        df[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
    else:
        df[size] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']

runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
var = import_data()
arb('Forties','Houston', *var).head()
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
df.drop(['Expiry'], axis = 1, inplace = True)
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]





"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))

return assay, ws, ports, total, rate_data, sub_to_ws, df
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]





"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arb('Azeri','Houston', *var).head()
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arb('Azeri','Houston', *var).head()
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]





"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]





"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df
crude = 'Forties'
destination = 'Houston'

#def pricing_adjustment():
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df['diff_vs_wti'] = diff
    return df


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = diff
    
    return df['diff_vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df['diff_vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column
"""
df.drop(['Expiry'], axis = 1)


"""
create the flat rates table for the rates calculations and column creation
"""

def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
df_freight = pd.DataFrame(index=df.index
df_freight = pd.DataFrame(index=df.index)
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))

#df['Rate'] = df.index.to_series().apply(lambda x: calculate_flat_rates(x))

print("df_freight['Rate'] created successfully")


"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df['Rate'] / assay[crude]['Conversion']
    
    return df_freight


calculate_world_scale()
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")

"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    
    return df_freight


calculate_world_scale()
df
df.drop(['Expiry'], axis = 1)
crude = 'Forties'
destination = 'Houston'

#def pricing_adjustment():
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df['diff_vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df['diff_vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df['diff_vs_wti'] = diff
    return df


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df['diff_vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df['diff_vs_dtd'] = diff
    
    return df['diff_vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df['diff_vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df['diff_vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df['diff_vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column and create dataframe to hold freight numbers
"""
df.drop(['Expiry'], axis = 1)
df_freight = pd.DataFrame(index=df.index)


"""
create the flat rates table for the rates calculations and column creation
"""
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")

"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    df_freight.drop(['Rate'], axis=1)     
    return df_freight


calculate_world_scale()
print("calculate_world_scale() created successfully")

print(df.head())
print(df_freight.head())
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    df_freight = df_freight.drop(['Rate'], axis=1)     
    return df_freight


calculate_world_scale()
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight

calculate_world_scale()
print(df.head())
print(df_freight.head())
def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight


calculate_world_scale()
print("calculate_world_scale() created successfully")

print(df.head())
print(df_freight.head())
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
df.columns
crude = 'Forties'
destination = 'Houston'

#def pricing_adjustment():
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df['vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df['vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df['vs_wti'] = diff
    return df


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df['vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df['vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df['vs_dtd'] = diff
    
    return df['vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df['vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df['vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df['vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column and create dataframe to hold freight numbers
"""
df.drop(['Expiry'], axis = 1)
df_freight = pd.DataFrame(index=df.index)


"""
create the flat rates table for the rates calculations and column creation
"""
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")

"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight


calculate_world_scale()
print("calculate_world_scale() created successfully")

df = df.drop(['Expiry'], axis=1)
df_freight = df_freight.drop(['Rate'], axis=1)

print(df.head())
print(df_freight.head())
df.columns
df
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]





"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))

return assay, ws, ports, total, rate_data, sub_to_ws, df

t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()



"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]





"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers



"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    


rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


#v_apply_expiry =  np.vectorize(apply_expiry)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
crude = 'Forties'
destination = 'Houston'

#def pricing_adjustment():

df_prices = pd.DataFrame(index=df.index)
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df_prices['vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df_prices['vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df_prices['vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df_prices['vs_dtd'] = diff
    
    return df_prices['vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df_prices['vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df_prices['vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column and create dataframe to hold freight numbers
"""
df_freight = pd.DataFrame(index=df.index)

"""
create the flat rates table for the rates calculations and column creation
"""
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")


"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

#x = '2017-12-06 00:00:00'
#type(dt.strptime(str(x), '%Y-%m-%d %H:%M:%S').year)
#flat_rate_table = 
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")

"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        size_landed = size + "_Landed"
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight


calculate_world_scale()
print("calculate_world_scale() created successfully")

df_freight = df_freight.drop(['Rate'], axis=1)

print(df_prices.head())
print(df_freight.head())
len(df_prices)
len(df_prices.columns)
len(df_freight.columns)
df_prices[1]
df_prices.iloc[1]
df_prices.iloc[:,1]
df_prices.columns
df_landed = pd.DataFrame(index=df.index)
for i in df_prices.columns:
    for k in df_freight.columns:
        name = k[:4]+"_"+i
        df_landed[name] = df_prices[i] + df_freight[k]

runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arb_values = arb('Azeri','Houston', *var).head()
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
arb_values = arb('Azeri','Houston', *var).head()
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons import import_data
from ArbEcons import arb

#from ArbEconsClasses import import_data
#from ArbEconsClasses import arb

#print(arbs)

var = import_data()
arb_values = arb('Azeri','Houston', *var).head()
destination
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers


"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    

rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df_prices = pd.DataFrame(index=df.index)
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df_prices['vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df_prices['vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df_prices['vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df_prices['vs_dtd'] = diff
    
    return df_prices['vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df_prices['vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df_prices['vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column and create dataframe to hold freight numbers
"""
df_freight = pd.DataFrame(index=df.index)

"""
create the flat rates table for the rates calculations and column creation
"""
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")

"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")

"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight


calculate_world_scale()
print("calculate_world_scale() created successfully")

df_freight = df_freight.drop(['Rate'], axis=1)



df_prices.iloc[:,1]

df_landed = pd.DataFrame(index=df.index)
for i in df_prices.columns:
    for k in df_freight.columns:
        name = k[:4]+"_"+i
        df_landed[name] = df_prices[i] + df_freight[k]



landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
crude = 'Forties'
destination = 'Houston'

df_prices = pd.DataFrame(index=df.index)
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df_prices['vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df_prices['vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df_prices['vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df_prices['vs_dtd'] = diff
    
    return df_prices['vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df_prices['vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df_prices['vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column and create dataframe to hold freight numbers
"""
df_freight = pd.DataFrame(index=df.index)

"""
create the flat rates table for the rates calculations and column creation
"""
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")

"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")

"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight


calculate_world_scale()
print("calculate_world_scale() created successfully")

df_freight = df_freight.drop(['Rate'], axis=1)



df_prices.iloc[:,1]

df_landed = pd.DataFrame(index=df.index)
for i in df_prices.columns:
    for k in df_freight.columns:
        name = k[:4]+"_"+i
        df_landed[name] = df_prices[i] + df_freight[k]



landed = pd.concat([df_prices,df_freight, df_landed], axis=1)

"""
Work out the landed values in 3rd dataframe then combine for the return
"""
landed
landed.columns
destination
crude
columns = pd.MultiIndex.from_product([[crude], [destination], [landed.columns]],
                                     names=['Crude','Destination','Freight'])
destination
columns = pd.MultiIndex.from_product([[crude], [destination], [landed.columns]],
                                     names=['Crude','Destination','Freight'])
landed.columns
columns = pd.MultiIndex.from_product([[crude], [destination], [list(landed.columns)]],
                                     names=['Crude','Destination','Freight'])
columns = pd.MultiIndex.from_product([[crude], [destination], list(landed.columns)],
                                     names=['Crude','Destination','Freight'])
columns
pd.DataFrame(landed, columns=columns)
landed
pd.DataFrame(landed, columns=columns)
pd.DataFrame(landed, index=df.index,columns=columns)
landed
df_landed = pd.DataFrame(index=df.index)
for i in df_prices.columns:
    for k in df_freight.columns:
        name = k[:4]+"_"+i
        df_landed[name] = df_prices[i] + df_freight[k]



landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
landed = pd.MultiIndex.from_product([[crude],[destination], landed.columns])
landed
print(landed.head())
df_landed = pd.DataFrame(index=df.index)
for i in df_prices.columns:
    for k in df_freight.columns:
        name = k[:4]+"_"+i
        df_landed[name] = df_prices[i] + df_freight[k]



landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
landed.columns = pd.MultiIndex.from_product([[crude],[destination], landed.columns])
print(landed.head())
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons import import_data
from ArbEcons import arb

#from ArbEconsClasses import import_data
#from ArbEconsClasses import arb

#print(arbs)

var = import_data()
arb_values = arb('Azeri','Houston', *var).head()
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons import import_data
from ArbEcons import arb

#from ArbEconsClasses import import_data
#from ArbEconsClasses import arb

#print(arbs)

var = import_data()
arb_values = arb('Azeri','Houston', *var).head()
from ArbEcons import import_data
from ArbEcons import arb

#from ArbEconsClasses import import_data
#from ArbEconsClasses import arb

#print(arbs)

var = import_data()
arb_values = arb('Azeri','Houston', *var)
arb_values = arb('Azeri','Houston', *var).head()
def make_arbs(crudes,destinations):
    
    for i in crudes:
        for k in destinations:
            counter = 0
            if counter = 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                pd.concat([arb_values,temp], axis=1)

def make_arbs(crudes,destinations):
    counter == 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                pd.concat([arb_values,temp], axis=1)
    return arb_values

crudes = ['Azeri','Forties']
destinations = ['Rotterdam','Houston']

def make_arbs(crudes,destinations):
    counter == 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                pd.concat([arb_values,temp], axis=1)
    return arb_values

make_arbs(crudes,destinations)        
def make_arbs(crudes,destinations):
    counter = 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                pd.concat([arb_values,temp], axis=1)
    return arb_values


make_arbs(crudes,destinations)   
from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
def make_arbs(crudes,destinations):
    counter = 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                pd.concat([arb_values,temp], axis=1)
    return arb_values


make_arbs(crudes,destinations)   
tester = make_arbs(crudes,destinations)    
tester.head()   
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
tester = make_arbs(crudes,destinations)    
tester.head() 
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd

#from ArbEconsClasses import import_data
#from ArbEconsClasses import arb

#print(arbs)

var = import_data()
#arb_values = arb('Azeri','Houston', *var).head()

crudes = ['Azeri','Forties','CPC Blend']
destinations = ['Rotterdam','Houston']

def make_arbs(crudes,destinations):
    counter = 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                arb_values = pd.concat([arb_values,temp], axis=1)
    return arb_values


tester = make_arbs(crudes,destinations)    
print(tester.head())   
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW Model V5.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
residue = {}
residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
imported_vgo = 0  
diluent_used = 0       
diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'volume_used': 0,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': diluent_used
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent['volume_used'] = res.x[0]
diluent['cst'] = res.fun

# =============================================================================
# Left blank for the addition of if vgo is needed to be imported
# Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
# 
# if target_viscosity(diluent_used) > 380:
#     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
#     
# =============================================================================

diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
runfile('C:/Users/mima/.spyder-py3/GPWEcons.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW Model V5.py', wdir='C:/Users/mima/.spyder-py3')
from GPW import lvn_gasolinepool
lvn_gasolinepool
runfile('C:/Users/mima/.spyder-py3/ArbEcons.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
var = import_data()
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input = cdu()
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
assay = raw_assay.to_dict('index')
crude = 'Forties'
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input


refinery_volume, reformer_input, kero_input, hvn_input, vgo_input = cdu()
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input


refinery_volume, reformer_input, kero_input, hvn_input, vgo_input = cdu(crude)
def reformer(reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


reformer_output = reformer()
fcc_output = fcc()
def fcc(fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


fcc_output = fcc()
def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

diluent = fo_blend()
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
final_yields = yields()
def yields():
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 
    
    return final_yields

final_yields = yields()
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
assay2 = raw_assay.to_dict('index')
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
final_yields = yields()
print(final_yields)
def main(crude, refinery_volume=None, lvn_gasolinepool=None, kero_gasolinepool=None, reformer_capacity=None, fcc_capacity=None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    final_yields = yields()
    print(final_yields)

def main(crude, refinery_volume=None, lvn_gasolinepool=None, kero_gasolinepool=None, reformer_capacity=None, fcc_capacity=None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    final_yields = yields()
    print(final_yields)

def cdu():
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer():
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc():
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def yields():
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 
    
    return final_yields

main('Amna')
ef cdu():
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
def cdu():
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool

main('Amna')
def main(crude, refinery_volume=None, lvn_gasolinepool=None, kero_gasolinepool=None, reformer_capacity=None, fcc_capacity=None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    final_yields = yields()
    print(final_yields)

main('Amna')
def main(crude, refinery_volume=None, lvn_gasolinepool=None, kero_gasolinepool=None, reformer_capacity=None, fcc_capacity=None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    final_yields = yields()
    print(final_yields)


def cdu():
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool

main('Amna')
def main(crude, refinery_volume=None, lvn_gasolinepool=None, kero_gasolinepool=None, reformer_capacity=None, fcc_capacity=None): 
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15                                      
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    final_yields = yields()
    print(final_yields)

main('Amna')
main('Amna')

def main(crude, refinery_volume=None, lvn_gasolinepool=None, kero_gasolinepool=None, reformer_capacity=None, fcc_capacity=None): 
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15                                      
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    final_yields = yields()
    print(final_yields)

def main(crude, refinery_volume=None, lvn_gasolinepool=None, kero_gasolinepool=None, reformer_capacity=None, fcc_capacity=None): 
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15                                      
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    final_yields = yields()
    print(final_yields)

main('Amna')
main()
main('Amna')
crude = 'Amna'

def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
reformer_output = reformer()
import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
#raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crude = 'Amna'
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer(reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
def fcc(fcc_capacity = None, refinery_volume):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output

def fcc(refinery_volume, vgo_input, fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output

import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
#raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crude = 'Amna'

def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(refinery_volume, vgo_input, fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend(refinery_volume, fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def yields(refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 
    
    return final_yields

import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
#raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crude = 'Amna'

def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(refinery_volume, vgo_input, fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend(refinery_volume, fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def yields(refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'] - 1) 
    
    return final_yields

def main():                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input)
    fcc_output = fcc(refinery_volume, vgo_input)
    diluent = fo_blend(refinery_volume, fcc_output)
    final_yields = yields(refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

main('Amna')
def main(crude):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input)
    fcc_output = fcc(refinery_volume, vgo_input)
    diluent = fo_blend(refinery_volume, fcc_output)
    final_yields = yields(refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

main('Amna')
assay
for i in assay:   
    main(i)

assay
key = list(assay.keys())[0]
assay.keys()
key = list(assay.keys())
for i in key:   
    main(i)

1-kero_gasolinepool
main('Amna')
main('Forties')
key
def yields(refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = 1- (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo']) 
    
    return final_yields

main('Amna')
main('Forties')
def main(crude):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input)
    fcc_output = fcc(refinery_volume, vgo_input)
    diluent = fo_blend(refinery_volume, fcc_output)
    final_yields = yields(refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

main('Amna')
main('Forties')
main('Amna')
main('Forties')
main('Azeri')
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15


hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
vgo_input = assay[crude]['VGO'] * refinery_volume
refinery_volume = 200
lvn_gasolinepool = 0.12
kero_gasolinepool = 0.15
hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
vgo_input = assay[crude]['VGO'] * refinery_volume
reformer_capacity = refinery_volume*0.21
reformer_output = {}
reformer_assay_standard = {
    'c3_c4': {'yield': 0.05},
    'h2': {'yield': 0.0},
    'btx': {'yield': 0.0},
    'gasoline': {'yield': 0.95}
    }
utilised_ref_cap = reformer_capacity * 0.97
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
fcc_capacity = refinery_volume*0.21
fcc_output = {}
fcc_assay_standard = {
    'c3_c4': {'yield': 0.02},
    'c3__': {'yield': 0.02},
    'lco': {'yield': 0.20},
    'clo': {'yield': 0.20},
    'gasoline': {'yield': 0.56}
    }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)

fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
residue = {}
residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
imported_vgo = 0  
diluent_used = 0       
diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': diluent_used
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    
    # create the 'cost' function - what do we want to minimise?
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent['volume_used'] = res.x[0]
diluent['cst'] = res.fun

# =============================================================================
# Left blank for the addition of if vgo is needed to be imported
# Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
# 
# if target_viscosity(diluent_used) > 380:
#     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
#     
# =============================================================================

diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
final_yields = {}

final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
final_yields['f_l'] = 1- (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo'])
assay[crude]['LGO']
(assay[crude]['LGO'] + assay[crude]['HGO'])
refinery_volume
crude = 'Azeri'
(assay[crude]['LGO'] + assay[crude]['HGO'])
((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
final_yields['ulsd']
final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
final_yields['ulsd']
final_yields = yields()
main('Amna')
main('Forties')
main('Azeri')
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(refinery_volume, vgo_input, fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend(crude, refinery_volume, fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = 1- (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo']) 
    
    return final_yields

import numpy as np 
import pandas as pd
from scipy.optimize import minimize

# Get the Data
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
#raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crude = 'Amna'

def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(refinery_volume, vgo_input, fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend(crude, refinery_volume, fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    final_yields = {}
    
    final_yields['lpg'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l'] = 1- (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo']) 
    
    return final_yields

def main(crude):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input)
    fcc_output = fcc(refinery_volume, vgo_input)
    diluent = fo_blend(refinery_volume, fcc_output)
    final_yields = yields(refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

main('Amna')
main('Forties')
main('Azeri')
def main(crude):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input)
    fcc_output = fcc(refinery_volume, vgo_input)
    diluent = fo_blend(crude, refinery_volume, fcc_output)
    final_yields = yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

main('Amna')
main('Forties')
main('Azeri')
for i in key:   
    main(i)

for i in list(assay.keys()):
    main(i)

def main(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None, reformer_capacity = None, fcc_capacity = None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity)
    fcc_output = fcc(refinery_volume, vgo_input, fcc_capacity)
    diluent = fo_blend(crude, refinery_volume, fcc_output)
    final_yields = yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

for i in list(assay.keys()):
    main(i)

for i in list(assay.keys()):
    main(i, kero_gasolinepool=0.8)

for i in list(assay.keys()):
    main(i, kero_gasolinepool=0.0)

check = ['Forties', 'Azeri', 'Amna']
for i in check:
    main(i, kero_gasolinepool=0.0)

for i in check:
    main(i, kero_gasolinepool=1)

check = ['Forties', 'Azeri', 'Amna','Mellitah']
for i in check:
    main(i, kero_gasolinepool=1)

for i in check:
    main(i)

from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
from GPW import main
for i in crudes:
    main(i)

crudes = ['Azeri','Forties','CPC Blend','Brent']
for i in crudes:
    main(i)

from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
from GPW import main
var = import_data()
#arb_values = arb('Azeri','Houston', *var).head()

crudes = ['Azeri','Forties','CPC Blend','Brent']
destinations = ['Rotterdam','Houston']

def make_arbs(crudes,destinations):
    counter = 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                arb_values = pd.concat([arb_values,temp], axis=1)
    return arb_values

tester = make_arbs(crudes,destinations) 
for i in crudes:
    main(i)

runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
df = pd.DataFrame({
    'a': [1,2,3,4,5],
    'b': [5,4,3,3,4],
    'c': [3,2,4,3,10],
    'd': [3, 2, 1, 1, 1]
})
import pandas as pd
df = pd.DataFrame({
    'a': [1,2,3,4,5],
    'b': [5,4,3,3,4],
    'c': [3,2,4,3,10],
    'd': [3, 2, 1, 1, 1]
})

params = {'a': 2.5, 'b': 3.0, 'c': 1.3, 'd': 0.9}

df1 = df.assign(**params).mul(df).sum(1)
print (df1)
import pandas as pd
df = pd.DataFrame({
    'a': [1,2,3,4,5],
    'b': [5,4,3,3,4],
    'c': [3,2,4,3,10],
    'd': [3, 2, 1, 1, 1]
})

params = {'a': 2.5, 'b': 3.0, 'c': 1.3, 'd': 0.9}
df.assign(**params)
df.assign(**params).mul(df)
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
main(*args)
main(*arg)
main(crude)
def yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    final_yields = {}
    
    final_yields['lpg']['yield'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha']['yield'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx']['yield'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline']['yield'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero']['yield'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd']['yield'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil']['yield'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo']['yield'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l']['yield'] = 1- (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo']) 
    
    return final_yields

main(crude)
main(crude)
final_yields['lpg']['yield']
def main(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None, reformer_capacity = None, fcc_capacity = None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity)
    fcc_output = fcc(refinery_volume, vgo_input, fcc_capacity)
    diluent = fo_blend(crude, refinery_volume, fcc_output)
    final_yields = yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

main(crude)
final_yields = {}
final_yields['lpg']['yield'] = 2    
from collections import defaultdict
final_yields['lpg']['yield'] = 2 
final_yields = {}
final_yields['lpg']['yield'] = 2  
final_yields = colletcions.defaultdict(dict)
final_yields = collections.defaultdict(dict)
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
import collections
final_yields['lpg']['yield'] = 2
final_yields = {{}}
final_yields = {}{}
inal_yields['lpg']['yield']
final_yields['lpg']['yield']
from collections import defaultdict
assay[crude]['LPG']
mydict = lambda: defaultdict(mydict)
final_yields = mydict()
def yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    mydict = lambda: defaultdict(mydict)
    final_yields = mydict()
    
    
    final_yields['lpg']['yield'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha']['yield'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx']['yield'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline']['yield'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero']['yield'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd']['yield'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil']['yield'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo']['yield'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l']['yield'] = 1- (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo']) 
    
    return final_yields

runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from collections import defaultdict

# Get the Data
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
#raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')

# with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
crude = 'Amna'

def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3_c4': {'yield': 0.05},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['butane'] = reformer_assay_standard['c3_c4']['yield'] * reformer_volume / 2
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(refinery_volume, vgo_input, fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3_c4': {'yield': 0.02},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume / 2
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend(crude, refinery_volume, fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    mydict = lambda: defaultdict(mydict)
    final_yields = mydict()
    
    
    final_yields['lpg']['yield'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields['naphtha']['yield'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields['btx']['yield'] = (reformer_output['btx'])/ refinery_volume
    final_yields['gasoline']['yield'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields['kero']['yield'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields['ulsd']['yield'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields['gasoil']['yield'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields['lsfo']['yield'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    final_yields['f_l']['yield'] = 1- (final_yields['lpg'] + final_yields['naphtha'] + final_yields['btx'] + final_yields['gasoline'] + final_yields['kero']+ final_yields['ulsd'] + final_yields['gasoil'] + final_yields['lsfo']) 
    
    return final_yields


#def generate_margin():






def main(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None, reformer_capacity = None, fcc_capacity = None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity)
    fcc_output = fcc(refinery_volume, vgo_input, fcc_capacity)
    diluent = fo_blend(crude, refinery_volume, fcc_output)
    final_yields = yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    print(final_yields)

from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
from GPW import main
for i in crudes:
    main(i)

crudes = ['Azeri','Forties','CPC Blend','Brent']
destinations = ['Rotterdam','Houston']
for i in crudes:
    main(i)

final_yields = defaultdict(dict)
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)

final_yields = defaultdict(dict)
final_yields['lpg']['yield'] = 4
final_yields = defaultdict(dict)
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)

runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)

runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)

runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)

final_yields[crude] = defaultdict(dict)
crude
def yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    final_yields[crude] = defaultdict(dict)
    #final_yields[crude]
    #final_yields['lpg']['yield'] = 4
    
    
    final_yields[crude]['lpg']['yield'] = (assay[crude]['LPG'] * refinery_volume + reformer_output['propane'] + reformer_output['butane'] + fcc_output['propane'] + fcc_output['butane'] + fcc_output['c3__']) / refinery_volume
    final_yields[crude]['naphtha']['yield'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool) + reformer_output['surplus_for_naphtha']) / refinery_volume
    final_yields[crude]['btx']['yield'] = (reformer_output['btx'])/ refinery_volume
    final_yields[crude]['gasoline']['yield'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    final_yields[crude]['kero']['yield'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    final_yields[crude]['ulsd']['yield'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    final_yields[crude]['gasoil']['yield'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    final_yields[crude]['lsfo']['yield'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    final_yields[crude]['f_l']['yield'] = 1- (final_yields[crude]['lpg']['yield'] +
                                                final_yields[crude]['naphtha']['yield'] +
                                                final_yields[crude]['btx']['yield'] +
                                                final_yields[crude]['gasoline']['yield'] +
                                                final_yields[crude]['kero']['yield'] +
                                                final_yields[crude]['ulsd']['yield'] +
                                                final_yields[crude]['gasoil']['yield'] +
                                                final_yields[crude]['lsfo']['yield']) 
    
    return final_yields

runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)

crudes = ['Amna','Azeri','Forties','CPC Blend','Brent']
for i in crudes:
    main(i)

for i in crudes:
    main(i)
    print(final_yields)

final_yields  
final_yields['Forties']  
x = final_yields['Forties']['kero']['yield']    
inal_yields[crude] = defaultdict(dict)
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)
    print(final_yields)

crudes = ['Amna','Azeri','Forties','CPC Blend','Brent']
from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
from GPW import main
for i in crudes:
    main(i)
    print(final_yields)

runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    main(i)
    print(final_yields)

for i in crudes:
    main(i)

for i in crudes:
    final_yields = main(i)
    print(final_yields)

final_yields
yield_dict = {}

for i in crudes:
    final_yields = main(i)
    yield_dict.update(final_yields)

x = yield_dict['Forties']['kero']['yield']     
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    yields = main(i)
    yields_dict.update(yields)


x = yield_dict['Forties']['kero']['yield']   
yield_dict = {}

for i in crudes:
    yields = main(i)
    yields_dict.update(yields)


x = yield_dict['Forties']['kero']['yield'] 
yields_dict = {}

for i in crudes:
    yields = main(i)
    yields_dict.update(yields)


x = yields_dict['Forties']['kero']['yield']    
sub_to_ws
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers


"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    

rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers


"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    

rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
          (rate_data['DischargePort'] == destination)]
crude = 'Forties'
destination = 'Houston'
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")
df_freight = pd.DataFrame(index=df.index)
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region
sub_to_ws[1]
sub_to_ws
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
sub_to_ws
sub_to_ws[1]
sub_region
sub_to_ws[1]
sub_region.map(sub_to_ws[1])
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
prices = pd.read_excel(data, 'prices', header = 1)
yields_dict
yields_dict.values() 
yields_dict.values().values() 
yields = {}
yields[crude] = defaultdict(dict)

yields[crude]['lpg']['yield'] = (assay[crude]['LPG'] * refinery_volume 
                                       + reformer_output['propane'] 
                                       + reformer_output['butane'] 
                                       + fcc_output['propane'] 
                                       + fcc_output['butane']
                                       + fcc_output['c3__']
                                       ) / refinery_volume

yields[crude]['naphtha']['yield'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                                           + reformer_output['surplus_for_naphtha']
                                           ) / refinery_volume

yields[crude]['btx']['yield'] = (reformer_output['btx']
                                        ) / refinery_volume

yields[crude]['gasoline']['yield'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
yields[crude]['kero']['yield'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields[crude]['ulsd']['yield'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
yields[crude]['gasoil']['yield'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields[crude]['lsfo']['yield'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume

yields[crude]['f_l']['yield'] = 1- (yields[crude]['lpg']['yield'] +
                                            yields[crude]['naphtha']['yield'] +
                                            yields[crude]['btx']['yield'] +
                                            yields[crude]['gasoline']['yield'] +
                                            yields[crude]['kero']['yield'] +
                                            yields[crude]['ulsd']['yield'] +
                                            yields[crude]['gasoil']['yield'] +
                                            yields[crude]['lsfo']['yield']) 
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from collections import defaultdict

# Get the Data
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15


hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
vgo_input = assay[crude]['VGO'] * refinery_volume
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15


hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input
vgo_input = assay[crude]['VGO'] * refinery_volume
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool

cdu()
cdu(crude)
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
refinery_volume = None
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None, reformer_capacity = None, fcc_capacity = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
yields = {}
yields[crude] = defaultdict(dict)

yields[crude]['lpg']['yield'] = (assay[crude]['LPG'] * refinery_volume 
                                       + reformer_output['propane'] 
                                       + reformer_output['butane'] 
                                       + fcc_output['propane'] 
                                       + fcc_output['butane']
                                       + fcc_output['c3__']
                                       ) / refinery_volume

yields[crude]['naphtha']['yield'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                                           + reformer_output['surplus_for_naphtha']
                                           ) / refinery_volume

yields[crude]['btx']['yield'] = (reformer_output['btx']
                                        ) / refinery_volume

yields[crude]['gasoline']['yield'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
yields[crude]['kero']['yield'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields[crude]['ulsd']['yield'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
yields[crude]['gasoil']['yield'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields[crude]['lsfo']['yield'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume

yields[crude]['f_l']['yield'] = 1- (yields[crude]['lpg']['yield'] +
                                            yields[crude]['naphtha']['yield'] +
                                            yields[crude]['btx']['yield'] +
                                            yields[crude]['gasoline']['yield'] +
                                            yields[crude]['kero']['yield'] +
                                            yields[crude]['ulsd']['yield'] +
                                            yields[crude]['gasoil']['yield'] +
                                            yields[crude]['lsfo']['yield']) 
reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity)
fcc_output = fcc(refinery_volume, vgo_input, fcc_capacity)
diluent = fo_blend(crude, refinery_volume, fcc_output)
yields = {}
yields[crude] = defaultdict(dict)

yields[crude]['lpg']['yield'] = (assay[crude]['LPG'] * refinery_volume 
                                       + reformer_output['propane'] 
                                       + reformer_output['butane'] 
                                       + fcc_output['propane'] 
                                       + fcc_output['butane']
                                       + fcc_output['c3__']
                                       ) / refinery_volume

yields[crude]['naphtha']['yield'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                                           + reformer_output['surplus_for_naphtha']
                                           ) / refinery_volume

yields[crude]['btx']['yield'] = (reformer_output['btx']
                                        ) / refinery_volume

yields[crude]['gasoline']['yield'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
yields[crude]['kero']['yield'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields[crude]['ulsd']['yield'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
yields[crude]['gasoil']['yield'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields[crude]['lsfo']['yield'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume

yields[crude]['f_l']['yield'] = 1- (yields[crude]['lpg']['yield'] +
                                            yields[crude]['naphtha']['yield'] +
                                            yields[crude]['btx']['yield'] +
                                            yields[crude]['gasoline']['yield'] +
                                            yields[crude]['kero']['yield'] +
                                            yields[crude]['ulsd']['yield'] +
                                            yields[crude]['gasoil']['yield'] +
                                            yields[crude]['lsfo']['yield']) 
yields
yields.get('yield')
m = yields.get('yield')
m
print(yields.get('yield'))
m = print(yields.get('lpg'))
yields[1]
yields
yields[crude].
yields[crude]
yields[crude].get('lpg')
total
defaultdict(dict)
sub_to_ws
sub_region
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers


"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    

rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
sub_to_ws[3]
sub_region.map(sub_to_ws[3]).to_string(index = False)
sub_region = ports[ports['Name'] == destination]['Subregion']
sub_to_ws[3]
sub_region
    yields = {}
# =============================================================================
#     yields[crude] = defaultdict(dict)
#     
#     pricing_centres = ['ROTTERDAM', 'AUGUSTA', 'NYH', ' HOUSTON', 'SINGAPORE']
#     products_list = ['propane', ' butane', 'naphtha','btx','gasoline','kero','ulsd','gasoil','lsfo']
#     
#     '''This is how we will determine which pricing centre to use'''
#     sub_region = ports[ports['Name'] == destination]['Subregion']
#     sub_region = sub_region.map(sub_to_ws[3]).to_string(index = False)
#     
#     ''' this is to create the look up neccessary for generating the table'''
#     
#     for product in product_list:
#         for centre in pricing_centres:
#             yields[crude][product][centre] = product_reference[centre][product][code]
# =============================================================================
    
    
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                           + reformer_output['propane'] 
                           + fcc_output['propane'] 
                           + fcc_output['c3__'] * 0.5
                           ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                       + reformer_output['butane'] 
                       + fcc_output['butane']
                       + fcc_output['c3__'] * 0.5
                       ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                           + reformer_output['surplus_for_naphtha']
                           ) / refinery_volume
    
    yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    yields['f_l'] = 1- (yields['lpg']
                        + yields['naphtha']
                        + yields['btx'] 
                        + yields['gasoline'] 
                        + yields['kero'] 
                        + yields['ulsd'] 
                        + yields['gasoil'] 
                        + yields['lsfo']) 
    yields = {}
# =============================================================================
#     yields[crude] = defaultdict(dict)
#     
#     pricing_centres = ['ROTTERDAM', 'AUGUSTA', 'NYH', ' HOUSTON', 'SINGAPORE']
#     products_list = ['propane', ' butane', 'naphtha','btx','gasoline','kero','ulsd','gasoil','lsfo']
#     
#     '''This is how we will determine which pricing centre to use'''
#     sub_region = ports[ports['Name'] == destination]['Subregion']
#     sub_region = sub_region.map(sub_to_ws[3]).to_string(index = False)
#     
#     ''' this is to create the look up neccessary for generating the table'''
#     
#     for product in product_list:
#         for centre in pricing_centres:
#             yields[crude][product][centre] = product_reference[centre][product][code]
# =============================================================================
    
    
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                           + reformer_output['propane'] 
                           + fcc_output['propane'] 
                           + fcc_output['c3__'] * 0.5
                           ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                       + reformer_output['butane'] 
                       + fcc_output['butane']
                       + fcc_output['c3__'] * 0.5
                       ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                           + reformer_output['surplus_for_naphtha']
                           ) / refinery_volume
    
    yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    yields['f_l'] = 1- (yields['propane']
                        + yields['butane']
                        + yields['naphtha']
                        + yields['btx'] 
                        + yields['gasoline'] 
                        + yields['kero'] 
                        + yields['ulsd'] 
                        + yields['gasoil'] 
                        + yields['lsfo']) 
d = [{'type_id': 6, 'type_name': 'Type 1'}, {'type_id': 12, 'type_name': 'Type 2'}]
print([{'type':x['type_id'],'name':x['type_name']} for x in d])
[{'type':x['type_id'],'name':x['type_name']} for x in d]
x
d
d[1]
'type':d[1]['type_id']
{'type':d[1]['type_id']}
d[1]
d[1]['type_id']
yields
ports[ports['Name'] == destination]['Subregion']
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
yields
yields.items()
for i, j in yields.items():
    print(i)

yields = {}  

yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                       + reformer_output['propane'] 
                       + fcc_output['propane'] 
                       + fcc_output['c3__'] * 0.5
                       ) / refinery_volume

yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                   + reformer_output['butane'] 
                   + fcc_output['butane']
                   + fcc_output['c3__'] * 0.5
                   ) / refinery_volume

yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                       + reformer_output['surplus_for_naphtha']
                       ) / refinery_volume

#yields['btx'] = (reformer_output['btx']) / refinery_volume

yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume

yields['f_l'] = 1- (yields['c']
                    + yields['butane']
                    + yields['naphtha']
                    #+ yields['btx'] 
                    + yields['gasoline'] 
                    + yields['kero'] 
                    + yields['ulsd'] 
                    + yields['gasoil'] 
                    + yields['lsfo']) 
yields = {}  

yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                       + reformer_output['propane'] 
                       + fcc_output['propane'] 
                       + fcc_output['c3__'] * 0.5
                       ) / refinery_volume

yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                   + reformer_output['butane'] 
                   + fcc_output['butane']
                   + fcc_output['c3__'] * 0.5
                   ) / refinery_volume

yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                       + reformer_output['surplus_for_naphtha']
                       ) / refinery_volume

#yields['btx'] = (reformer_output['btx']) / refinery_volume

yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume

yields['f_l'] = 1- (yields['propane']
                    + yields['butane']
                    + yields['naphtha']
                    #+ yields['btx'] 
                    + yields['gasoline'] 
                    + yields['kero'] 
                    + yields['ulsd'] 
                    + yields['gasoil'] 
                    + yields['lsfo'])
yields
foo = {'ROTTERDAM':{'propane':'abc',
                  'butane':'def',
                  'naphtha':'efg',
                  'gasoline':'kli',
                  'kero':'jkl',
                  'ulsd':'ytr'
                  'gasoil':'ahy',
                  'lsfo':'iko'}}
foo = {'ROTTERDAM':{'propane':'abc',
                  'butane':'def',
                  'naphtha':'efg',
                  'gasoline':'kli',
                  'kero':'jkl',
                  'ulsd':'ytr',
                  'gasoil':'ahy',
                  'lsfo':'iko'}}
foo = {'HOUSTON':{'propane':'abc',
                  'butane':'def',
                  'naphtha':'efg',
                  'gasoline':'kli',
                  'kero':'jkl',
                  'ulsd':'ytr',
                  'gasoil':'ahy',
                  'lsfo':'iko'}}

pricing centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
pricing centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
yields
yields.items()
i
yields
yields.items()
for i, j in yields.items():
    #{foo[pricing_centre][i]: j}
    
    print(i)

for i, j in yields.items():
    print({foo[pricing_centre][i]: j})

pricing_centre
foo[pricing_centre]
pricing_centre
foo
foo = {'HOUSTON':{'propane':'abc',
                  'butane':'def',
                  'naphtha':'efg',
                  'gasoline':'kli',
                  'kero':'jkl',
                  'ulsd':'ytr',
                  'gasoil':'ahy',
                  'lsfo':'iko'}}
yields.items()
for i, j in yields.items():
    print({foo[pricing_centre][i]: j})

yields.keys()    
list(yield_codes.keys())
yield_codes = {}
for i, j in yields.items():
    yield_codes.append({foo[pricing_centre][i]: j})

yield_codes = {}
for i, j in yields.items():
    yield_codes[foo[pricing_centre][i]] = j

yield_codes
list(yield_codes.keys())
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
prices = pd.read_excel(data, 'prices', header = 1)
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]
total[list(yield_codes.keys())]
total
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from collections import defaultdict
from datetime import datetime as dt
from ArbEcons import import_data

# =============================================================================
# # Get the Data
# data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
# assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
# #raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')
# 
# # with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
# crude = 'Amna'
# 
# 
# 
# paper_prices = pd.read_excel(data, 'paper prices', header = 1)
# prices = pd.read_excel(data, 'prices', header = 1)
# total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
# total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
# total = total.iloc[total.index > dt(2015,12,31)]
# =============================================================================

assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
total
price_codes = {'HOUSTON':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'}}

pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j

    yields = {}  
    
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                           + reformer_output['propane'] 
                           + fcc_output['propane'] 
                           + fcc_output['c3__'] * 0.5
                           ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                       + reformer_output['butane'] 
                       + fcc_output['butane']
                       + fcc_output['c3__'] * 0.5
                       ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                           + reformer_output['surplus_for_naphtha']
                           ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume

# =============================================================================
#     yields['f_l'] = 1- (yields['propane']
#                         + yields['butane']
#                         + yields['naphtha']
#                         #+ yields['btx'] 
#                         + yields['gasoline'] 
#                         + yields['kero'] 
#                         + yields['ulsd'] 
#                         + yields['gasoil'] 
#                         + yields['lsfo']) 
# =============================================================================
price_codes = {'HOUSTON':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'}}

pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)


yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j


regional_price_set = total[list(yield_codes.keys())] 
regional_price_set
gpw = regional_price_set(**yields).mul(regional_price_set).sum(1)
regional_price_set
yields
yield_codes
gpw = regional_price_set(**yield_codes).mul(regional_price_set).sum(1)
gpw = regional_price_set(yield_codes).mul(regional_price_set).sum(1)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
total
regional_price_set
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1).fillna(method='ffill')
regional_price_set
regional_price_set.fillna(method='ffill')
regional_price_set.fillna(method='bfill')
gpw = regional_price_set.fillna(method='ffill').assign(**yield_codes).mul(regional_price_set).sum(1)
regional_price_set.fillna(method='ffill')
import pandas as pd
df = pd.DataFrame({
    'a': [1,2,3,4,5],
    'b': [5,4,3,3,4],
    'c': [3,2,4,3,10],
    'd': [3, 2, 1, 1, 1]
})

params = {'a': 2.5, 'b': 3.0, 'c': 1.3, 'd': 0.9}

df1 = df.assign(**params).mul(df).sum(1)
print (df1)
regional_price_set.fillna(method='ffill').assign(**yield_codes)
regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill')  
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill') 
regional_price_set
regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
regional_price_set
regional_price_set.assign(**yield_codes).mul(regional_price_set)
regional_price_set.assign(**yield_codes)
regional_price_set
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set, 1).sum(1)
mul(regional_price_set, 1)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set.values).sum(1)
regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill').astype(float) 
total
total[list(yield_codes.keys())].fillna(method='ffill')
regional_price_set.describe
regional_price_set.describe()
regional_price_set.isnull().values.any()
regional_price_set.isnull().sum().sum()
regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill').astype(float) 
regional_price_set
regional_price_set.assign(**yield_codes)
regional_price_set
total[list(yield_codes.keys())].fillna(method='ffill').astype(float) 
total[list(yield_codes.keys())].fillna(method='ffill').astype(int) 
regional_price_set = total[list(yield_codes.keys())].
total[list(yield_codes.keys())].fillna(method='ffill')
float(total[list(yield_codes.keys())].fillna(method='ffill'))
total[list(yield_codes.keys())].astype(float)
total[list(yield_codes.keys())].fillna(method='ffill') 
total[list(yield_codes.keys())].fillna(method='ffill').astype(float) 
total[list(yield_codes.keys())].fillna(method='ffill')
regional_price_set
regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill')
regional_price_set.assign(**yield_codes)
regional_price_set.values
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set.values).sum(1)
regional_price_set.values.assign(**yield_codes)
gpw = regional_price_set.assign(**yield_codes).values.mul(regional_price_set.values).sum(1)
regional_price_set.values
regional_price_set.assign(**yield_codes)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
df = pd.DataFrame({
    'a': [1,2,3,4,5],
    'b': [5,4,3,3,4],
    'c': [3,2,4,3,10],
    'd': [3, 2, 1, 1, 1]
})
params = {'a': 2.5, 'b': 3.0, 'c': 1.3, 'd': 0.9}
df1 = df.assign(**params).mul(df).sum(1)
print (df1)
df.assign(**params)
df.assign(**params).mul(df)
price_codes = {'HOUSTON':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'}}

pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)


yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j


regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill')
regional_price_set
regional_price_set.describe(include='all')
regional_price_set
regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
regional_price_set
yield_codes
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = int(j)


regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill')
regional_price_set.describe(include='all')
regional_price_set
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = float(j)


regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill')
regional_price_set.describe(include='all')
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = float(j)


regional_price_set = total[list(yield_codes.keys())].fillna(method='ffill')
regional_price_set.describe(include='all')
print(total['PCALB00'])
total[list(yield_codes.keys())].fillna(method='bfill')
regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
regional_price_set.describe(include='all')
regional_price_set = pd.to_numeric(total[list(yield_codes.keys())].fillna(method='bfill'))
print(total['PCALB00'])
total['PCALB00']
total['PCALB00'].describe
total['PCALB00'].fillna(method='bfill').describe
total['PCALB00'].fillna(method='ffill').describe
total['PCALB00'].fillna(method='ffill').describe()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
price_codes = {'HOUSTON':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'}}

pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)


yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = float(j)
    
    print(total['PCALB00'].fillna(method='ffill').describe())


regional_price_set = pd.to_numeric(total[list(yield_codes.keys())].fillna(method='bfill'))
regional_price_set.describe(include='all')
regional_price_set.isnull().values.any()
regional_price_set.isnull().sum().sum()
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from collections import defaultdict
from datetime import datetime as dt
from ArbEcons import import_data

# =============================================================================
# # Get the Data
# data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
# assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
# #raw_assay = pd.read_excel('L://TRADING//ANALYSIS//GLOBAL//Arb Models//GPW model RA.xlsm', sheetname = 'Upload_Test', header = 0, index_col = 'Database_Name')
# 
# # with the crude name set as the index, we use pd.to_dict to set a dictioanry of dictionaries
# crude = 'Amna'
# 
# 
# 
# paper_prices = pd.read_excel(data, 'paper prices', header = 1)
# prices = pd.read_excel(data, 'prices', header = 1)
# total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
# total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
# total = total.iloc[total.index > dt(2015,12,31)]
# =============================================================================

assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
def cdu(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None):
    ### INPUT CRUDE ###
    #crude = 'Forties'
    #refinery_volume = 200
    
    ### Fractions for gaosline pool
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity = None):
    
    #reformer_capacity = 42
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc(refinery_volume, vgo_input, fcc_capacity = None):
    #fcc_capacity = 48
    if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21
    fcc_output = {}
    fcc_assay_standard = {
        'c3': {'yield': 0.1},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay_standard['c3_c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def fo_blend(crude, refinery_volume, fcc_output):
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        
        # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def calculate_yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent):
    
    yields = {}  
    
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                           + reformer_output['propane'] 
                           + fcc_output['propane'] 
                           + fcc_output['c3__'] * 0.5
                           ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                       + reformer_output['butane'] 
                       + fcc_output['butane']
                       + fcc_output['c3__'] * 0.5
                       ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                           + reformer_output['surplus_for_naphtha']
                           ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume


# =============================================================================
#     yields['f_l'] = 1- (yields['propane']
#                         + yields['butane']
#                         + yields['naphtha']
#                         #+ yields['btx'] 
#                         + yields['gasoline'] 
#                         + yields['kero'] 
#                         + yields['ulsd'] 
#                         + yields['gasoil'] 
#                         + yields['lsfo']) 
# =============================================================================
    
    return yields
destination
crude
crude = 'Forties'
destination = 'HOUSTON'
def calculate_margin(yields, destination, ports, sub_to_ws):
    
    ''' this is to create the look up neccessary for generating the table'''
    
    price_codes = {'HOUSTON':{'propane':'PCALB00',
                      'butane':'ASGCB00',
                      'naphtha':'AASAV00',
                      'gasoline':'AAGXH00',
                      'kero':'AAWEY00',
                      'ulsd':'AASAT00',
                      'gasoil':'AAUFL00',
                      'lsfo':'AAHPM00'}}
    
    pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]] = float(j)
    
    regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
    gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
    return gpw

runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
main(crude)
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
main(crude)
yields
yields = calculate_yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None, reformer_capacity = None, fcc_capacity = None
refinery_volume = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity)
fcc_output = fcc(refinery_volume, vgo_input, fcc_capacity)
diluent = fo_blend(crude, refinery_volume, fcc_output)
yields = calculate_yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
gpw = calculate_margin(yields, destination, ports, sub_to_ws)
def calculate_margin(yields, destination, ports, sub_to_ws):
    
    ''' this is to create the look up neccessary for generating the table'''
    
    price_codes = {'HOUSTON':{'propane':'PCALB00',
                      'butane':'ASGCB00',
                      'naphtha':'AASAV00',
                      'gasoline':'AAGXH00',
                      'kero':'AAWEY00',
                      'ulsd':'AASAT00',
                      'gasoil':'AAUFL00',
                      'lsfo':'AAHPM00'}}
    
    pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]] = j
    
    regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
    gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
    return gpw

gpw = calculate_margin(yields, destination, ports, sub_to_ws)
yield_codes
price_codes = {'HOUSTON':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'}}
pricing_centre = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
yield_codes = {}
for i, j in yields.items():

for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j

yields
price_codes
pricing_centre
ports
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
sub_to_ws[3]
destination
ports['Name'] == destination
destination.lower()
pricing_centre = ports[ports['Name'] == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
pricing_centre
orts['Name'] == destination.lower()
ports[ports['Name'] == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
ports[ports['Name'] == destination.lower()]
ports
ports['Name'].lower() == destination.lower()
str(ports['Name']).lower() 
str(
ports['Name'].lower()
ports['Name'].str.lower
ports['Name'].str.lower()
ports['Name'].str.lower() == destination.lower()
pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
pricing_centre
price_codes = {'HOUSTON':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'}}

pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   

yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j


regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
main(crude)
def calculate_margin(yields, destination, ports, sub_to_ws):
    
    ''' this is to create the look up neccessary for generating the table'''
    
    price_codes = {'HOUSTON':{'propane':'PCALB00',
                      'butane':'ASGCB00',
                      'naphtha':'AASAV00',
                      'gasoline':'AAGXH00',
                      'kero':'AAWEY00',
                      'ulsd':'AASAT00',
                      'gasoil':'AAUFL00',
                      'lsfo':'AAHPM00'}}
    
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]] = j
    
    regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
    gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
    return gpw

def main(crude, refinery_volume = None, lvn_gasolinepool = None, kero_gasolinepool = None, reformer_capacity = None, fcc_capacity = None):                                       
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude, refinery_volume, lvn_gasolinepool, kero_gasolinepool)
    reformer_output = reformer(refinery_volume, reformer_input, kero_input, hvn_input, reformer_capacity)
    fcc_output = fcc(refinery_volume, vgo_input, fcc_capacity)
    diluent = fo_blend(crude, refinery_volume, fcc_output)
    yields = calculate_yields(crude, refinery_volume, reformer_output, fcc_output, lvn_gasolinepool, kero_gasolinepool, diluent)
    gpw = calculate_margin(yields, destination, ports, sub_to_ws)
    return gpw

main(crude)
def make_arbs(crudes,destinations):
    counter = 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *var)
                counter +=1
            else:
                temp = arb(i,k, *var)
                arb_values = pd.concat([arb_values,temp], axis=1)
    return arb_values

tester = make_arbs('Amna','Houston')
from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
from GPW import main
tester = make_arbs('Amna','Houston')  
from ArbEcons import import_data
from ArbEcons import arb
import pandas as pd
from GPW import main

# =============================================================================
# from GPW import lvn_gasolinepool
# 
# lvn_gasolinepool
# =============================================================================

#from ArbEconsClasses import import_data
#from ArbEconsClasses import arb

#print(arbs)

var = import_data()
tester = make_arbs('Amna','Houston') 
tester = make_arbs(['Amna'],['Houston']) 
tester = make_arbs(['Forties'],['Houston'])    
tester = make_arbs(['Forties'],['Houston'])    
print(tester.head())  
(list(df.index.year)
list(df.index.year)
tester
tester['Forties']
*import_data()
def make_arbs(crudes,destinations):
    counter = 0
    for i in crudes:
        for k in destinations:
            
            if counter == 0:
                arb_values = arb(i,k, *import_data())
                counter +=1
            else:
                temp = arb(i,k, *import_data())
                arb_values = pd.concat([arb_values,temp], axis=1)
    return arb_values

tester = make_arbs(['Forties'],['Houston'])   
var = import_data()
arb_values
main(crude)
runfile('C:/Users/mima/.spyder-py3/GPW.py', wdir='C:/Users/mima/.spyder-py3')
crudes = ['Azeri','Forties','CPC Blend','Brent']
destinations = ['Houston']
main('Azeri','Houston')
v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))  
df_prices = pd.DataFrame(index=df.index)
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df_prices['vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df_prices['vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df_prices['vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df_prices['vs_dtd'] = diff
    
    return df_prices['vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df_prices['vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df_prices['vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column and create dataframe to hold freight numbers
"""
df_freight = pd.DataFrame(index=df.index)

"""
create the flat rates table for the rates calculations and column creation
"""
def calculate_flat_rate():
    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    return flat_rate_table


flat_rate_table = calculate_flat_rate()
print("flat_rate_table created successfully")

"""
THIS IS ESSENTIALLY A VLOOKUP

inputs are the table to look up values from for each row of another table

in this case we use it to look up the year in the dataframes row and take the year and match it against the corresponding row (or rows? possibly but need to investigate)
and return the value in the rate row.

use the .iat function to return the inger values - this speeds up operations

x refers to the row in the dataframe we are applying to and the name is the index reference of the row - which is the dateindex.
Hence, since you cant take the index.year directly, must turn the date into a string, get the date as a dt object from the string and then get the year

"""

def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])


""" 
Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW
"""

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))   
print("df_freight['Rate'] created successfully")

"""
Then we need to calculate the worldscale - for this we need to get the relevent from and to ws rate, regardless of size so can compare suez vs afra etc
We get the first rate by looking up where the loadport attached to our crude matches the load port in the ports list and returns the subregion
As the subregions dont completely match off (i.e. in targo its MED OECD / MED MID EAST etc etc which is all med, we need to map the sub regions to make sure aligned
We then convert the result to a string and we set index = false as we dont want the header row returned
Do the same for the destination region and then slice the ws table which ahs the routes, origins, destinations etc by where the to and from regions match
This gives us the neccessary world scale codes 

We then loop through ecah item in the list annd determine what to do with it. The ws table has a column called terms which states if lumpsum or $/mt
If lump, then we need to ahndle differently - i.e. we take the value of the lumpsum, multiply by 1mn and divide by the size of the vessel against the lumpsum
If normal then we take the WS, divide by 100, multiply by the rate we placed in the temp frame earlier and divide y the crude specific bt factor

Final values will be in $/bbl

"""

def calculate_world_scale():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    
    ws_codes = ws[(ws['Origin'] == sub_region) &
                      (ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight


calculate_world_scale()
print("calculate_world_scale() created successfully")

df_freight = df_freight.drop(['Rate'], axis=1)



df_prices.iloc[:,1]

df_landed = pd.DataFrame(index=df.index)
for i in df_prices.columns:
    for k in df_freight.columns:
        name = k[:4]+"_"+i
        df_landed[name] = df_prices[i] + df_freight[k]



landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
landed.columns = pd.MultiIndex.from_product([[crude],[destination], landed.columns])
#print(landed.head())
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers


"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    

rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df_prices = pd.DataFrame(index=df.index)
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def convert_wti():
    if crude_vs in index_wti:
        df_prices['vs_wti'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2),
                                      diff)
    elif crude_vs in index_dtd:
        df_prices['vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices


convert_wti()
print("convert_wti() created successfully")

def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df_prices['vs_dtd'] = np.select(conditions, choices)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    
    elif crude_vs in index_dub:
        pass
    
    else:
        df_prices['vs_dtd'] = diff
    
    return df_prices['vs_dtd']


convert_dtd()
print("convert_dtd() created successfully")

def convert_dub():
    if crude_vs in index_wti:
        df_prices['vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    
    elif crude_vs in index_dtd:
        df_prices['vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    
    elif crude_vs in index_dub:
        pass
    
    return df_prices['vs_dub'] 


convert_dub()
print("convert_dub() created successfully")

"""
Drop the expiry column and create dataframe to hold freight numbers
"""
df_freight = pd.DataFrame(index=df.index)
crude = 'Forties'
destination = 'Houston'
     crude = 'Forties'
     destination = 'Houston'
# =============================================================================
    
    df_prices = pd.DataFrame(index=df.index)
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['vs_wti'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2),
                                          diff)
        elif crude_vs in index_dtd:
            df_prices['vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['vs_dtd'] = np.select(conditions, choices)
        
        elif crude_vs in index_dtd:
            df_prices['vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif crude_vs in index_dub:
            pass
        
        else:
            df_prices['vs_dtd'] = diff
        
        return df_prices['vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        
        elif crude_vs in index_dtd:
            df_prices['vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        
        elif crude_vs in index_dub:
            pass
        
        return df_prices['vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    
    """
    Drop the expiry column and create dataframe to hold freight numbers
    """
    df_freight = pd.DataFrame(index=df.index)
    crude = 'Forties'
    destination = 'Houston'
# =============================================================================
    
    df_prices = pd.DataFrame(index=df.index)
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['vs_wti'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2),
                                          diff)
        elif crude_vs in index_dtd:
            df_prices['vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    convert_wti()
    print("convert_wti() created successfully")
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['vs_dtd'] = np.select(conditions, choices)
        
        elif crude_vs in index_dtd:
            df_prices['vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        
        elif crude_vs in index_dub:
            pass
        
        else:
            df_prices['vs_dtd'] = diff
        
        return df_prices['vs_dtd']
    
    convert_dtd()
    print("convert_dtd() created successfully")
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        
        elif crude_vs in index_dtd:
            df_prices['vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        
        elif crude_vs in index_dub:
            pass
        
        return df_prices['vs_dub'] 
    
    convert_dub()
    print("convert_dub() created successfully")
    
    """
    Drop the expiry column and create dataframe to hold freight numbers
    """
    df_freight = pd.DataFrame(index=df.index)
    
    """
    create the flat rates table for the rates calculations and column creation
    """
    def calculate_flat_rate():
        
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        return flat_rate_table
    
    flat_rate_table = calculate_flat_rate()
    print("flat_rate_table created successfully")
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])

calculate_flat_rates
v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,1,np.array(df.index.year))   
df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,1,np.array(df.index.year).transpose())   
df_freight
df_freight = pd.DataFrame(index=df.index)
df_prices.iloc[:,1]
df_freight, df_prices, df_landed = pd.DataFrame(index=df.index)
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)
df_prices
landed
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
def add(x,y):
    z = 4
    def add_z():
        return z+2
    return x+y+z

add(2,3)
calculate_flat_rate()
df_freight
calculate_flat_rate()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        df_freight = df_freight.drop(['Rate'], axis=1)
        return df_freight

calculate_flat_rate()
calculate_freight()
def calculate_freight():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    df_freight = df_freight.drop(['Rate'], axis=1)
    return df_freight

calculate_freight()
df_freight
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
var = import_data() 
def import_data():
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df

var = import_data() 
import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()    
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
crude = 'Forties'
destination = 'Houston'
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
df_freight
calculate_flat_rate()
ef construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        df_freight = df_freight.drop(['Rate'], axis=1)
        return df_freight
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        df_freight = df_freight.drop(['Rate'], axis=1)
        return df_freight
    
    df_freight = calculate_flat_rate()
    df_freight = calculate_freight()

df_freight
calculate_flat_rate()
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

calculate_flat_rate()
df_freight = calculate_flat_rate()
calculate_freight()
def calculate_freight():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    df_freight = df_freight.drop(['Rate'], axis=1)
    return df_freight

calculate_freight()
df_freight = calculate_flat_rate()
calculate_freight()
def calculate_freight():
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        else:
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    return df_freight.drop(['Rate'], axis=1)

calculate_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight.drop(['Rate'], axis=1)
    
    df_freight = calculate_flat_rate()
    df_freight = calculate_freight()
    return df_freight

construct_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight.drop(['Rate'], axis=1)
    
    return df_freigh

def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight.drop(['Rate'], axis=1)
    
    return df_freight

construct_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight.drop(['Rate'], axis=1)
    
    return df_freight

construct_freight()
df_freight
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight.drop(['Rate'], axis=1, inplace=True)
    
    return df_freight

construct_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight.drop(['Rate'], axis=1, inplace=True)

construct_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000).drop(['Rate'], axis=1, inplace=True)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'].drop(['Rate'], axis=1, inplace=True)
        return df_freight
    
    return df_freight

construct_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
       # return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000).drop(['Rate'], axis=1, inplace=True)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'].drop(['Rate'], axis=1, inplace=True)
        return df_freight
    
    return df_freight

construct_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight

construct_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight.drop(['Rate'], axis=1)

construct_freight()
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()    
def import_data():
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df


assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
crude = 'Forties'
destination = 'Houston'
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight.drop(['Rate'], axis=1)

df_freight
construct_freight()
sub_to_ws
sub_to_ws[2]
index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
index_region
def convert_prices():
    """depending on discharge, choose the appropriate index"""
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['vs_wti'] = np.where(expiry_condition, diff + (wtim1 -  wtim2), diff)
        elif crude_vs in index_dtd:
            df_prices['vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['vs_dtd'] = np.select(conditions, choices)
        elif crude_vs in index_dtd:
            df_prices['vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_dtd'] = diff
        return df_prices
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        elif crude_vs in index_dtd:
            df_prices['vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        elif crude_vs in index_dub:
            pass
        return df_prices

index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
df_prices = [f() for index ,f in func_list if index == index_region]
def convert_wti():
    if crude_vs in index_wti:
        df_prices['vs_wti'] = np.where(expiry_condition, diff + (wtim1 -  wtim2), diff)
    elif crude_vs in index_dtd:
        df_prices['vs_wti']  = np.where(cfd_condition,
                                      diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                      diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices


def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df_prices['vs_dtd'] = np.select(conditions, choices)
    elif crude_vs in index_dtd:
        df_prices['vs_dtd'] = np.where(cfd_condition,
                                     diff + (4 * (cfd2 - cfd1) / 14),
                                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_dtd'] = diff
    return df_prices


def convert_dub():
    if crude_vs in index_wti:
        df_prices['vs_dub'] = np.where(expiry_condition,
                                      diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                      diff - (brentm2-wtim2) + efs2)
    elif crude_vs in index_dtd:
        df_prices['vs_dub'] = np.where(cfd_condition, 
                                      diff + cfd2 +  efpm2 + efs2, 
                                      diff + cfd1 +  efpm2 + efs2)
    elif crude_vs in index_dub:
        pass
    return df_prices

index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
df_prices = [f() for index ,f in func_list if index == index_region]
func_list
index
[f() for index, f in func_list if index == index_region]
df_prices = [f() for index, f in func_list.items() if index == index_region]
def convert_prices():
    """depending on discharge, choose the appropriate index"""
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['vs_wti'] = np.where(expiry_condition, diff + (wtim1 -  wtim2), diff)
        elif crude_vs in index_dtd:
            df_prices['vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['vs_dtd'] = np.select(conditions, choices)
        elif crude_vs in index_dtd:
            df_prices['vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_dtd'] = diff
        return df_prices
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        elif crude_vs in index_dtd:
            df_prices['vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        elif crude_vs in index_dub:
            pass
        return df_prices
    
    index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
    func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
    df_prices = [f() for index, f in func_list.items() if index == index_region]
    return df_prices

convert_prices()    
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)
convert_prices() 
[f() for index, f in func_list.items() if index == index_region]
[f() for index, f in func_list.items() if index == index_region][0]
def convert_prices():
    """depending on discharge, choose the appropriate index"""
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['vs_wti'] = np.where(expiry_condition, diff + (wtim1 -  wtim2), diff)
        elif crude_vs in index_dtd:
            df_prices['vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['vs_dtd'] = np.select(conditions, choices)
        elif crude_vs in index_dtd:
            df_prices['vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_dtd'] = diff
        return df_prices
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        elif crude_vs in index_dtd:
            df_prices['vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        elif crude_vs in index_dub:
            pass
        return df_prices
    
    index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
    func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
    df_prices = [f() for index, f in func_list.items() if index == index_region][0]
    return df_prices

convert_prices()  
def convert_prices():
    """depending on discharge, choose the appropriate index"""
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['vs_wti'] = np.where(expiry_condition, diff + (wtim1 -  wtim2), diff)
        elif crude_vs in index_dtd:
            df_prices['vs_wti']  = np.where(cfd_condition,
                                          diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                                          diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['vs_dtd'] = np.select(conditions, choices)
        elif crude_vs in index_dtd:
            df_prices['vs_dtd'] = np.where(cfd_condition,
                                         diff + (4 * (cfd2 - cfd1) / 14),
                                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_dtd'] = diff
        return df_prices
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['vs_dub'] = np.where(expiry_condition,
                                          diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                                          diff - (brentm2-wtim2) + efs2)
        elif crude_vs in index_dtd:
            df_prices['vs_dub'] = np.where(cfd_condition, 
                                          diff + cfd2 +  efpm2 + efs2, 
                                          diff + cfd1 +  efpm2 + efs2)
        elif crude_vs in index_dub:
            pass
        return df_prices
    
    index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
    func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
    [f() for index, f in func_list.items() if index == index_region][0]
    return df_prices

convert_prices()  
df_freight = construct_freight()
df_prices = convert_prices()
df_freight
df_prices
create_landed_values()
def create_landed_values():    
    for i in df_prices.columns:
        for k in df_freight.columns:
            name = k[:4]+"_"+i
            df_landed[name] = df_prices[i] + df_freight[k]
    
    landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
    landed.columns = pd.MultiIndex.from_product([[crude],[destination], landed.columns])
    return landed

create_landed_values()
import pandas as pd
import numpy as np
from datetime import datetime as dt
import random
import time

def import_data():
    
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    prices = pd.read_excel(data, 'prices', header = 1)
    paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    products = pd.read_excel(data, 'rott products')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """
    Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
    """
    prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})
    
    
    """
    Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column
    """
    total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    total = total.iloc[total.index > dt(2015,12,31)]
    
    
    """
    Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
    """
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    
    """
    This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
    """    
    
    rates = []
    
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """
    Also initialise the temp df with index of total.
    Temp df is tol hold the dataseries needed to calculate the freight
    """
    df = pd.DataFrame(index=total.index)
    
    """
    This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    
    Have tried timing and slight improvment with the blow of 0.2seconds....
    """
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                      (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]
    
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df

assay, ws, ports, total, rate_data, sub_to_ws, df = import_data() 
def arb(crude,destination,assay, ws, ports, total, rate_data, sub_to_ws, df): 
    crude = 'Forties'
    destination = 'Houston'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                else:
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight.drop(['Rate'], axis=1)


#construct_freight()
    
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in index_wti:
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + (wtim1 -  wtim2),
                         diff)
            elif crude_vs in index_dtd:
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                         diff + cfd1 +  efpm2 + (brentm2 - wtim2))
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            conditions = [(expiry_condition & cfd_condition),
                          (expiry_condition & np.invert(cfd_condition)),
                          (np.invert(expiry_condition) & cfd_condition),
                          (np.invert(expiry_condition) & np.invert(cfd_condition))]
            choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                       (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                       (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                       (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
            
            if crude_vs in index_wti:
                df_prices['vs_dtd'] = np.select(conditions, choices)
            elif crude_vs in index_dtd:
                df_prices['vs_dtd'] = np.where(cfd_condition,
                         diff + (4 * (cfd2 - cfd1) / 14),
                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in index_wti:
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                         diff - (brentm2-wtim2) + efs2)
            elif crude_vs in index_dtd:
                df_prices['vs_dub'] = np.where(cfd_condition,
                         diff + cfd2 +  efpm2 + efs2,
                         diff + cfd1 +  efpm2 + efs2)
            elif crude_vs in index_dub:
                pass
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices


#convert_prices()                  
    
    def create_landed_values():    
        for i in df_prices.columns:
            for k in df_freight.columns:
                name = k[:4]+"_"+i
                df_landed[name] = df_prices[i] + df_freight[k]
        
        landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
        landed.columns = pd.MultiIndex.from_product([[crude],[destination], landed.columns])
        return landed
    
    df_freight = construct_freight()
    df_prices = convert_prices()
    landed = create_landed_values()
    return landed
arb('Forties','Houdton',assay, ws, ports, total, rate_data, sub_to_ws, df )
arb('Azeri','Houdton',assay, ws, ports, total, rate_data, sub_to_ws, df )
def arb(crude,destination,assay, ws, ports, total, rate_data, sub_to_ws, df): 
    #crude = 'Forties'
    #destination = 'Houston'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                else:
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight.drop(['Rate'], axis=1)


#construct_freight()
    
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in index_wti:
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + (wtim1 -  wtim2),
                         diff)
            elif crude_vs in index_dtd:
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                         diff + cfd1 +  efpm2 + (brentm2 - wtim2))
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            conditions = [(expiry_condition & cfd_condition),
                          (expiry_condition & np.invert(cfd_condition)),
                          (np.invert(expiry_condition) & cfd_condition),
                          (np.invert(expiry_condition) & np.invert(cfd_condition))]
            choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                       (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                       (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                       (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
            
            if crude_vs in index_wti:
                df_prices['vs_dtd'] = np.select(conditions, choices)
            elif crude_vs in index_dtd:
                df_prices['vs_dtd'] = np.where(cfd_condition,
                         diff + (4 * (cfd2 - cfd1) / 14),
                         diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in index_wti:
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                         diff - (brentm2-wtim2) + efs2)
            elif crude_vs in index_dtd:
                df_prices['vs_dub'] = np.where(cfd_condition,
                         diff + cfd2 +  efpm2 + efs2,
                         diff + cfd1 +  efpm2 + efs2)
            elif crude_vs in index_dub:
                pass
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices


#convert_prices()                  
    
    def create_landed_values():    
        for i in df_prices.columns:
            for k in df_freight.columns:
                name = k[:4]+"_"+i
                df_landed[name] = df_prices[i] + df_freight[k]
        
        landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
        landed.columns = pd.MultiIndex.from_product([[crude],[destination], landed.columns])
        return landed
    
    df_freight = construct_freight()
    df_prices = convert_prices()
    landed = create_landed_values()
    return landed

arb('Azeri','Houdton',assay, ws, ports, total, rate_data, sub_to_ws, df )
arb('Azeri','Houston',assay, ws, ports, total, rate_data, sub_to_ws, df )
var = import_data()
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from collections import defaultdict
from datetime import datetime as dt
from ArbEcons import import_data
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
def standard_ref(crude, assay, ws, ports, total, rate_data, sub_to_ws, df, refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 ):
    def cdu():
        ### Fractions for gaosline pool
        if refinery_volume is None:
            refinery_volume = 200
        if lvn_gasolinepool is None:
            lvn_gasolinepool = 0.12
        if kero_gasolinepool is None:
            kero_gasolinepool = 0.15
        
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        if reformer_capacity is None:
            reformer_capacity = refinery_volume*0.21
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.21
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.1},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    def calculate_margin():
        """this is to create the look up neccessary for generating the table"""
        price_codes = {'HOUSTON':{'propane':'PCALB00',
                          'butane':'ASGCB00',
                          'naphtha':'AASAV00',
                          'gasoline':'AAGXH00',
                          'kero':'AAWEY00',
                          'ulsd':'AASAT00',
                          'gasoil':'AAUFL00',
                          'lsfo':'AAHPM00'}}
        
        pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]] = j
        
        regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
        gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
        return gpw
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    gpw = calculate_margin()
    return gpw

def standard_ref(crude, destinaton, assay, ws, ports, total, rate_data, sub_to_ws, df, refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 ):
    def cdu():
        ### Fractions for gaosline pool
        if refinery_volume is None:
            refinery_volume = 200
        if lvn_gasolinepool is None:
            lvn_gasolinepool = 0.12
        if kero_gasolinepool is None:
            kero_gasolinepool = 0.15
        
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        if reformer_capacity is None:
            reformer_capacity = refinery_volume*0.21
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.21
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.1},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    def calculate_margin():
        """this is to create the look up neccessary for generating the table"""
        price_codes = {'HOUSTON':{'propane':'PCALB00',
                          'butane':'ASGCB00',
                          'naphtha':'AASAV00',
                          'gasoline':'AAGXH00',
                          'kero':'AAWEY00',
                          'ulsd':'AASAT00',
                          'gasoil':'AAUFL00',
                          'lsfo':'AAHPM00'}}
        
        pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]] = j
        
        regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
        gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
        return gpw
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    gpw = calculate_margin()
    return gpw

standard_ref('Azeri', 'Houston',assay, ws, ports, total, rate_data, sub_to_ws, df, refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 )
refinery_volume = None,
lvn_gasolinepool = None,
kero_gasolinepool = None,
reformer_capacity = None,
fcc_capacity = None
standard_ref('Azeri', 'Houston',assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 )
def standard_ref(crude, destinaton, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 ):
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.21
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.1},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    def calculate_margin():
        """this is to create the look up neccessary for generating the table"""
        price_codes = {'HOUSTON':{'propane':'PCALB00',
                          'butane':'ASGCB00',
                          'naphtha':'AASAV00',
                          'gasoline':'AAGXH00',
                          'kero':'AAWEY00',
                          'ulsd':'AASAT00',
                          'gasoil':'AAUFL00',
                          'lsfo':'AAHPM00'}}
        
        pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]] = j
        
        regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
        gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
        return gpw
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    gpw = calculate_margin()
    return gpw

standard_ref('Azeri', 'Houston',assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 )
def standard_ref(crude, destinaton, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 ):
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.21
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.1},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    def calculate_margin():
        """this is to create the look up neccessary for generating the table"""
        price_codes = {'HOUSTON':{'propane':'PCALB00',
                          'butane':'ASGCB00',
                          'naphtha':'AASAV00',
                          'gasoline':'AAGXH00',
                          'kero':'AAWEY00',
                          'ulsd':'AASAT00',
                          'gasoil':'AAUFL00',
                          'lsfo':'AAHPM00'}}
        
        pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]] = j
        
        regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
        gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
        return gpw

standard_ref('Azeri', 'Houston',assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 )
gpw = standard_ref('Azeri', 'Houston',assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 )
destination
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from collections import defaultdict
from datetime import datetime as dt
from ArbEcons import import_data

assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()

def standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 ):
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.21
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.1},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    def calculate_margin():
        """this is to create the look up neccessary for generating the table"""
        price_codes = {'HOUSTON':{'propane':'PCALB00',
                          'butane':'ASGCB00',
                          'naphtha':'AASAV00',
                          'gasoline':'AAGXH00',
                          'kero':'AAWEY00',
                          'ulsd':'AASAT00',
                          'gasoil':'AAUFL00',
                          'lsfo':'AAHPM00'}}
        
        pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]] = j
        
        regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
        gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
        return gpw
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    gpw = calculate_margin()
    return gpw

gpw = standard_ref('Azeri', 'Houston',assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None
                 )
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
arb('Azeri','Houston',*var)
var = import_data()
arb('Azeri','Houston',*var)
standard_ref('Azeri','Houston',*var)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
arb('Azeri','Houston',*var)
standard_ref('Azeri','Houston',*var)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
standard_ref('Azeri','Houston',*var)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
standard_ref('Azeri','Houston',*var)
from GPW2504 import standard_ref
standard_ref('Azeri','Houston',*var)
from GPW2504 import standard_ref
standard_ref('Azeri','Houston',*var)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
standard_ref('Azeri','Houston',*var)
gpw = calculate_margin()
gpw = pd.DataFrame(gpw, columns = 'gpw')
gpw = pd.DataFrame(gpw, columns = ['gpw'])
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
standard_ref('Azeri','Houston',*var)
arb('Azeri','Houston',*var)
standard_ref('Azeri','Houston',*var)
arb('Azeri','Houston',*var)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
def make_arbs(crudes,destinations):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                counter +=1
            else:
                next_arb = arb(i,k, *var)
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
    return arb_values

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                counter +=1
            else:
                next_arb = arb(i,k, *var)
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
    return arb_values

make_arbs(crudes, destinations, *var)
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston']
make_arbs(crudes, destinations, *var)
global_arb = make_arbs(crudes, destinations, *var)
standard_ref('Azeri','Houston',*var)
from GPW2504 import standard_ref
standard_ref('Azeri','Houston',*var)
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
arb('Azeri','Houston',*var)
arb('Azeri','Houston',*var)[1]
arb('Azeri','Houston',*var)
arb('Azeri','Houston',*var)[1]
arb('Azeri','Houston',*var)[0]
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
arb('Azeri','Houston',*var)[0]
arb('Azeri','Houston',*var)[1]
arb('Azeri','Houston',*var)[2]
arb('Azeri','Houston',*var)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
arb('Azeri','Houston',*var)
arb('Azeri','Houston',*var)[1]
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for i in arb_values.columns:
                    name = i[:4]+"_margin"
                    gpw[name] = gpw - arb_values[i]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for i in next_arb.columns:
                    name = i[:4]+"_margin"
                    next_gpw[name] = next_gpw - next_arb[i]
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
    return arb_values

make_arbs(crudes, destinations, *var)
arb_values.columns
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for i in arb_values.columns.levels[2]:
                    name = i[:4]+"_margin"
                    gpw[name] = gpw - arb_values[i]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for i in next_arb.columns.levels[2]:
                    name = i[:4]+"_margin"
                    next_gpw[name] = next_gpw - next_arb[i]
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
name
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
global_arb[1]
global_arb.levels[1]
global_arb.get_level_values(1)
global_arb.get_level_values(0)
arb('Azeri','Houston',*var)[1]
standard_ref('Azeri','Houston',*var)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
arb('Azeri','Houston',*var)[1]
standard_ref('Azeri','Houston',*var)
arb('Azeri','Houston',*var)
arb('Azeri','Houston',*var)[1]
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[crude],[destination], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[crude],[destination], arb_values.columns])
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[j], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[j], arb_values.columns])
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                #arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
                #arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
arb_values = arb('Azeri','Houston', *var)[1]
gpw = standard_ref('Azeri','Houston', *var)
arb_values - gpw
for i in arb_values.columns:
    print(arb_values[i] - gpw)

arb_values
arb_values[i]
arb_values[i] - gpw
gpw
arb_values[i].subtract(gpw)
arb_values[i].subtract(gpw, 1)
arb_values[i].subtract(gpw, 0)
arb_values[i].subtract(gpw, 'index')
arb_values[i
arb_values[i]
arb_values[i].subtract(gpw, axis= 'index')
gpw -1
gpw
arb_values -1
arb_values - gpw
arb_values - gpw.values
arb_values[i].subtract(gpw.values, axis= 'index')
arb_values[i]
gpw
arb_values[i]
arb_values[i]values.subtract(gpw.values, axis= 'index')
arb_values[i]
arb_values[i]values
arb_values[i].values.subtract(gpw.values, axis= 'index')
arb_values[i].values - gpw.values
for i in arb_values.columns:
    print(arb_values[i].values - gpw.values)

arb_values[i]
print(arb_values[i].values)
arb_values - gpw
arb_values
arb_values = arb('Azeri','Houston', *var)[1]
gpw = standard_ref('Azeri','Houston', *var)
arb_values - gpw
for i in arb_values.columns:
    arb_values[i] - gpw

arb('Azeri','Houston', *var)[1]
arb_values['margin'] = arb_values - gpw
for i in arb_values.columns:
    arb_values['margin'] = arb_values[i] - gpw

arb_values.columns
arb_values[i]
for i in arb_values.columns:
    arb_values['margin'] = arb_values[i].values - gpw.values

gpw
for i in arb_values.columns:
    arb_values['margin'] = arb_values[i].values - gpw['GPW'].values

arb_values
gpw['margin'] = [gpw['GPW'] - arb_values[i] for i in arb_values.columns] 
arb_values.columns
arb_values = arb('Azeri','Houston', *var)[1]
gpw = standard_ref('Azeri','Houston', *var)
gpw['margin'] = [gpw['GPW'] - arb_values[i] for i in arb_values.columns] 
arb_values[i]
gpw['GPW']
arb_values.columns
for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]

gpw
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                #arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
                #arb_va

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                #arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
                #arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                arb_values = pd.concat([arb_values,next_arb,next_gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
arb_values
arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
arb('Azeri','Houston',*var)[1]
standard_ref('Azeri','Houston',*var
arb('Azeri','Houston',*var)[1]
standard_ref('Azeri','Houston',*var)
arb('Azeri','Houston',*var)[0]
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb
crudes = ['Azeri','Forties','Brent']
make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)
    
    
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)
    
    
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
var = import_data()
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston']

#arb('Azeri','Houston',*var)[0]
#standard_ref('Azeri','Houston',*var)


def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)
    
    
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb
global_arb['Azeri']['Houston']
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston','Rotterdam']
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)                         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb = make_arbs(crudes, destinations, *var)
print(global_arb)
global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
var = import_data()
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)                         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston','Rotterdam']
global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
global_arb['Azeri']['Houston']
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx'
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
raw_rates
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[1]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[1]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)                         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
global_arb = make_arbs(crudes, destinations, *var)
global_arb = make_arbs(crudes, destinations, *var
global_arb = make_arbs(crudes, destinations, *var)
price_codes
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston','Rotterdam','Singapore']
global_arb = make_arbs(crudes, destinations, *var)
global_arb
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston','Rotterdam','Augusta','Singapore']
global_arb = make_arbs(crudes, destinations, *var)
global_arb
crudes = ['Azeri','Forties','Brent']
destinations = ['Houston','Rotterdam','Augusta','Singapore']
global_arb = make_arbs(crudes, destinations, *var)
var[0]
pd.DataFrame(var[0])
pd.DataFrame(var[0]).head()
pd.DataFrame(var[0].transpose()).head()
pd.DataFrame(var[0]).transpose().head()
test_crudes.dropna(axis = 0, inplace = True)
test_crudes = pd.DataFrame(var[0]).transpose().head()
test_crudes.dropna(axis = 0, inplace = True)
test_crudes = pd.DataFrame(var[0]).transpose().head()
test_crudes = pd.DataFrame(var[0]).transpose()
test_crudes = pd.DataFrame(var[0]).transpose().head()['Code']
.head()
test_crudes = pd.DataFrame(var[0]).transpose()['Code']
test_crudes
test_crudes.dropna(axis = 0, inplace = True)
test_crudes = pd.DataFrame(var[0]).transpose()['Code']
test_crudes.dropna(axis = 0, inplace = True).index
test_crudes = pd.DataFrame(var[0]).transpose()['Code']
test_crudes.dropna(axis = 0, inplace = True).index.values.tolist()
test_crudes = pd.DataFrame(var[0]).transpose()['Code']
test_crudes.dropna(axis = 0, inplace = True).index.values.tolist()
test_crudes = pd.DataFrame(var[0]).transpose()['Code']
list(test_crudes.index.values)
test_crudes
test_crudes = pd.DataFrame(var[0]).transpose()['Code']
test_crudes.dropna(axis = 0, inplace = True)
list(test_crudes.index.values)
test_crudes = pd.DataFrame(var[0]).transpose()['Code']
test_crudes.dropna(axis = 0, inplace = True)
list(test_crudes.index.values)

#crudes = ['Azeri','Forties','Brent']
crudes = list(test_crudes.index.values)
destinations = ['Houston','Rotterdam','Augusta','Singapore']
global_arb = make_arbs(crudes, destinations, *var)
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes.dropna(axis = 0, inplace = True)
rate_data
var[4]
var[4]['LoadPort'].unique()
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes.dropna(axis = 0, inplace = True)
test_crudes = list(test_crudes.index.values)
test_crudes
test_crudes[test_crudes['Loadport'].isin(test_crudes)]
test_crudes[test_crudes['Loadport'].isin([test_crudes])]
test_crudes['Loadport']
test_crudes
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes.dropna(axis = 0, inplace = True)
test_crudes
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes.dropna(axis = 0, inplace = True)
var[4]['LoadPort'].unique()
test_crudes.loc[test_crudes['LoadPort'].isin(port_list)]
port_list = var[4]['LoadPort'].unique()
test_crudes.loc[test_crudes['LoadPort'].isin(port_list)]
test_crudes
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis = 0, inplace = True)

"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage"""
ports_with_rates = var[4]['LoadPort'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis = 0, inplace = True)
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis = 0, inplace = True)

"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage"""
ports_with_rates = var[4]['LoadPort'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis = 1, inplace = True)

"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage"""
ports_with_rates = var[4]['LoadPort'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]

"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage"""
ports_with_rates = var[4]['LoadPort'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
test_crudes
test_crudes.dropna(axis=0)
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes = test_crudes.dropna(axis=0)
test_crudes
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage"""
ports_with_rates = var[4]['LoadPort'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
test_crudes
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']]
test_crudes
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
test_crudes
len(test_crudes)
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
len(test_crudes)
crudes = list(test_crudes.index.values)
global_arb = make_arbs(crudes, destinations, *var)
total.columns
test_crudes
var[3]
crudes_with_codes = var[3].columns.values.unique()
var[3].columns.values
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
crudes_with_codes = var[3].columns.values

test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
test_crudes
crudes = list(test_crudes.index.values)
destinations = ['Houston','Rotterdam','Augusta','Singapore']
global_arb = make_arbs(crudes, destinations, *var)
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
len(test_crudes)

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
test_crudes
crudes = list(test_crudes.index.values)
global_arb = make_arbs(crudes, destinations, *var)
var[4]
ports_with_rates
test_crudes
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
len(test_crudes)

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]




crudes = list(test_crudes.index.values)
crudes
var[4]
var[3]
var[2]
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
test_crudes
crudes = list(test_crudes.index.values)
global_arb = make_arbs(crudes, destinations, *var)
crudes
crudes = ['Brent','Azeri']
crudes = ['Brent','Azeri']
destinations = ['Houston','Rotterdam','Augusta','Singapore']
global_arb = make_arbs(crudes, destinations, *var)
global_arb
for i in crudes:
    make_arbs(crudes, destinations, *var)

for i in crudes:
    print(i)
    print(make_arbs(crudes, destinations, *var))

crudes = ['Brent','Azeri','CPC Blend']
global_arb = make_arbs(crudes, destinations, *var)
global_arb
global_arb['Azeri']['Houston']
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})

ports
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
ef make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[0]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[0]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)                         
    
    return arb_values
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            if counter == 0:
                arb_values = arb(i,k, *var)[0]
                gpw = standard_ref(i,k, *var)
                for j in arb_values.columns:
                   name = j[:4]+"_margin"
                   gpw[name] = gpw['GPW'] - arb_values[j]
                arb_values = pd.concat([arb_values,gpw], axis=1)
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1
            else:
                next_arb = arb(i,k, *var)[0]
                next_gpw = standard_ref(i,k, *var)
                for j in next_arb.columns:
                   name = j[:4]+"_margin"
                   next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                next_arb = pd.concat([next_arb,next_gpw], axis=1)
                next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                arb_values = pd.concat([arb_values,next_arb], axis=1)                         

global_arb = make_arbs(crudes, destinations, *var)
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
crudes = list(test_crudes.index.values)
crudes
for i in crudes:
    print(i)
    try:
        make_arbs(i, destinations, *var)
    except:
        print("{} didn't work".format(i))

crudes
make_arbs(i, destinations, *var)
i
for i in crudes:
    print(i)
    try:
        make_arbs(i, destinations, *var)
    except:
        print("{} didn't work".format(i))

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except:
                print("{} didn't work".format(i))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except:
                print("{} failed into {}".format(i,j))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except:
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:44,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:45,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
ef make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
crudes
flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
          (rate_data['DischargePort'] == destination)]
rate_data
var = import_data()
global_arb = make_arbs(crudes, destinations, *var)
global_arb
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_with_rates)]
"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]







crudes = list(test_crudes.index.values)
crudes = ['Brent','Azeri','CPC Blend']
destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb
global_arb.head()
test_crudes_to_check = test_crudes.loc[~test_crudes['LoadPort'].isin(ports_with_rates)]
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
test_crudes = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
test_crudes
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targp = crude_check.loc[crude_check['LoadPort'].isin(ports_in_targo)]
test_crudes
not_in_targp
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[crude_check['Code'].isin(crudes_with_codes)]
not_in_targo
codes_missing
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
codes_missing
no_flat_rates
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - tis prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
no_flat_rates
global_arb = make_arbs(crudes, destinations, *var)
global_arb
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
len(test_crudes)
"""Check that these posrt all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]






crudes = list(test_crudes.index.values)
global_arb = make_arbs(crudes, destinations, *var)
import pyodbc
import pandas as pd
query = ("""exec sp_CurrentStems @datasource = 'clipper', @minLoadMonth = '2014-01-01'""")
con = pyodbc.connect(
        '''DRIVER=SQL Server Native Client 11.0;
        SERVER=STCHGS126;UID=mima;
        Trusted_Connection=Yes;
        APP=Microsoft Office 2010;
        WSID=STUKLW050;
        DATABASE=STG_Targo;''')
test = pd.read_sql(query, con)
test.columns
import pyodbc
import numpy as np
import pandas as pd
table = pd.pivot_table(test, values='quantity', index = ['LoadCountry','DischargeCountry'], columns = 'LoadMonth', aggfunc=np.sum)
table = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'LoadMonth', aggfunc=np.sum)
table[table.LoadCountry == 'USA']
table.ix['United States']
table.columns
pd.to_datetime(table.columns)
table.columns = pd.to_datetime(table.columns)
table.ix['United States']
table.reindex_axis(sorted(table.columns))
table.reindex_axis(sorted(table.columns), axis=1)
table.ix['United States']['South Korea']
table.ix['United States','South Korea']
table
table.reindex_axis(sorted(table.columns), axis=1, inplace=True)
table = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'LoadMonth', aggfunc=np.sum)
table.columns = pd.to_datetime(table.columns)
table = table.reindex_axis(sorted(table.columns), axis=1)
table.ix['United States','South Korea']
table.iloc['United States','South Korea']
table.loc['United States','South Korea']
table = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','LoadPort','LoadPoint','DischargeCountry'], columns = 'LoadMonth', aggfunc=np.sum)
table.columns = pd.to_datetime(table.columns)
table = table.reindex_axis(sorted(table.columns), axis=1)
table.loc['United States','South Korea']
table.loc['United States',,,'South Korea']
table.loc['United States',*,*,'South Korea']
table.loc['Transneft - Novorossiysk CPC Terminal','South Korea']
table
table = pd.pivot_table(test, values='Quantity', index = ['LoadPoint','DischargeCountry'], columns = 'LoadMonth', aggfunc=np.sum)
table.columns = pd.to_datetime(table.columns)
table = table.reindex_axis(sorted(table.columns), axis=1)
table.loc['United States','South Korea']
table = pd.pivot_table(test, values='Quantity', index = ['LoadPoint','DischargeCountry'], columns = 'LoadMonth', aggfunc=np.sum)
table.columns = pd.to_datetime(table.columns)
table = table.reindex_axis(sorted(table.columns), axis=1)
table
table.loc['Transneft - Novorossiysk CPC Terminal','South Korea']
import seaborn as sns
sns.set_style("darkgrid")
plt.plot(np.cumsum(np.random.randn(1000,1)))
plt.show()
import matplotlib.pyplot as plt
sns.set_style("darkgrid")
plt.plot(np.cumsum(np.random.randn(1000,1)))
plt.show()
flow = table.loc['Transneft - Novorossiysk CPC Terminal','South Korea']
flow = table.loc['Transneft - Novorossiysk CPC Terminal','South Korea']
sns.set_style("darkgrid")
plt.plot(flow)
plt.show()
flow = table.loc['Transneft - Novorossiysk CPC Terminal','South Korea']
flow
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['CleanGrade','DischargeCountry'], columns = ['DischargeMonth','DischargeDecade'], aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['CleanGrade','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals
sk_arrivals = sk_arrivals.loc['CPC Blend','South Korea']
sk_arrivals
us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
us_korea.loc['United States','South Korea']
grades = ['Forties','CPC Blend']
df = pd.DataFrame()
for i in grades:
    df[i] = sk_arrivals.loc[i,'South Korea']

print(df)
grades = ['FORTIES','CPC Blend']
df = pd.DataFrame()
for i in grades:
    df[i] = sk_arrivals.loc[i,'South Korea']

print(df)
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['CleanGrade','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
grades = ['FORTIES','CPC Blend']
df = pd.DataFrame()
for i in grades:
    df[i] = sk_arrivals.loc[i,'South Korea']

print(df)
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['CleanGrade','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)

grades = ['FORTIES','CPC Blend']
df = pd.DataFrame()
for i in grades:
    df[i] = sk_arrivals.loc[i,'South Korea']

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
df
df.sum(axis=0)
df.sum(axis=1)
plt.plot(df)
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['CleanGrade','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)

grades = ['FORTIES','CPC Blend']
df = pd.DataFrame()
for i in grades:
    df[i] = sk_arrivals.loc[i,'South Korea']

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
df['Total']= df.sum(axis=1)
df.fillna(0)
plt.plot(df)
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['CleanGrade','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)

grades = ['FORTIES','CPC Blend']
df = pd.DataFrame()
for i in grades:
    df[i] = sk_arrivals.loc[i,'South Korea']

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
df['Total']= df.sum(axis=1)
df = df.fillna(0)
plt.plot(df)
sk_arrivals
sk_arrivals.ix[,'South Korea']
sk_arrivals.ix[:,'South Korea']
sk_arrivals.loc[:,'South Korea']
sk_arrivals
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals.loc['South Korea']
sk_arrivals
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['DischargeCountry', 'CleanGrade'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals.loc['South Korea',:]
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals.loc['South Korea',:]
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['DischargeCountry', 'Grade'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals.loc['South Korea',:]
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['DischargeCountry', 'Grade'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals.loc['South Korea',:]


grades = ['FORTIES','CPC Blend','ES SIDER']
df = pd.DataFrame()
for i in grades:
    df[i] = sk_arrivals.loc['South Korea',i]

print(df)
grades = ['FORTIES','CPC Blend','ES SIDER']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)
us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
df['Total']= df.sum(axis=1)
df = df.fillna(0)
plt.plot(df)
table.columns
table = table.loc[:,'2017-01-01':]
sk_arrivals = pd.pivot_table(test, values='Quantity', index = ['DischargeCountry', 'Grade'], columns = 'DischargeMonth', aggfunc=np.sum)
sk_arrivals.columns = pd.to_datetime(sk_arrivals.columns)
sk_arrivals = sk_arrivals.reindex_axis(sorted(sk_arrivals.columns), axis=1)
sk_arrivals.loc['South Korea',:]


grades = ['FORTIES','CPC Blend','ES SIDER']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
df['Total']= df.sum(axis=1)
df = df.fillna(0)
plt.plot(df)
df.loc[:,'2017-01-01':]
plt.plot(df)
df
df.loc['2017-01-01':,:]
plt.plot(df)
df = df.loc['2017-01-01':,:]
plt.plot(df)
us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
#df['Total']= df.sum(axis=1)
df = df.fillna(0)
df = df.loc['2017-01-01':,:]
plt.plot(df)
ax = sns.barplot(x = df.index, y = 'bbls', data = df)
ax = sns.barplot(x = df.index, data = df)
df = df.fillna(0)
df = df.loc['2017-01-01':,:]
plt.plot(df)
df = df.fillna(0)
df = df.loc['2017-01-01':,:]
plt.legend()
plt.plot(df)
df
grades = ['FORTIES','CPC Blend','ES SIDER','rasgas condensate','northwest shelf condensate','SOUTH PARS CONDENSATE','algerian condensate']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
grades = ['FORTIES','CPC Blend','ES SIDER','rasgas condensate','northwest shelf condensate','SOUTH PARS CONDENSATE','algerian condensate']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']

df = df.fillna(0)
df = df.loc['2017-01-01':,:]

plt.plot(df)
grades = ['FORTIES','CPC Blend','ES SIDER','rasgas condensate','northwest shelf condensate','SOUTH PARS CONDENSATE','algerian condensate']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']

df = df.fillna(0)
df = df.loc['2017-01-01':,:]
pl.legend()
plt.plot(df)
grades = ['FORTIES','CPC Blend','ES SIDER','rasgas condensate','northwest shelf condensate','SOUTH PARS CONDENSATE','algerian condensate']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']

df = df.fillna(0)
df = df.loc['2017-01-01':,:]
plt.legend()
plt.plot(df)
df = df.loc['2017-01-01':,:]
grades = ['FORTIES','CPC Blend','ES SIDER','rasgas condensate','northwest shelf condensate','SOUTH PARS CONDENSATE','algerian condensate']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']

df = df.fillna(0)
df = df.loc['2017-01-01':,:]
plt.legend()
plt.plot(df, label)
for i in df.columns:
    ax.plot(x=df.index, y=df[i].values, label = i)

fig, ax = plt.subplots()
for i in df.columns:
    ax.plot(x=df.index, y=df[i].values, label = i)

fig, ax = plt.subplots()
for i in df.columns:
    ax.plot(x=df.index, y=df[i].values, label = i)

leg = ax.legend()
ax.plot()
df.columns
ax.plot(x=df.index, y=df[i].values, label = i)
i
df[i].values
fig, ax = plt.subplots()
for i in df.columns:
    ax.plot(x=df.index, y=df[i].values, label = i)
    ax.plot()

for i in df.columns:
    plt.plot(x=df.index, y=df[i].values, label = i)

for i in df.columns:
    plt.plot(x=df.index, y=df[i].values)

plt
plt.show()
for i in df.columns:
    plt.plot(x=df.index, y=df[i].values)

df.index
for i in df.columns:
    plt.plot(df.index, df[i].values)

for i in df.columns:
    plt.plot(df.index, df[i].values, labels = i)

fig = plt.figure()
ax = plt.axes()

x = np.linspace(0, 10, 1000)
ax.plot(x, np.sin(x));
fig = plt.figure()
ax = plt.axes()
x = df.index
y= df['USA'].values
ax.plot(x, y)
for i in df.columns:
    plt.plot(df.index, df[i].values, labels = i)

fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, labels = i)

df.columns
fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, labels = str(i))

fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, label = i)

fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, label = i)


plt.legend()
fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, label = i)

plt.xticks(rotation=90)    
plt.legend()
df['Grand Total'] = df.sum(axis=0)
df = df.fillna(0)
df = df.loc['2017-01-01':,:]

fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, label = i)

plt.xticks(rotation=90)    
plt.legend()
grades = ['FORTIES','CPC Blend','ES SIDER','rasgas condensate','northwest shelf condensate','SOUTH PARS CONDENSATE','algerian condensate']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
df['Grand Total'] = df.sum(axis=1)
df = df.fillna(0)
df = df.loc['2017-01-01':,:]

fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, label = i)

plt.xticks(rotation=90)    
plt.legend()
grades = ['FORTIES','CPC Blend','ES SIDER','rasgas condensate','northwest shelf condensate','SOUTH PARS CONDENSATE','algerian condensate']
df = pd.DataFrame()
for i in grades:
    try:
        df[i] = sk_arrivals.loc['South Korea',i]
    except Exception as e:
        print(e)
        print(i)

print(df)

us_korea = pd.pivot_table(test, values='Quantity', index = ['LoadCountry','DischargeCountry'], columns = 'DischargeMonth', aggfunc=np.sum)
us_korea.columns = pd.to_datetime(us_korea.columns)
us_korea = us_korea.reindex_axis(sorted(us_korea.columns), axis=1)
df['USA'] = us_korea.loc['United States','South Korea']
#df['Grand Total'] = df.sum(axis=1)
df = df.fillna(0)
df = df.loc['2017-01-01':,:]

fig = plt.figure()
ax = plt.axes()
for i in df.columns:
    plt.plot(df.index, df[i].values, label = i)

plt.xticks(rotation=90)    
plt.legend()
total
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
ws = pd.read_excel(data, 'ws')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
ws_table
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]
prices
data
ws

## ---(Tue May  1 09:48:46 2018)---
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
crudes = list(test_crudes.index.values)
crudes
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes = list(test_crudes.index.values)
destinations = ['Houston']
global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]
total
"""This finds the correct worldscale rate and adjusts if it is lumpsum"""
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
crude = 'Azeri'
finds the correct worldscale rate and adjusts if it is lumpsum"""
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
"""This finds the correct worldscale rate and adjusts if it is lumpsum"""
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
crude = 'Azeri'
destination = 'Houston'
"""This finds the correct worldscale rate and adjusts if it is lumpsum"""
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes
total
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
var = import_data()
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]
test_crudes
no_flat_rates
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes
crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
destinations = ['Houston']
var
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)[1]
                    gpw = standard_ref(i,k, *var)
                    for j in arb_values.columns:
                       name = j[:4]+"_margin"
                       gpw[name] = gpw['GPW'] - arb_values[j]
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)[1]
                    next_gpw = standard_ref(i,k, *var)
                    for j in next_arb.columns:
                       name = j[:4]+"_margin"
                       next_gpw[name] = next_gpw['GPW'] - next_arb[j]
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
global_arb['Azeri']['Houston']
global_arb['Brent']['Houston']
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
df_prices
df_prices['wtim1'], wtim1 = total['CLc1'] 
df_prices['wtim1'] = wtim1 = total['CLc1'] 
df_prices
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]
df_prices['diff'] = diff
df_prices['wtim1'] = wtim1
df_prices['wtim2'] = wtim2
df_prices['vs_wti'] = np.where(expiry_condition,
         diff + (wtim1 -  wtim2),
         diff)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
df_prices['diff'] = diff
df_prices['wtim1'] = wtim1
df_prices['wtim2'] = wtim2
df_prices['vs_wti'] = np.where(expiry_condition,
         diff + (wtim1 -  wtim2),
         diff)
df_prices
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)
df_prices['diff'] = diff
df_prices['cfd1'] = cfd1
df_prices['cfd2'] = cfd2
df_prices['wtim2'] = wtim2
df_prices['efpm2'] = efpm2
df_prices['brentm2'] = brentm2
df_prices['vs_wti']  = np.where(cfd_condition,
         diff + cfd2 +  efpm2 + (brentm2 - wtim2),
         diff + cfd1 +  efpm2 + (brentm2 - wtim2))
df_prices
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight.drop(['Rate'], axis=1)


def convert_prices():
    """depending on discharge, choose the appropriate index"""
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['vs_wti'] = np.where(expiry_condition,
                     diff + (wtim1 -  wtim2),
                     diff)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim2'] = wtim2
            df_prices['efpm2'] = efpm2
            df_prices['brentm2'] = brentm2
            df_prices['vs_wti']  = np.where(cfd_condition,
                     diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                     diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['brentm2'] = brentm2
            df_prices['efpm2'] = efpm2
            df_prices['vs_dtd'] = np.select(conditions, choices)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['vs_dtd'] = np.where(cfd_condition,
                     diff + (4 * (cfd2 - cfd1) / 14),
                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_dtd'] = diff
        return df_prices
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['brentm2'] = brentm2
            df_prices['efs2'] = efs2
            df_prices['vs_dub'] = np.where(expiry_condition,
                     diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                     diff - (brentm2-wtim2) + efs2)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['efpm2'] = efpm2
            df_prices['efs2'] = efs2
            df_prices['vs_dub'] = np.where(cfd_condition,
                     diff + cfd2 +  efpm2 + efs2,
                     diff + cfd1 +  efpm2 + efs2)
        elif crude_vs in index_dub:
            pass
        return df_prices
    
    index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
    func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
    [f() for index, f in func_list.items() if index == index_region][0]
    return df_prices

index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
[f() for index, f in func_list.items() if index == index_region][0]
convert_wti
def convert_wti():
    if crude_vs in index_wti:
        df_prices['diff'] = diff
        df_prices['wtim1'] = wtim1
        df_prices['wtim2'] = wtim2
        df_prices['vs_wti'] = np.where(expiry_condition,
                 diff + (wtim1 -  wtim2),
                 diff)
    elif crude_vs in index_dtd:
        df_prices['diff'] = diff
        df_prices['cfd1'] = cfd1
        df_prices['cfd2'] = cfd2
        df_prices['wtim2'] = wtim2
        df_prices['efpm2'] = efpm2
        df_prices['brentm2'] = brentm2
        df_prices['vs_wti']  = np.where(cfd_condition,
                 diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                 diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices

def convert_prices():

def convert_wti():
    if crude_vs in index_wti:
        df_prices['diff'] = diff
        df_prices['wtim1'] = wtim1
        df_prices['wtim2'] = wtim2
        df_prices['vs_wti'] = np.where(expiry_condition,
                 diff + (wtim1 -  wtim2),
                 diff)
    elif crude_vs in index_dtd:
        df_prices['diff'] = diff
        df_prices['cfd1'] = cfd1
        df_prices['cfd2'] = cfd2
        df_prices['wtim2'] = wtim2
        df_prices['efpm2'] = efpm2
        df_prices['brentm2'] = brentm2
        df_prices['vs_wti']  = np.where(cfd_condition,
                 diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                 diff + cfd1 +  efpm2 + (brentm2 - wtim2))
    
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices


def convert_dtd():
    conditions = [(expiry_condition & cfd_condition),
                  (expiry_condition & np.invert(cfd_condition)),
                  (np.invert(expiry_condition) & cfd_condition),
                  (np.invert(expiry_condition) & np.invert(cfd_condition))]
    choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
               (diff - (brentm2 - wtim2) - efpm2 - cfd2),
               (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
    
    if crude_vs in index_wti:
        df_prices['diff'] = diff
        df_prices['cfd1'] = cfd1
        df_prices['cfd2'] = cfd2
        df_prices['wtim1'] = wtim1
        df_prices['wtim2'] = wtim2
        df_prices['brentm2'] = brentm2
        df_prices['efpm2'] = efpm2
        df_prices['vs_dtd'] = np.select(conditions, choices)
    elif crude_vs in index_dtd:
        df_prices['diff'] = diff
        df_prices['cfd1'] = cfd1
        df_prices['cfd2'] = cfd2
        df_prices['vs_dtd'] = np.where(cfd_condition,
                 diff + (4 * (cfd2 - cfd1) / 14),
                 diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
    elif crude_vs in index_dub:
        pass
    else:
        df_prices['vs_dtd'] = diff
    return df_prices


def convert_dub():
    if crude_vs in index_wti:
        df_prices['diff'] = diff
        df_prices['cfd1'] = cfd1
        df_prices['cfd2'] = cfd2
        df_prices['wtim1'] = wtim1
        df_prices['wtim2'] = wtim2
        df_prices['brentm2'] = brentm2
        df_prices['efs2'] = efs2
        df_prices['vs_dub'] = np.where(expiry_condition,
                 diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                 diff - (brentm2-wtim2) + efs2)
    elif crude_vs in index_dtd:
        df_prices['diff'] = diff
        df_prices['cfd1'] = cfd1
        df_prices['cfd2'] = cfd2
        df_prices['efpm2'] = efpm2
        df_prices['efs2'] = efs2
        df_prices['vs_dub'] = np.where(cfd_condition,
                 diff + cfd2 +  efpm2 + efs2,
                 diff + cfd1 +  efpm2 + efs2)
    elif crude_vs in index_dub:
        pass
    return df_prices


index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
[f() for index, f in func_list.items() if index == index_region][0]
df_prices.columns
df_freight.columns
df_freight = construct_freight()
df_freight
[s for s in df_prices.columns if 'vs_' in s]
[s for s in df_prices.columns if 'vs_' in s].iat[0]
[s for s in df_prices.columns if 'vs_' in s].values
[s for s in df_prices.columns if 'vs_' in s][0]
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
def create_landed_values():
    index = [s for s in df_prices.columns if 'vs_' in s][0]
    #for i in df_prices.columns:
    for k in df_freight.columns:
        name = k[:4]+"_"+index
        df_prices[name] = df_prices[index] + df_freight[k]
    return df_prices

create_landed_values()
def create_landed_values():
    index = [s for s in df_prices.columns if 'vs_' in s][0]
    #for i in df_prices.columns:
    for k in df_freight.columns:
        df_prices[k] = df_freight[k]
        name = k[:4]+"_"+index
        df_prices[name] = df_prices[index] + df_freight[k]
    return df_prices

create_landed_values()
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight.drop(['Rate'], axis=1)


def convert_prices():
    """depending on discharge, choose the appropriate index"""
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['vs_wti'] = np.where(expiry_condition,
                     diff + (wtim1 -  wtim2),
                     diff)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim2'] = wtim2
            df_prices['efpm2'] = efpm2
            df_prices['brentm2'] = brentm2
            df_prices['vs_wti']  = np.where(cfd_condition,
                     diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                     diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['brentm2'] = brentm2
            df_prices['efpm2'] = efpm2
            df_prices['vs_dtd'] = np.select(conditions, choices)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['vs_dtd'] = np.where(cfd_condition,
                     diff + (4 * (cfd2 - cfd1) / 14),
                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_dtd'] = diff
        return df_prices
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['brentm2'] = brentm2
            df_prices['efs2'] = efs2
            df_prices['vs_dub'] = np.where(expiry_condition,
                     diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                     diff - (brentm2-wtim2) + efs2)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['efpm2'] = efpm2
            df_prices['efs2'] = efs2
            df_prices['vs_dub'] = np.where(cfd_condition,
                     diff + cfd2 +  efpm2 + efs2,
                     diff + cfd1 +  efpm2 + efs2)
        elif crude_vs in index_dub:
            pass
        return df_prices
    
    index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
    func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
    [f() for index, f in func_list.items() if index == index_region][0]
    return df_prices


def create_landed_values():
    index = [s for s in df_prices.columns if 'vs_' in s][0]
    #for i in df_prices.columns:
    for k in df_freight.columns:
        df_prices[k] = df_freight[k]
        name = k[:4]+"_"+index
        df_prices[name] = df_prices[index] + df_freight[k]
    return df_prices

df_freight = construct_freight()
df_prices = convert_prices()
create_landed_values()
arb_values.column
convert_prices()
df_prices = convert_prices()
df_prices
create_landed_values()
df_prices = create_landed_values(
df_prices = create_landed_values()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
ports_with_rates
crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
destinations = ['Houston']
arb_values = arb(i,k, *var)
arb_values = arb(crudes,destinations, *var)
arb_values = arb('Brent','Houston', *var)
arb_values
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
gpw = standard_ref('Brent','Houston', *var)
gpw
arb_values.columns
arb_values
arb_values['gpw'] = standard_ref('Brent','Houston', *var)
arb_values
index = [index for index in df_prices.columns if 'vs_' in index][0]
index
[index for index in arb_values.columns if 'vs_' in index]
[index for index in arb_values.columns if '_vs_' in index]
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    arb_values['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['gpw'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_arb['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
destinations
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    arb_values['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['gpw'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_arb['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['gpw'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
global_arb = make_arbs(crudes, destinations, *var)
global_arb
print(global_arb.head())
global_arb = make_arbs('WTI Midland', 'Houston', *var)
global_arb = make_arbs('Brent', 'Houston', *var)
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    arb_values['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['gpw'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_arb['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['gpw'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs('Brent', 'Houston', *var)
global_arb = make_arbs(['Brent'], ['Houston'], *var)
global_arb
df_freight
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            else:
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight.drop(['Rate'], axis=1)

ub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes
total[ws_codes[ws_codes['Code'] == i]['Code'].values]
def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight = df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
    return df_freight

calculate_freight()
def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
    return df_freight

calculate_freight()
calculate_flat_rate()
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

calculate_flat_rate()
df_freight = pd.DataFrame(index=df.index)
calculate_flat_rate()
calculate_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight

df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BREN, DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight


def convert_prices():
    """depending on discharge, choose the appropriate index"""
    def convert_wti():
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['vs_wti'] = np.where(expiry_condition,
                     diff + (wtim1 -  wtim2),
                     diff)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim2'] = wtim2
            df_prices['efpm2'] = efpm2
            df_prices['brentm2'] = brentm2
            df_prices['vs_wti']  = np.where(cfd_condition,
                     diff + cfd2 +  efpm2 + (brentm2 - wtim2),
                     diff + cfd1 +  efpm2 + (brentm2 - wtim2))
        
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_wti'] = diff
        return df_prices
    
    def convert_dtd():
        conditions = [(expiry_condition & cfd_condition),
                      (expiry_condition & np.invert(cfd_condition)),
                      (np.invert(expiry_condition) & cfd_condition),
                      (np.invert(expiry_condition) & np.invert(cfd_condition))]
        choices = [(diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff +  (wtim1 -  wtim2) - (brentm2 - wtim2) - efpm2 - cfd1),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd2),
                   (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
        
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['brentm2'] = brentm2
            df_prices['efpm2'] = efpm2
            df_prices['vs_dtd'] = np.select(conditions, choices)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['vs_dtd'] = np.where(cfd_condition,
                     diff + (4 * (cfd2 - cfd1) / 14),
                     diff + (4 * (cfd1 - cfd2) / 14)) # the 4 will be replaces by the journey time between thge two ports
        elif crude_vs in index_dub:
            pass
        else:
            df_prices['vs_dtd'] = diff
        return df_prices
    
    def convert_dub():
        if crude_vs in index_wti:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['wtim1'] = wtim1
            df_prices['wtim2'] = wtim2
            df_prices['brentm2'] = brentm2
            df_prices['efs2'] = efs2
            df_prices['vs_dub'] = np.where(expiry_condition,
                     diff + (wtim1 -  wtim2)  - (brentm2-wtim2) + efs2,
                     diff - (brentm2-wtim2) + efs2)
        elif crude_vs in index_dtd:
            df_prices['diff'] = diff
            df_prices['cfd1'] = cfd1
            df_prices['cfd2'] = cfd2
            df_prices['efpm2'] = efpm2
            df_prices['efs2'] = efs2
            df_prices['vs_dub'] = np.where(cfd_condition,
                     diff + cfd2 +  efpm2 + efs2,
                     diff + cfd1 +  efpm2 + efs2)
        elif crude_vs in index_dub:
            pass
        return df_prices
    
    index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
    func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
    [f() for index, f in func_list.items() if index == index_region][0]
    return df_prices


def create_landed_values():
    index = [index for index in df_prices.columns if 'vs_' in index][0]
    #for i in df_prices.columns:
    for k in df_freight.columns:
        df_prices[k] = df_freight[k]
        name = k[:4]+"_"+index
        df_prices[name] = df_prices[index] + df_freight[k]
    return df_prices
    
    #landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
    #return landed, df_landed


df_freight = construct_freight()
df_prices = convert_prices()
#landed, df_landed = create_landed_values()
df_prices = create_landed_values()
return df_prices
df_freight = construct_freight()
df_freight
convert_prices()
index
index_region
df_freight = construct_freight()
df_freight
convert_prices()
index
df_freight.columns
[freight for freight in df_freight.columns if 'max' in freight]
[price_index for price_index in df_prices.columns if 'vs_' in price_index]
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)
df_freight = construct_freight()
df_freight
[f() for index, f in func_list.items() if index == index_region]
[f() for index, f in func_list.items() if index == index_region][0]
[price_index for price_index in df_prices.columns if 'vs_' in price_index]
def create_landed_values():
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    #for i in df_prices.columns:
    for k in [freight for freight in df_freight.columns if 'max' in freight]
        df_prices[k] = df_freight[k]
        name = k[:4]+"_"+price_index
        df_prices[name] = df_prices[price_index] + df_freight[k]
    return df_prices

def create_landed_values():
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    #for i in df_prices.columns:
    for k in [freight for freight in df_freight.columns if 'max' in freight]:
        df_prices[k] = df_freight[k]
        name = k[:4]+"_"+price_index
        df_prices[name] = df_prices[price_index] + df_freight[k]
    return df_prices

df_prices = convert_prices()
df_prices
df_freight
df_prices = create_landed_values()
df_prices
df_freight
df_prices
[freight for freight in df_freight.columns if 'max' in freight]
df_freight = construct_freight()
df_prices = convert_prices()
df_prices
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)
df_freight = construct_freight()
df_prices = convert_prices()
df_prices
df_freight
df_prices = pd.concat([df_prices,df_freight], axis=1)
df_prices
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
price_index
[f() for index, f in func_list.items() if index == index_region]
[freight for freight in df_freight.columns if 'max' in freight]
def create_landed_values():
    df_prices = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight]
    #for i in df_prices.columns:
    for k in freight_list:
        name = k[:4]+"_landed_"+price_index
        df_prices[name] = df_prices[price_index] + df_freight[k]            
    return df_prices

df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)
df_freight = construct_freight()
df_prices = convert_prices()
df_prices = create_landed_values()
def create_landed_values(df_prices):
    df_prices = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight]
    #for i in df_prices.columns:
    for k in freight_list:
        name = k[:4]+"_landed_"+price_index
        df_prices[name] = df_prices[price_index] + df_freight[k]            
    return df_prices
    
    #landed = pd.concat([df_prices,df_freight, df_landed], axis=1)
    #return landed, df_landed


df_freight = construct_freight()
df_prices = convert_prices()
#landed, df_landed = create_landed_values()
df_prices = create_landed_values(df_prices)
df_prices
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
crudes = list(test_crudes.index.values)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
crudes = list(test_crudes.index.values)
crudes
destinations = ['Houston']
global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
global_arb.columns
global_arb['Brent']['Houston']
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
crudes = list(test_crudes.index.values)
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
crudes = list(test_crudes.index.values)
crudes
destinations = ['Houston','Rotterdam','Augusta']
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    arb_values['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['gpw'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_arb['gpw'] = standard_ref(i,k, *var)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['gpw'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
crudes
global_arb
global_arb.head()
price_codes = {'HOUSTON':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'},
    'ROTTERDAM':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'},
    'AUGUSTA':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'},
    'SINGAPORE':{'propane':'PCALB00',
                  'butane':'ASGCB00',
                  'naphtha':'AASAV00',
                  'gasoline':'AAGXH00',
                  'kero':'AAWEY00',
                  'ulsd':'AASAT00',
                  'gasoil':'AAUFL00',
                  'lsfo':'AAHPM00'}
                 }

pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   

yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j

yields = {}  
yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
      + reformer_output['propane'] 
      + fcc_output['propane'] 
      + fcc_output['c3__'] * 0.5
      ) / refinery_volume

yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
       + reformer_output['butane'] 
       + fcc_output['butane']
       + fcc_output['c3__'] * 0.5
       ) / refinery_volume

yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
        + reformer_output['surplus_for_naphtha']
        ) / refinery_volume

#yields['btx'] = (reformer_output['btx']) / refinery_volume

yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer():
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc():
    fcc_output = {}
    fcc_assay_standard = {
        'c3': {'yield': 0.1},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def coker():
    pass


def alkylation_unit():
    pass


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21

refinery_volume = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
pricing_centre
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j

yield_codes.keys()
total[list(yield_codes.keys())].fillna(method='bfill')
product_conversion = {'propane':11,
                  'butane':11,
                  'naphtha':8.9,
                  'gasoline':8.34,
                  'kero':7.87,
                  'ulsd':7.45,
                  'gasoil':7.45,
                  'lsfo':6.32}
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':11},
                  'butane':{'Code':'PMAAI00', 'Conversion':11},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':8.34},
                  'kero':{'Code':'PJABP00', 'Conversion':7.87},
                  'ulsd':{'Code':'AATGX00', 'Conversion':7.45},
                  'gasoil':{'Code':'POAED00', 'Conversion':7.45},
                  'lsfo':{'Code':'POAED00', 'Conversion':6.32}},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':8.9},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':8.34},
                  'kero':{'Code':'PJABF00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }
prices_codes.keys()
price_codes.keys()
price_codes.items()
[print(i) for i in price_codes.items()]
price_codes.items()
[print(i) for i in price_codes[i]
[print(i) for i in price_codes[i]]
[print(i) for i in price_codes[i]]
ports
price_codes
[print(i) for i in price_codes[i]]
price_codes['HOUSTON']
price_codes['HOUSTON']['propane']['code']
price_codes['HOUSTON']['propane']['Code']
price_codes['HOUSTON']['propane']['Conversion']
yields
pricing_centre
i
total[price_codes[pricing_centre][i]['Code']]
total
price_codes[pricing_centre][i]['Code']
[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]
yields.items()
{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for i, j in yields.items()}
{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
ws
ws_table
total
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
temp = pd.read_excel(data, 'price_warehouse', header = 1)
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
temp = pd.read_excel(data, 'price_warehouse', header = 1)
temp = pd.read_excel(data, 'price_warehouse', header = 4)
temp
temp = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
temp
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna('ffil')
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna('ffill')
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
total.index
total.index = pd.to_datetime(total.index)
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j


regional_price_set = total[list(yield_codes.keys())].fillna(method='bfill')
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
gpw = pd.DataFrame(gpw, columns = ['GPW'])
yields.items()
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]] = j

yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes][pricing_centre][i] = j

yields.items()
[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()] 
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre]][i] = j

price_codes[pricing_centre][i]
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j

yield_codes
yields.items()
total[list(yield_codes.keys())]
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
regional_price_set = total[list(yield_codes.keys())]
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set).sum(1)
gpw
regional_price_set
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='bfill')
total.index = pd.to_datetime(total.index)
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill').dropna(axis=0, how = 'all')
total
pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill').dropna(axis=0, how = 'all')
total[list(yield_codes.keys())]
total[list(yield_codes.keys())].dropna(axis=0)
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)
otal[list(yield_codes.keys())]
total[list(yield_codes.keys())]
total[list(yield_codes.keys())].dropna(axis=0)
regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
yield_codes
yields.items()
conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']

conversion_codes
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set)

conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


gpw = gpw.assign(**conversion_codes).div(gpw)
gpw
pricing_centre
(100/42)
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':11},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':8.34},
                  'kero':{'Code':'PJABP00', 'Conversion':7.87},
                  'ulsd':{'Code':'AATGX00', 'Conversion':7.45},
                  'gasoil':{'Code':'POAED00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':6.32}},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':8.9},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':8.34},
                  'kero':{'Code':'PJABF00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set)

conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


gpw = gpw.assign(**conversion_codes).div(gpw)
gpw
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':11},
                  'butane':{'Code':'PMAAI00', 'Conversion':11},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':8.34},
                  'kero':{'Code':'PJABP00', 'Conversion':7.87},
                  'ulsd':{'Code':'AATGX00', 'Conversion':7.45},
                  'gasoil':{'Code':'POAED00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':6.32}},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':8.9},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':8.34},
                  'kero':{'Code':'PJABF00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }


pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
price_codes['HOUSTON']['propane']['Code']
price_codes['HOUSTON']['propane']['Conversion']

price_codes[pricing_centre][i]['Code']

"""The below gets us our list of prroduct price codes"""
[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  

{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}







yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set)

conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


gpw = gpw.assign(**conversion_codes).div(gpw)
yield_codes
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':11},
                  'butane':{'Code':'PMAAI00', 'Conversion':11},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':8.34},
                  'kero':{'Code':'PJABP00', 'Conversion':7.87},
                  'ulsd':{'Code':'AATGX00', 'Conversion':7.45},
                  'gasoil':{'Code':'POAED00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':6.32}},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':8.9},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':8.34},
                  'kero':{'Code':'PJABF00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }


pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   


"""The below gets us our list of prroduct price codes"""
#[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  

#{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}







yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set)

conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


gpw = gpw.assign(**conversion_codes).div(gpw)
gpw
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':50},
                  'butane':{'Code':'PMAAI00', 'Conversion':11},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':8.34},
                  'kero':{'Code':'PJABP00', 'Conversion':7.87},
                  'ulsd':{'Code':'AATGX00', 'Conversion':7.45},
                  'gasoil':{'Code':'POAED00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':6.32}},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':8.9},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':8.34},
                  'kero':{'Code':'PJABF00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }


pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   


"""The below gets us our list of prroduct price codes"""
#[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  

#{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}







yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set)

conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


gpw = gpw.assign(**conversion_codes).div(gpw)
gpw
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1}},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }


pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   


"""The below gets us our list of prroduct price codes"""
#[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  

#{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}







yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set)

conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


gpw = gpw.assign(**conversion_codes).div(gpw)
gpw['GPW'] = gpw.sum(1)
gpw
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
standard_ref('Azeri','Houston', *var)  
standard_ref('Azeri','Houston', *var) 
*var
var
total
total[list(yield_codes.keys())]
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()
standard_ref('Azeri','Houston', *var) 
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
print(global_arb.head())
global_arb['Brent']['Houston']
global_arb['Brent']['Houston'].head()
global_arb.columns
mike = global_arb['Brent']['Houston'].head()
mike
mike = global_arb['Brent']['Houston'].tail()
mike
mike = global_arb['Azeri']['Houston'].tail()
mike = global_arb['Azeri']['Rotterdam'].tail()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
crude_check
crudes = list(test_crudes.index.values)
destinations = ['Houston','Rotterdam','Augusta','Singapore']
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb['Ekofisk']['Houston'].tail()
global_arb['Ekofisk']['Houston'].loc[-5:,:]
global_arb.columns
mike = global_arb['Ekofisk']['Houston'].loc[:-5,:]
mike = global_arb['Ekofisk']['Houston'].iloc[:-5]
mike
global_arb['Ekofisk']['Houston'].iloc[:-5]
global_arb['Ekofisk']['Houston']
global_arb['Ekofisk']['Houston'].iloc[:-5]
mike = global_arb['Ekofisk']['Houston'].iloc[-10:]
mike
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlsx')
global_arb.to_excel(writer, 'Arbs')
writer.save()
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlsm')
global_arb.to_excel(writer, sheet_name='Arbs', engine='io.excel.xlsm.writer')
writer.save()
crude_vs
assay
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False)
sub_region
sub_to_ws
ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'] =='MED'
ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'] == ports[ports['Name'] == destination]['Subregion']
ports[ports['Name'] == destination]['Subregion']
ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
sub_region == sub_region2
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
sub_region == sub_region_2
sub_region == 'NWE' and sub_region_2 == 'MED'
crude_vs
crude_vs in ['btc dated']
ports
ports[ports['Name'] == assay[crude]['LoadPort']]['Country']
ports[ports['Name'] == assay[crude]['LoadPort']]['Country'] in ['Iraq']
ports[ports['Name'] == assay[crude]['LoadPort']]['Country']
ports[ports['Name'] == assay[crude]['LoadPort']]['Country'] in ['Iraq']
crude_vs in ['wti cma']
crude_vs
ports[ports['Name'] == assay[crude]['LoadPort']]['Country'].iat[0]
ports[ports['Name'] == assay[crude]['LoadPort']]['Country'].iat[0] in ['Iraq']
assay
assay[crude]['LoadPort']
destinations = ['Houston','Rotterdam','Augusta','Singapore']
test_crudes
crudes = list(test_crudes.index.values)
global_arb = make_arbs(crudes, destinations, *var)
from ArbEcons2504 import arb
from GPW2504 import standard_ref
global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
from ArbEcons2504 import arb
global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
#global_arb.columns
#mike = global_arb['Ekofisk']['Houston'].iloc[-10:]


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlsm')
global_arb.to_excel(writer, sheet_name='Arbs', engine='io.excel.xlsm.writer')
writer.save()
from ArbEcons2504 import arb
arb_values = arb('Es Sider','Singapore', *var)
pass
arb_values = arb('Es Sider','Singapore', *var)
arb_values = arb('Azeri','Singapore', *var)
arb_values
arb_values = arb('Azeri','Rotterdam', *var)
arb_values = arb('Es Sider','Rotterdam', *var)
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
index_region
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
sub_to_ws
cfd8
dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]
days = 10
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
total
crude = 'Es Sider'
destination = 'Singapore'
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values(df_prices):
        df_prices = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            df_prices[name] = df_prices[price_index] + df_freight[k]            
        return df_prices
    
    df_freight = construct_freight()
    df_prices = convert_prices()
    df_prices = create_landed_values(df_prices)
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    crude = 'Es Sider'
    destination = 'Singapore'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values(df_prices):
        df_prices = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            df_prices[name] = df_prices[price_index] + df_freight[k]            
        return df_prices
    
    df_freight = construct_freight()
    df_prices = convert_prices()
    df_prices = create_landed_values(df_prices)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
from ArbEcons2504 import arb
from GPW2504 import standard_ref
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
#global_arb.columns
#mike = global_arb['Ekofisk']['Houston'].iloc[-10:]


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlsm')
global_arb.to_excel(writer, sheet_name='Arbs', engine='io.excel.xlsm.writer')
writer.save()
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlxm')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
df_freight
destination = 'Singapore'
crude = 'Es Sider'
calculate_flat_rate()
calculate_freight()
ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region.map(sub_to_ws[1]).to_string(index = False)
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region.map(sub_to_ws[1]).to_string(index = False)
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
sub_region_2
ws
crude = 'Mars'
calculate_flat_rate()
calculate_freight()
df_freight = construct_freight()
df_freight
df_prices = convert_prices()
df_prices
df_freight = construct_freight()
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    crude = 'Mars'
    destination = 'Singapore'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values(df_prices):
        df_prices = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            df_prices[name] = df_prices[price_index] + df_freight[k]            
        return df_prices
construct_freight()
df_freight = construct_freight()
convert_prices()
df_prices = convert_prices()
df_prices
create_landed_values(df_prices)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
#global_arb.columns
#mike = global_arb['Ekofisk']['Houston'].iloc[-10:]


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
total
crude = 'Mars'
crude = 'Mars'
destination = 'Singapore'
calculate_flat_rate()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
        return df_freight

calculate_flat_rate()
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight


def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
    return df_freight

calculate_flat_rate()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
    return df_freight

calculate_flat_rate()
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
calculate_flat_rate()
crude = 'Mars'
destination = 'Singapore'
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

"""Declare the main prices that will be used in order to use shorthand notation"""
dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]
days = 10
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
calculate_flat_rate()
calculate_freight()
df_freight
construct_freight()
flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
          (rate_data['DischargePort'] == destination)]
flat_rate_table
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes
for i in list(ws_codes['Code']):
    size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
    name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
    if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
        df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
        df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        df_freight.drop(['Rate'], axis=1)
    else:
        df_freight[name] = total[i]
        df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']  

df_freight
df_freight = calculate_flat_rate()
df_freight = calculate_freight()
df_freight
arb_values = arb('Mars', 'Singapore', *var)
arb_values = arb('Mars', 'Rotterdam', *var)
df_prices
convert_prices()
    crude = 'Mars'
    destination = 'Singapore'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        df_freight = calculate_flat_rate()
        df_freight = calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values(df_prices):
        df_prices = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            df_prices[name] = df_prices[price_index] + df_freight[k]            
        return df_prices
construct_freight()
df_freight = construct_freight()
    crude = 'Mars'
    destination = 'Singapore'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values(df_prices):
        df_prices = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            df_prices[name] = df_prices[price_index] + df_freight[k]            
        return df_prices
df_freight = construct_freight()
df_freight
df_prices = convert_prices()
df_prices
create_landed_values()
def create_landed_values():
    df_prices = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight]
    #for i in df_prices.columns:
    for k in freight_list:
        name = k[:4]+"_landed_"+price_index
        df_prices[name] = df_prices[price_index] + df_freight[k]            
    return df_prices

create_landed_values()
def create_landed_values():
    temp = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight]
    #for i in df_prices.columns:
    for k in freight_list:
        name = k[:4]+"_landed_"+price_index
        temp[name] = df_prices[price_index] + df_freight[k]            
    return temp

create_landed_values()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
arb('Mars', 'Rotterdam', *var)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
#global_arb.columns
#mike = global_arb['Ekofisk']['Houston'].iloc[-10:]


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
print(global_arb.head())
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import standard_ref
var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
destinations = ['Houston','Rotterdam','Augusta','Singapore']
def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = standard_ref(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = standard_ref(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
df_freight
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values():
        temp = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' or 'vlcc' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            temp[name] = df_prices[price_index] + df_freight[k]            
        return temp
    crude = 'Brent'
    destination = 'Singapore'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values():
        temp = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' or 'vlcc' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            temp[name] = df_prices[price_index] + df_freight[k]            
        return temp
[freight for freight in df_freight.columns if 'max' or 'vlcc' in freight]
df_freight
df_freight = construct_freight()
df_freight
[freight for freight in df_freight.columns if 'max' in freight]
[freight for freight in df_freight.columns if 'max' or 'VLCC' in freight]
[freight for freight in df_freight.columns if 'max' in freight]
[freight for freight in df_freight.columns if 'max' in freight or if 'VLCC' in freight]
[freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
price_index
global_arb['Eagle Ford']['Rotterdam']
global_arb['Eagle Ford']['Rotterdam'].tail()
df_prices
    crude = 'Eagle Ford'
    destination = 'Rotterdam'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 10
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
df_prices
df_freight = construct_freight()
df_freight
construct_freight()
convert_prices()
create_landed_values()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
global_arb
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs2.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
refinery_volume = None,
lvn_gasolinepool = None,
kero_gasolinepool = None,
reformer_capacity = None
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21

if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

refinery_volume
refinery_volume = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.21

def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume

def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool

def reformer():
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output

def fcc():
    fcc_output = {}
    fcc_assay_standard = {
        'c3': {'yield': 0.1},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output

def coker():
    pass


def alkylation_unit():
    pass


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

ef calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields
def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields

def calculate_margin():
    """this is to create the look up neccessary for generating the table"""
    price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                      'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                      'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                      'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                      'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                      'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                      'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                      'lsfo':{'Code':'PUAAO00', 'Conversion':1}},
        'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                      'butane':{'Code':'PMAAK00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                      'kero':{'Code':'PJABA00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
        'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                      'butane':{'Code':'PMAAM00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                      'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
        'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                      'butane':{'Code':'AAJTT00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                      'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                      'kero':{'Code':'PJABF00', 'Conversion':1},
                      'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                      'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                      'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                     }
    
    """Choose which set of products we use based on discharge locations"""   
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
    
    
    """The below gets us our list of prroduct price codes"""
    #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
    
    #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
    
    
    
    
    
    
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    gpw = regional_price_set.assign(**yield_codes).mul(regional_price_set)
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    gpw = gpw.assign(**conversion_codes).div(gpw)
    gpw['GPW'] = gpw.sum(1)
    #gpw = pd.DataFrame(gpw, columns = ['GPW'])
    #gpw.columns = pd.MultiIndex.from_product([[crude],[destination], gpw.columns])
    return gpw

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from ArbEcons2504 import import_data

crude = 'Azeri'
destination = 'Houston'
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
reformer_output['propane']
reformer_output['butane']
def fcc():
    fcc_output = {}
    fcc_assay_standard = {
        'c3': {'yield': 0.01},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
fcc_output['gasoline']
utilised_fcc_cap
fcc_capacity
fcc_capacity = refinery_volume*0.23
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
yields['gasoline']
fcc_capacity * 0.97
fcc_capacity = refinery_volume*0.24
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
reformer_capacity
utilised_ref_cap
utilised_ref_cap = reformer_capacity * 0.97
utilised_ref_cap
reformer_input
hvn_input
kero_input
assay[crude]['LVN']
hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input + lvn_input
vgo_input = assay[crude]['VGO'] * refinery_volume
def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input + lvn_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j

"""this is to create the look up neccessary for generating the table"""
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1}},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }

"""Choose which set of products we use based on discharge locations"""   
pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   


"""The below gets us our list of prroduct price codes"""
#[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  

#{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}







yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j

yield_codes
regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
regional_price_set
yield_codes
conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']

conversion_codes
regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
regional_price_set
gpw = regional_price_set.assign(**conversion_codes).div(regional_price_set)
gpw
regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
regional_price_set.assign(**conversion_codes)
regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)
gpw
conversion_codes
gpw
temp_df = regional_price_set.assign(**yield_codes)
temp_df
gpw = gpw.mul(temp_df)
gpw
gpw['GPW'] = gpw.sum(1)
gpw
yields
{'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                  'base_blend':'BaseBlendUSGC'}
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                  'base_blend':'BaseBlendUSGC'},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32}},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32}},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32}}
                 }
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                  'base_blend':'BaseBlendUSGC'},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                  'base_blend':'BaseBlendNWE'},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                  'base_blend':'BaseBlendMED'},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                  'base_blend':'BaseBlendASIA'}
                 }
pricing_centre
price_codes[pricing_centre]['base_blend']
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude=base_blend)
base_blend = price_codes[pricing_centre]['base_blend']
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude=base_blend)
crude = price_codes[pricing_centre]['base_blend']
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
crude = 'Azeri'
destination = 'Rotterdam'
pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)  
crude = price_codes[pricing_centre]['base_blend']
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
cdu()
crude = price_codes[pricing_centre]['base_blend']
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
crude = price_codes[pricing_centre]['base_blend']
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()

yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df)

gpw['GPW'] = gpw.sum(1)
gpw
crude = price_codes[pricing_centre]['base_blend']
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()

yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df)

gpw['Base_Blend_GPW'] = gpw.sum(1)
gpw
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
gpw = calculate_margin()
gpw
def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input + lvn_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer():
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc():
    fcc_output = {}
    fcc_assay_standard = {
        'c3': {'yield': 0.01},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def coker():
    pass


def alkylation_unit():
    pass


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields


def calculate_margin():
    """this is to create the look up neccessary for generating the table"""
    price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                      'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                      'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                      'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                      'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                      'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                      'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                      'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                      'base_blend':'BaseBlendUSGC'},
        'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                      'butane':{'Code':'PMAAK00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                      'kero':{'Code':'PJABA00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                      'base_blend':'BaseBlendNWE'},
        'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                      'butane':{'Code':'PMAAM00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                      'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                      'base_blend':'BaseBlendMED'},
        'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                      'butane':{'Code':'AAJTT00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                      'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                      'kero':{'Code':'PJABF00', 'Conversion':1},
                      'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                      'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                      'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                      'base_blend':'BaseBlendASIA'}
                     }
    
    """Choose which set of products we use based on discharge locations"""   
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
    
    
    
    
    def calculate_base_blend():
        crude = price_codes[pricing_centre]['base_blend']
        refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
        reformer_output = reformer()
        fcc_output = fcc()
        diluent = fo_blend()
        yields = calculate_yields()
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]['Code']] = j
        
        conversion_codes = {}
        for i, j in yields.items():
            conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
        
        regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
        temp_df = regional_price_set.assign(**conversion_codes)
        gpw = regional_price_set.div(temp_df)
        
        temp_df = regional_price_set.assign(**yield_codes)
        gpw = gpw.mul(temp_df)
        
        gpw['Base_Blend_GPW'] = gpw.sum(1)
        return gpw['Base_Blend_GPW']
    
    
    
    
    
    """The below gets us our list of prroduct price codes"""
    #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
    
    #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    
    gpw['GPW'] = gpw.sum(1)
    gpw['Base_Blend_GPW'] = calculate_base_blend()
    return gpw






refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
gpw = calculate_margin()
gpw
calculate_base_blend()
def calculate_base_blend():
    crude = price_codes[pricing_centre]['base_blend']
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    
    gpw['Base_Blend_GPW'] = gpw.sum(1)
    return gpw['Base_Blend_GPW']

calculate_base_blend()
calculate_margin()
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from ArbEcons2504 import import_data

crude = 'Azeri'
destination = 'Rotterdam'
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
refinery_volume = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input + lvn_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool


def reformer():
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def fcc():
    fcc_output = {}
    fcc_assay_standard = {
        'c3': {'yield': 0.01},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output


def coker():
    pass


def alkylation_unit():
    pass


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields


def calculate_margin():
    """this is to create the look up neccessary for generating the table"""
    price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                      'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                      'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                      'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                      'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                      'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                      'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                      'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                      'base_blend':'BaseBlendUSGC'},
        'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                      'butane':{'Code':'PMAAK00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                      'kero':{'Code':'PJABA00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                      'base_blend':'BaseBlendNWE'},
        'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                      'butane':{'Code':'PMAAM00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                      'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                      'base_blend':'BaseBlendMED'},
        'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                      'butane':{'Code':'AAJTT00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                      'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                      'kero':{'Code':'PJABF00', 'Conversion':1},
                      'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                      'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                      'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                      'base_blend':'BaseBlendASIA'}
                     }
    
    """Choose which set of products we use based on discharge locations"""   
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
    
    
    
    
    def calculate_base_blend():
        crude = price_codes[pricing_centre]['base_blend']
        refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
        reformer_output = reformer()
        fcc_output = fcc()
        diluent = fo_blend()
        yields = calculate_yields()
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]['Code']] = j
        
        conversion_codes = {}
        for i, j in yields.items():
            conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
        
        regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
        temp_df = regional_price_set.assign(**conversion_codes)
        gpw = regional_price_set.div(temp_df)
        
        temp_df = regional_price_set.assign(**yield_codes)
        gpw = gpw.mul(temp_df)
        
        gpw['Base_Blend_GPW'] = gpw.sum(1)
        return gpw['Base_Blend_GPW']
    
    
    
    
    
    """The below gets us our list of prroduct price codes"""
    #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
    
    #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    
    gpw['GPW'] = gpw.sum(1)
    gpw['Base_Blend_GPW'] = calculate_base_blend()
    return gpw

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
reformer_output = reformer()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
gpw = calculate_margin()
gpw
calculate_base_blend()
def calculate_base_blend():
    crude = price_codes[pricing_centre]['base_blend']
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    
    gpw['Base_Blend_GPW'] = gpw.sum(1)
    return gpw['Base_Blend_GPW']

calculate_base_blend()
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                  'base_blend':'BaseBlendUSGC'},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                  'base_blend':'BaseBlendNWE'},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                  'base_blend':'BaseBlendMED'},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                  'base_blend':'BaseBlendASIA'}
                 }

"""Choose which set of products we use based on discharge locations"""   
pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
calculate_base_blend()
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df)

gpw['GPW'] = gpw.sum(1)
gpw['Base_Blend_GPW'] = calculate_base_blend()
gpw
crude = price_codes[pricing_centre]['base_blend']
crude
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu(crude)
crude = price_codes[pricing_centre]['base_blend']
cdu()
crude = 'Azeri'
cdu()
def standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None):
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.24
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def coker():
        pass
    
    def alkylation_unit():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    def calculate_margin():
        """this is to create the look up neccessary for generating the table"""
        price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                          'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                          'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                          'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                          'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                          'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                          'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                          'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                          'base_blend':'BaseBlendUSGC'},
            'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                          'butane':{'Code':'PMAAK00', 'Conversion':11},
                          'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                          'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                          'kero':{'Code':'PJABA00', 'Conversion':7.87},
                          'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                          'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                          'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                          'base_blend':'BaseBlendNWE'},
            'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                          'butane':{'Code':'PMAAM00', 'Conversion':11},
                          'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                          'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                          'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                          'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                          'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                          'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                          'base_blend':'BaseBlendMED'},
            'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                          'butane':{'Code':'AAJTT00', 'Conversion':11},
                          'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                          'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                          'kero':{'Code':'PJABF00', 'Conversion':1},
                          'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                          'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                          'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                          'base_blend':'BaseBlendASIA'}
                         }
        
        """Choose which set of products we use based on discharge locations"""   
        pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False)   
        
        """The below gets us our list of prroduct price codes"""
        #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
        
        #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]['Code']] = j
        
        conversion_codes = {}
        for i, j in yields.items():
            conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
        
        regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
        temp_df = regional_price_set.assign(**conversion_codes)
        gpw = regional_price_set.div(temp_df)
        
        temp_df = regional_price_set.assign(**yield_codes)
        gpw = gpw.mul(temp_df)
        
        gpw['GPW'] = gpw.sum(1)
        return gpw
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    gpw = calculate_margin()
    return gpw

standard_ref(crude = price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
standard_ref(price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
standard_ref('Azeri', destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
def standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None):
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.24
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def coker():
        pass
    
    def alkylation_unit():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    return yields

standard_ref(price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
yields
yield_codes
yield_codes.items(
yield_codes.items()
yields
yield_codes
yields
yields.items()
yields
yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]] = price_codes[pricing_centre][i]['Code']

i
price_codes[pricing_centre][i]['Code']
price_codes[pricing_centre][i]
yield_names = {}
for i, j in yields.items():
    yield_names[i] = price_codes[pricing_centre][i]['Code']

yield_names
sub_to_ws
yield_names = {}
for i, j in yields.items():
    price_codes[pricing_centre][i]['Code'] = yield_names[i]

yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i

yield_names
gpw
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df)
gpw
yield_names
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df).map(yield_names)
gpw.columns.values
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df).map(yield_names)
gpw.rename(columns=yield_names, inplace=True)
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df)
gpw.rename(columns=yield_names, inplace=True)
gpw
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j


yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i


conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']


regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
temp_df = regional_price_set.assign(**conversion_codes)
gpw = regional_price_set.div(temp_df)

temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df)
gpw.rename(columns=yield_names, inplace=True)

gpw['GPW'] = gpw.sum(1)
gpw
standard_ref(price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
standard_ref('Azeri', destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
crude_yields = standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
crude_yields
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
base_yields
calculate_margin(base_yields)
def calculate_margin(yields):
    
    """The below gets us our list of prroduct price codes"""
    #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
    
    #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    yield_names = {}
    for i, j in yields.items():
        yield_names[price_codes[pricing_centre][i]['Code']] = i
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    gpw.rename(columns=yield_names, inplace=True)
    
    gpw['GPW'] = gpw.sum(1)
    return gpw

calculate_margin(base_yields)
calculate_margin(crude_yields)
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from ArbEcons2504 import import_data

crude = 'Azeri'
destination = 'Rotterdam'
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()

def standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None):
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.24
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def coker():
        pass
    
    def alkylation_unit():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    return yields


def calculate_margin(yields):
    
    """The below gets us our list of prroduct price codes"""
    #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
    
    #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    yield_names = {}
    for i, j in yields.items():
        yield_names[price_codes[pricing_centre][i]['Code']] = i
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    gpw.rename(columns=yield_names, inplace=True)
    
    gpw['GPW'] = gpw.sum(1)
    return gpw

"""this is to create the look up neccessary for generating the table"""
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                  'base_blend':'BaseBlendUSGC'},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                  'base_blend':'BaseBlendNWE'},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                  'base_blend':'BaseBlendMED'},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                  'base_blend':'BaseBlendASIA'}
                         }

"""Choose which set of products we use based on discharge locations"""   
pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 

base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)

base_crude = calculate_margin(base_yields)

crude_yields = standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)

crude_gpw = calculate_margin(crude_yields)
crude_gpw['Base_GPW'] = base_crude['GPW']
crude_gpw
crude_gpw = gpw_calculation(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df)
def gpw_calculation(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df):
    
    def standard_ref(refinery_volume = None,
                     lvn_gasolinepool = None,
                     kero_gasolinepool = None,
                     reformer_capacity = None,
                     fcc_capacity = None):
        if refinery_volume is None:
            refinery_volume = 200
        if lvn_gasolinepool is None:
            lvn_gasolinepool = 0.12
        if kero_gasolinepool is None:
            kero_gasolinepool = 0.15
        if reformer_capacity is None:
            reformer_capacity = refinery_volume*0.21
        if fcc_capacity is None:
                fcc_capacity = refinery_volume*0.24
        
        def cdu():
            ### Fractions for gaosline pool       
            hvn_input = assay[crude]['HVN'] * refinery_volume
            kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
            lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
            reformer_input = hvn_input + kero_input + lvn_input
            vgo_input = assay[crude]['VGO'] * refinery_volume
            
            return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
        
        def reformer():
            reformer_output = {}
            reformer_assay_standard = {
                'c3': {'yield': 0.025},
                'c4': {'yield': 0.025},
                'h2': {'yield': 0.0},
                'btx': {'yield': 0.0},
                'gasoline': {'yield': 0.95}
                }
            utilised_ref_cap = reformer_capacity * 0.97
            reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
            reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
            reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
            reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
            reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
            reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
            reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
            reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
            
            return reformer_output
        
        def fcc():
            fcc_output = {}
            fcc_assay_standard = {
                'c3': {'yield': 0.01},
                'c4': {'yield': 0.01},
                'c3__': {'yield': 0.02},
                'lco': {'yield': 0.20},
                'clo': {'yield': 0.20},
                'gasoline': {'yield': 0.56}
                }
            
            utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
            fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
            
            fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
            fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
            fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
            fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
            fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
            fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
            fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
            
            return fcc_output
        
        def coker():
            pass
        
        def alkylation_unit():
            pass
        
        def fo_blend():
            residue = {}
            residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
            
            # HDS removing suplhur from the bbl before processing
            low_HDS_factor = 0.5
            high_HDS_factor = 0.8
            if assay[crude]['RESIDUE_sulphur'] < 0.035:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
            else:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
            
            residue['density'] = assay[crude]['RESIDUE_density'] 
            residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
            residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
            
            # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
            imported_vgo = 0  
            diluent_used = 0       
            diluent = {
                    'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                    'sulphur': 0.001,
                    'density': 0.897,
                    'viscosity': 3,
                    'lco_for_blending': 0,
                    'target_sulphur': 0,
                    'volume_used': diluent_used
                    }
            
            diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
            diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
            
            # create the 'cost' function - what do we want to minimise?
            def target_viscosity(diluent_used):
                return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
            
            # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
            cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                     {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
            
            # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
            res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
            diluent['volume_used'] = res.x[0]
            diluent['cst'] = res.fun
            
            # =============================================================================
            # Left blank for the addition of if vgo is needed to be imported
            # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
            # 
            # if target_viscosity(diluent_used) > 380:
            #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
            #     
            # =============================================================================
            
            diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
            diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
            diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
            residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
            residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
            
            combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
            combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
            diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
            
            return diluent
        
        def calculate_yields():
            yields = {}  
            yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
                  + reformer_output['propane'] 
                  + fcc_output['propane'] 
                  + fcc_output['c3__'] * 0.5
                  ) / refinery_volume
            
            yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                   + reformer_output['butane'] 
                   + fcc_output['butane']
                   + fcc_output['c3__'] * 0.5
                   ) / refinery_volume
            
            yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                    + reformer_output['surplus_for_naphtha']
                    ) / refinery_volume
            
            #yields['btx'] = (reformer_output['btx']) / refinery_volume
            
            yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
            yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
            yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
            yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
            yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
            
            return yields
        
        
        
        refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
        reformer_output = reformer()
        fcc_output = fcc()
        diluent = fo_blend()
        yields = calculate_yields()
        return yields
    
    def calculate_margin(yields):
        
        """The below gets us our list of prroduct price codes"""
        #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
        
        #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]['Code']] = j
        
        yield_names = {}
        for i, j in yields.items():
            yield_names[price_codes[pricing_centre][i]['Code']] = i
        
        conversion_codes = {}
        for i, j in yields.items():
            conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
        
        regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
        temp_df = regional_price_set.assign(**conversion_codes)
        gpw = regional_price_set.div(temp_df)
        
        temp_df = regional_price_set.assign(**yield_codes)
        gpw = gpw.mul(temp_df)
        gpw.rename(columns=yield_names, inplace=True)
        
        gpw['GPW'] = gpw.sum(1)
        return gpw
    
    
    """this is to create the look up neccessary for generating the table"""
    price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                      'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                      'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                      'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                      'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                      'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                      'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                      'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                      'base_blend':'BaseBlendUSGC'},
        'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                      'butane':{'Code':'PMAAK00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                      'kero':{'Code':'PJABA00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                      'base_blend':'BaseBlendNWE'},
        'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                      'butane':{'Code':'PMAAM00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                      'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                      'base_blend':'BaseBlendMED'},
        'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                      'butane':{'Code':'AAJTT00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                      'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                      'kero':{'Code':'PJABF00', 'Conversion':1},
                      'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                      'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                      'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                      'base_blend':'BaseBlendASIA'}
                             }
    
    """Choose which set of products we use based on discharge locations"""   
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 
    
    base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                     refinery_volume = None,
                     lvn_gasolinepool = None,
                     kero_gasolinepool = None,
                     reformer_capacity = None,
                     fcc_capacity = None)
    
    base_crude = calculate_margin(base_yields)
    
    crude_yields = standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                     refinery_volume = None,
                     lvn_gasolinepool = None,
                     kero_gasolinepool = None,
                     reformer_capacity = None,
                     fcc_capacity = None)
    
    crude_gpw = calculate_margin(crude_yields)
    crude_gpw['Base_GPW'] = base_crude['GPW']
    return crude_gpw

gpw_calculation(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df)
def standard_ref(crude, refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None):
    if refinery_volume is None:
        refinery_volume = 200
    if lvn_gasolinepool is None:
        lvn_gasolinepool = 0.12
    if kero_gasolinepool is None:
        kero_gasolinepool = 0.15
    if reformer_capacity is None:
        reformer_capacity = refinery_volume*0.21
    if fcc_capacity is None:
            fcc_capacity = refinery_volume*0.24
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def fcc():
        fcc_output = {}
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input)
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    def coker():
        pass
    
    def alkylation_unit():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume) / refinery_volume
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    return yields

standard_ref(price_codes[pricing_centre]['base_blend'], destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)

standard_ref('Azeri', destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)


def calculate_base_blend():
    crude = price_codes[pricing_centre]['base_blend']
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool = cdu()
    reformer_output = reformer()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    
    gpw['Base_Blend_GPW'] = gpw.sum(1)
    return gpw['Base_Blend_GPW']

base_yields = standard_ref(price_codes[pricing_centre]['base_blend'],
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
crude_yields = standard_ref(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df,
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
crude_gpw
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW_delta'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW_delta'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(['Azeri'], destinations, *var)
global_arb
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs4.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders')
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
pd.read_excel(trader_assessed, 'Crude Diffs Traders')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = None)
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = None)
crude_diffs[0]
crude_diffs.index = crude_diffs[0]
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index = crude_diffs[0]
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0).drop(1)
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0).drop([1])
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0).drop(['NaT'])
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0).drop(['NaT'], axis=0)
crude_diffs
df.index['NaT'
df.index['NaT']
crude_diffs = crude_diffs.drop(crude_diffs.index['NaT'])
crude_diffs.index
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs.index
crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
crude_diffs.reindex(dcrude_diffs.index).fillna(method='ffill')
crude_diffs.reindex(crude_diffs.index).fillna(method='ffill')
crude_diffs.reindex(days).fillna(method='ffill')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
crude_diffs.resample('D')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D')
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
mike = crude_diffs.resample('D')
mike
mike = crude_diffs.resample('D').mean()
mike
mike = crude_diffs.resample('D').interpolate()
mike
mike = crude_diffs.resample('D').interpolate().fillna(method='ffill')
mike
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
mike = crude_diffs.resample('D').interpolate().fillna(method='bfill')
mike
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)






assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
total
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
crude_diffs
crude_diffs.columns
[name for name in crude_diffs.columns if 'Unnamed' in name]
[~name for name in crude_diffs.columns if 'Unnamed' in name]
[name for name in crude_diffs.columns if 'Unnamed' not in name]
mike = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' not in name], axis=1)
mike
[name for name in crude_diffs.columns if 'Unnamed' not in name]
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
mike = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
mike
total
mike = total[[name for name in crude_diffs.columns if 'Unnamed' in name]]
mike = total[[name for name in crude_diffs.columns if 'Unnamed' not in name]]
total = total.update(crude_diffs)
total[[name for name in crude_diffs.columns if 'Unnamed' not in name]].head()
mike
mike['AAQZA00']
total['AAQZA00']
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
total
mike
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
total.update(crude_diffs)
total
james = total.update(crude_diffs)
james
crude_diffs.index is in total.index
crude_diffs.index in total.index
crude_diffs[crude_diffs.index in total.index]
crude_diffs[[crude_diffs.index in total.index]]
crude_diffs.index
crude_diffs.index[total.index]
total.index
crude_diffs.index[crude_diffs.index == total.index]
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs.index.loc[crude_diffs.index == total.index]
crude_diffs.index.iloc[crude_diffs.index == total.index]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs.loc[crude_diffs.index == total.index]
crude_diffs.loc[crude_diffs.index == total.index]
crude_diffs.index
total.index
crude_diffs.loc[crude_diffs.index in total.index]
crude_diffs.index
crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs[total.index.isin(crude_diffs.index)]
crude_diffs.index.isin(total.index)
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
total.update(crude_diffs)
total
total['AAQZA00']
crude_diffs
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs
total['AAGXF00']
james = total.update(crude_diffs)
james['AAGXF00']
crude_diffs.columns
list(crude_diffs.columns)
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
james = total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
james
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
otal.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
total
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
total
total['AAGXF00']
total.index.isin(crude_diffs.index)
crude_diffs
total['AAGXF00']
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
total.loc[total.index > dt(2017,06,30), list(crude_diffs.columns)]
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)]
from datetime import datetime as dt
import time
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)]
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)] - (total['PCAAQ00'] - total['PCAAR00'])
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)]
(total['PCAAQ00'] - total['PCAAR00'])
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)] 
(total['PCAAQ00'] - total['PCAAR00'])
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)] - (total['PCAAQ00'] - total['PCAAR00'])
otal.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)].subtract(pd.Series(total['PCAAQ00'] - total['PCAAR00']))
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)].subtract(pd.Series(total['PCAAQ00'] - total['PCAAR00']))
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)]
pd.Series(total['PCAAQ00'] - total['PCAAR00'])
total.loc[total.index > dt(2017,6,30), list(crude_diffs.columns)].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
cfd_list = ['PCAKG00','AAGLV00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
total.loc[total.index > dt(2017,6,30), cfd_list] = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
total.loc[total.index > dt(2017,6,30), cfd_list]
total.loc[total.index > dt(2017,6,30), cfd_list] = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
total.loc[total.index > dt(2017,6,30), cfd_list]
total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp[temp.index > dt(2017,6,30)]
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index > dt(2017,6,30), cfd_list] = temp
temp
total.loc[total.index > dt(2017,6,30), cfd_list] 
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

destinations = ['Houston','Rotterdam','Augusta','Singapore']

def make_arbs(crudes,destinations, *var):
    counter = 0
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW_delta'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW_delta'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
#print(global_arb.head())
#global_arb['Eagle Ford']['Rotterdam'].tail()


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs4.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
cfd_list = ['PCAKG00','AAGLV00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index > dt(2017,6,30), cfd_list] = tem
total.loc[total.index > dt(2017,6,30), cfd_list] = temp
temp
total.loc[total.index > dt(2017,6,30), cfd_list]
temp
cfd_list = ['PCAKG00','AAGLV00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
temp
total.loc[total.index > dt(2017,6,30), cfd_list] = temp
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp
cfd_list = ['PCAKG00','AAGLV00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
temp
total.loc[total.index > dt(2017,6,30), cfd_list] = temp
total.loc[total.index > dt(2017,6,30), cfd_list]
temp
total
temp
lstDates=total.index[total.index > dt(2017,6,30)]
total.loc[lstDates, cfd_list] = temp[lstDates]
total[lstDates, cfd_list] = temp[lstDates]
temp[lstDates]
lstDates
temp
total.loc[lstDates, cfd_list] = temp.loc[lstDates,cfd_list]
total
cfd_list = ['PCAKG00','AAGLV00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index > dt(2017,6,30), cfd_list] = temp.values
total
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
rude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
crude_diffs[list(crude_diffs.columns)]
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)]
crude_diffs.index
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
total
total.loc[total.index > dt(2018,4,1)]
total.loc[total.index > dt(2018,4,1), list(crude_diffs.columns)]
total.loc[total.index > dt(2017,6,30), cfd_list]
pd.Series(total['PCAAQ00'] - total['PCAAR00']
pd.Series(total['PCAAQ00'] - total['PCAAR00'])
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
total.loc[total.index > dt(2017,6,30), cfd_list]
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp
temp = temp[temp.index > dt(2017,6,30)]
temp
total.loc[total.index > dt(2017,6,30), cfd_list] = temp[cfd_list]
temp
total.loc[total.index > dt(2017,6,30), cfd_list] = temp[total.index > dt(2017,6,30), cfd_list]
total.loc[total.index > dt(2017,6,30), cfd_list] = temp[temp.index > dt(2017,6,30), cfd_list]
total.loc[total.index > dt(2017,6,30), cfd_list] = temp.loc[temp.index > dt(2017,6,30), cfd_list]
total.loc[total.index.isin(temp.index), cfd_list] = temp[cfd_list]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
temp
total
temp = total.loc[total.index > dt(2017,6,30), cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
total.loc[total.index > dt(2017,6,30), cfd_list]
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp
temp = temp[temp.index > dt(2017,6,30)]
temp
total
temp
total
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs
total.index.isin(crude_diffs.index),
list(crude_diffs.columns)
total.index.isin(temp.index),
list(temp.columns)
temp[list(temp.columns)]
total.loc[total.index.isin(temp.index), list(temp.columns)]
temp[list(temp.columns)]
list(temp.columns)
temp[list(temp.columns)]
temp
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
total
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
def make_arbs(crudes,destinations, *var):
    counter = 0
    #gpw_landed = arb
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW_delta'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW_delta'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

base_blends = {'ROTTERDAM':{'Brent':0.4,'Urals':0.3,'Forties':0.4},
        'AUGUSTA':{'Brent':0.4,'Urals':0.3,'Forties':0.4},
        'HOUSTON':{'Brent':0.4,'Urals':0.3,'Forties':0.4},
        'SINGAPORE':{'Brent':0.4,'Urals':0.3,'Forties':0.4}}
base_blends['ROTTERDAM']
base_blends.items()
base_blends['ROTTERDAM'].items()
base_blends.items()[0][0]
base_blends.items()
base_blends['ROTTERDAM'].items()
base_blends.items()[0]
base_blends.keys()
base_blends['ROTTERDAM'].keys()
base_blends['ROTTERDAM'].values()
arb('Brent','ROTTERDAM',*var)
arb('Brent','Rotterdam',*var)
arb('Brent','Rotterdam',*var).loc[:,-2]
arb('Brent','Rotterdam',*var).loc[:,2]
arb('Brent','Rotterdam',*var)[:,2]
arb('Brent','Rotterdam',*var).iloc[:,2]
arb('Brent','Rotterdam',*var).iloc[:,3]
arb('Brent','Rotterdam',*var).iloc[:,-2]
arb('Brent','Rotterdam',*var).iloc[:,:-2]
arb('Brent','Rotterdam',*var).iloc[:,-2:]
freight_list = [freight for frieght in mike.columns if 'max' in freight or 'VLCC' in freight]
mike = arb('Brent','Rotterdam',*var)
freight_list = [freight for frieght in mike.columns if 'max' in freight or 'VLCC' in freight]
freight_list = [freight for freight in mike.columns if 'max' in freight or 'VLCC' in freight]
freight_list
mike
freight_list = [freight for freight in mike.columns if 'landed' in freight]
freight_list
mike = arb('Brent','Rotterdam',*var)[freight for freight in mike.columns if 'landed' in freight]
mike[freight_list]
mike[freight_list].mean(axis=0)
mike[freight_list].mean(axis=1)
mike = arb('Brent','Rotterdam',*var)
freight_list = [freight for freight in mike.columns if 'landed' in freight]
base_landed['Brent'] = mike[freight_list].mean(axis=1)
base_landed
mike = arb('Brent','Rotterdam',*var)
freight_list = [freight for freight in mike.columns if 'landed' in freight]
base_landed = pd.DataFrame()
base_landed['Brent'] = mike[freight_list].mean(axis=1)
base_landed
base_landed['Brent'] = mike[freight_list].mean(axis=1) * 0.4
base_landed
base_blends['ROTTERDAM'].values()
base_blends['Rotterdam']['Brent']
base_blends = {'Rotterdam':{'Brent':0.4,'Urals':0.3,'Forties':0.4},
        'Augusta':{'Brent':0.4,'Urals':0.3,'Forties':0.4},
        'Houston':{'Brent':0.4,'Urals':0.3,'Forties':0.4},
        'Singapore':{'Brent':0.4,'Urals':0.3,'Forties':0.4}}
base_blends['Rotterdam']['Brent']
base_landed.sum(1)
base_blends.keys()
base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.4},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.4}}
for j in base_blends.keys():
    base_costs = pd.DataFrame()
    for i in base_blends[j].keys():
        base_landed = pd.DataFrame()
        arb_frame = arb(i,j,*var)
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
        base_costs[j] = base_landed.sum(1)

base_costs
base_blends.keys()
base_costs = pd.DataFrame()
for j in base_blends.keys():    
    for i in base_blends[j].keys():
        base_landed = pd.DataFrame()
        arb_frame = arb(i,j,*var)
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
        base_costs[j] = base_landed.sum(1)

base_costs
base_cost = base_costs['Rotterdam'].rename({'Rotterdam':'Base_blend_landed'})
base_cost
base_costs['Rotterdam'].rename({'Rotterdam':'Base_blend_landed'})
base_cost = base_costs['Rotterdam'].rename(columns = {'Rotterdam':'Base_blend_landed'})
base_cost
base_cost = base_costs['Rotterdam'].rename(columns = {'Rotterdam':'Base_blend_landed'}, axis = 'columns')
base_cost
base_cost['x'] = base_costs['Rotterdam']
base_cost
base_costs
base_costs['Rotterdam']
base_cost['x'] = base_costs['Rotterdam']
base_cost
base_cost = base_costs['Rotterdam'].rename(columns = {0:'Base_blend_landed'}, axis = 'columns')
base_cost
base_cost = base_costs['Rotterdam'].rename({0:'Base_blend_landed'})
base_cost
base_cost = base_costs['Rotterdam'].rename({'Rotterdam':'Base_blend_landed'})
base_cost
base_cost = base_costs['Rotterdam'].rename(columns={'Rotterdam':'Base_blend_landed'})
base_cost
base_costs['Rotterdam']
base_costs
base_costs['Rotterdam'].name
base_cost = base_costs['Rotterdam'].rename(columns={'Rotterdam':'Base_blend_landed'}, inplace=True)
base_cost
base_cost.name
base_cost
base_cost = base_costs['Rotterdam']
base_cost.name
base_costs
base_costs = pd.DataFrame()
for j in base_blends.keys():    
    for i in base_blends[j].keys():
        base_landed = pd.DataFrame()
        arb_frame = arb(i,j,*var)
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
        base_costs[j] = base_landed.sum(1)

base_costs
type(base_costs)
base_cost = base_costs['Rotterdam']
type(base_cost)
base_cost.columns
base_cost.name = 'mike'
base_cost
crudes = ['Brent','Azeri','CPC Blend']
def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       arb_values[name] = arb_values['GPW_delta'] - arb_values[j]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       next_arb[name] = next_arb['GPW_delta'] - next_arb[j]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb
base_costs
arb_values
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_vs_' in index]:
   name = j[:4]+"_margin"
   arb_values[name] = arb_values['GPW_delta'] - arb_values[j]

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
arb_values
arb_values.columns
arb_values.head()
arb_values['Rott_base_landed']
arb_values.columns
arb_values['Azeri']['Rott_base_landed']
arb_values['Azeri']['Rotterdam']['Rott_base_landed']
[index for index in arb_values.columns if '_vs_' in index]
arb_values
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_vs_' in index]:
   name = j[:4]+"_margin"
   arb_values[name] = arb_values['GPW_delta'] - arb_values[j]

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.head()
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
arb_values
arb_values.columns
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed_vs_'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_vs_' in index]:
   name = j[:4]+"_margin"
   arb_values[name] = arb_values['GPW_delta'] - arb_values[j]

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.head()
arb_values['Azeri']['Rotterdam']['Rott_base_landed']
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed_vs_'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_vs_' in index]:
   name = j[:4]+"_margin"
   arb_values[name] = arb_values['GPW_delta'] - arb_values[j]

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.head()
arb_values['Azeri']['Rotterdam']['Rott_base_landed_vs_']
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed_vs_'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
arb_values.columns
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   arb_values[name] = arb_values['GPW_delta'] - arb_values[j]

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
arb_values.columns
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
[index for index in arb_values.columns if '_landed' in index]
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in name:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j]
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] 

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.head()
arb_values['Azeri']['Rotterdam']['Rott_base_landed_vs_']
arb_values.columns
arb_values
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values():
        temp = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            temp[name] = df_prices[price_index] + df_freight[k]            
        return temp
    
    df_freight = construct_freight()
    df_prices = convert_prices()
    df_prices = create_landed_values()
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    crude = 'Azeri'
    destination = 'Rotterdam'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    dtd = total['PCAAS00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    crude_vs = assay[crude]['Index'].lower().strip()
    diff = total[assay[crude]['Code']]
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values():
        temp = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            temp[name] = df_prices[price_index] + df_freight[k]            
        return temp
    
    df_freight = construct_freight()
    df_prices = convert_prices()
    df_prices = create_landed_values()
df_prices
df_freight = construct_freight()
df_freight
df_prices = convert_prices()
df_prices
f_prices = convert_prices()
df_prices = convert_prices()
df_prices
index_region
crude = 'Azeri'
destination = 'Rotterdam'
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2']]

"""Declare the main prices that will be used in order to use shorthand notation"""
dtd = total['PCAAS00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
crude_vs = assay[crude]['Index'].lower().strip()
diff = total[assay[crude]['Code']]
days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
df_prices
df_freight = construct_freight()
df_freight
convert_prices()
temp = pd.concat([df_prices,df_freight], axis=1)
[price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
[price_index for price_index in df_prices.columns if 'vs_' in price_index]
[freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in name:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j]
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] 

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.head()
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in name:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j]
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] 

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.tail()
arb_values
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in name:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] 

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.tail()
[index for index in arb_values.columns if '_landed' in index]
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
arb_values
[index for index in arb_values.columns if '_landed' in index]
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
[index for index in arb_values.columns if '_landed' in index]
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in name:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

arb_values.columns
arb_values.tail()
arb_values
destinations = ['Rotterdam','Augusta']
crudes = ['Brent','Azeri','CPC Blend']
def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_vs_' in index]:
                        name = j[:4]+"_margin"
                        if 'base' in name:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_vs_' in index]:
                       name = j[:4]+"_margin"
                       if 'base' in name:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb
global_arb.columns
destinations = ['Rotterdam','Augusta']
crudes = ['Brent','Azeri','CPC Blend']

def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        if 'base' in name:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       if 'base' in name:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb.columns
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in name:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
arb_values.columns
base_costs = pd.DataFrame()
for j in base_blends.keys():    
    for i in base_blends[j].keys():
        base_landed = pd.DataFrame()
        arb_frame = arb(i,j,*var)
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
        base_costs[j] = base_landed.sum(1)

base_costs
[column_header for column_header in arb_frame.columns if 'landed' in column_header]
base_blends[j][i]
base_costs = pd.DataFrame()
for j in base_blends.keys():    
    for i in base_blends[j].keys():
        base_landed = pd.DataFrame()
        arb_frame = arb(i,j,*var)
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
        base_costs[j] = base_landed.sum(1)

base_costs
for i in crudes:
    for k in destinations:
        try:
            if counter == 0:
                arb_values = arb(i,k, *var)
                base = k[:4]+'_base_landed'
                arb_values[base] = base_costs[k]
                gpw = gpw_calculation(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                for j in [index for index in arb_values.columns if '_landed' in index]:
                    name = j[:4]+"_margin"
                    if 'base' in name:
                        arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                    else:
                        arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                counter +=1

crudes
destinations
for i in crudes:
    for k in destinations:
        try:
            if counter == 0:
                arb_values = arb(i,k, *var)
                base = k[:4]+'_base_landed'
                arb_values[base] = base_costs[k]
                gpw = gpw_calculation(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                for j in [index for index in arb_values.columns if '_landed' in index]:
                    name = j[:4]+"_margin"
                    if 'base' in name:
                        arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                    else:
                        arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

for i in crudes:
    for k in destinations:
        try:
            if counter == 0:
                arb_values = arb(i,k, *var)
                base = k[:4]+'_base_landed'
                arb_values[base] = base_costs[k]
                gpw = gpw_calculation(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                for j in [index for index in arb_values.columns if '_landed' in index]:
                    name = j[:4]+"_margin"
                    if 'base' in name:
                        arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                    else:
                        arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

for i in crudes:
    for k in destinations:
        #try:
            if counter == 0:
                arb_values = arb(i,k, *var)
                base = k[:4]+'_base_landed'
                arb_values[base] = base_costs[k]
                gpw = gpw_calculation(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                for j in [index for index in arb_values.columns if '_landed' in index]:
                    name = j[:4]+"_margin"
                    if 'base' in name:
                        arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                    else:
                        arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

counter = 0

base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
    'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}

"""generate the landed base blends for each region"""
base_costs = pd.DataFrame()
for j in base_blends.keys():    
    for i in base_blends[j].keys():
        base_landed = pd.DataFrame()
        arb_frame = arb(i,j,*var)
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
        base_costs[j] = base_landed.sum(1)


for i in crudes:
    for k in destinations:
        #try:
            if counter == 0:
                arb_values = arb(i,k, *var)
                base = k[:4]+'_base_landed'
                arb_values[base] = base_costs[k]
                gpw = gpw_calculation(i,k, *var)
                arb_values = pd.concat([arb_values,gpw], axis=1)
                for j in [index for index in arb_values.columns if '_landed' in index]:
                    name = j[:4]+"_margin"
                    if 'base' in name:
                        arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                    else:
                        arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.columns
destinations = ['Rotterdam','Augusta']
crudes = ['Brent','Azeri','CPC Blend']

def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        if 'base' in name:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       if 'base' in name:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
global_arb.columns
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs5.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
base_blends[j].keys()
[index for index in arb_values.columns if '_landed' in index]
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
[index for index in arb_values.columns if '_landed' in index]
destinations = ['Rotterdam','Augusta']
crudes = ['Brent','Azeri','CPC Blend']

def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        if 'base' in j:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       if 'base' in j:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
global_arb
global_arb.columns
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs5.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in j:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])

arb_values.tail()
arb_values['Azeri']['Rotterdam']['Rott_base_landed_vs_']
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
[index for index in arb_values.columns if '_landed' in index]
[margins for margins in arb_values.columns if '_margin' in margins and 'base' in margins]
arb_values.columns
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   if 'base' in j:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

[margins for margins in arb_values.columns if '_margin' in margins and 'base' in margins]
arb_values.columns
[margins for margins in arb_values.columns if 'margin' in margins and 'base' not in margins]
[margins for margins in arb_values.columns if 'margin' in margins and 'base' not in margins][0]
[margins for margins in arb_values.columns if 'Afra' in margins or 'Suez'in margins or 'VLCC' in margins]
[margins for margins in arb_values.columns if 'Afra' in margins or 'Suez'in margins or 'VLCC' in margins and 'margin' in margins]
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
margin_headers = []
for j in [index for index in arb_values.columns if '_landed' in index]:
   margin_headers.append(j)
   name = j[:4]+"_margin"
   if 'base' in j:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

margin_headers
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
margin_headers = []
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   margin_headers.append(name)
   if 'base' in j:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']

margin_headers
range(3)
for i in range(3):
    print(i)

for i in range(len(margin_headers):
    print(i)

for i in range(len(margin_headers)):
    print(i)

for i in range(len(margin_headers)-1):
    print(i)

for i in range(len(margin_headers)-1):
    print(margin_headers[i])
    print(i)

len(margin_headers)-1
for i in range(len(margin_headers)-1):
    name = i+"_adv"
    arb_values[name] = arb_values[margin_headers[i]] - arb_values[margin_headers[-1]]

name = i+"_adv"
i
for i in range(len(margin_headers)-1):
    name = i+'_adv'

for i in range(len(margin_headers)-1):
    name = margin_headers[i]+'_adv'

name
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
margin_headers = []
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   margin_headers.append(name)
   if 'base' in j:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']


for i in range(len(margin_headers)-1):
    name = margin_headers[i]+'_advantage'
    arb_values[name] = arb_values[margin_headers[i]] - arb_values[margin_headers[-1]]

arb_values.columns
destinations = ['Rotterdam','Augusta']
crudes = ['Brent','Azeri','CPC Blend']

def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        margin_headers.append(name)
                        if 'base' in j:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    for i in range(len(margin_headers)-1):
                        name = margin_headers[i]+'_advantage'
                        arb_values[name] = arb_values[margin_headers[i]] - arb_values[margin_headers[-1]]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       margin_headers.append(name)
                       if 'base' in j:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    for i in range(len(margin_headers)-1):
                       name = margin_headers[i]+'_advantage'
                       arb_values[name] = arb_values[margin_headers[i]] - arb_values[margin_headers[-1]]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        margin_headers.append(name)
                        if 'base' in j:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    for i in range(len(margin_headers)-1):
                        name = margin_headers[i]+'_advantage'
                        arb_values[name] = arb_values[margin_headers[i]] - arb_values[margin_headers[-1]]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       margin_headers.append(name)
                       if 'base' in j:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    for i in range(len(margin_headers)-1):
                       name = margin_headers[i]+'_advantage'
                       next_arb[name] = next_arb[margin_headers[i]] - next_arb[margin_headers[-1]]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
i = 'Azeri'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var)
arb_values = pd.concat([arb_values,gpw], axis=1)
margin_headers = []
for j in [index for index in arb_values.columns if '_landed' in index]:
   name = j[:4]+"_margin"
   margin_headers.append(name)
   if 'base' in j:
       arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
   else:
       arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']


for i in range(len(margin_headers)-1):
    name = margin_headers[i]+'_advantage'
    arb_values[name] = arb_values[margin_headers[i]] - arb_values[margin_headers[-1]]




arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
arb_values.columns
destinations = ['Rotterdam','Augusta']
crudes = ['Brent','Azeri','CPC Blend']
global_arb = make_arbs(crudes, destinations, *var)
global_arb
destinations = ['Rotterdam','Augusta']
crudes = ['Brent','Azeri','CPC Blend']

def make_arbs(crudes,destinations, *var):
    counter = 0
    
    base_blends = {'Rotterdam':{'Brent':0.4,'Ekofisk':0.3,'Forties':0.3},
        'Augusta':{'Azeri':0.4,'Es Sider':0.3,'CPC Blend':0.3}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        margin_headers.append(name)
                        if 'base' in j:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    for x in range(len(margin_headers)-1):
                        name = margin_headers[x]+'_advantage'
                        arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       margin_headers.append(name)
                       if 'base' in j:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    for x in range(len(margin_headers)-1):
                       name = margin_headers[x]+'_advantage'
                       next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs5.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input + lvn_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    coker_input = assay[crude]['RESIDUE'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input

assay[crude]['RESIDUE'] 
assay[crude]['HGO']
coker_output['lgo'])
coker_output['lgo']
coker_output = coker()
def coker():
    coker_output = {}
    coker_assay_standard = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lvn': {'yield': 0.09},
        'hvn': {'yield': 0.11},
        'lgo': {'yield': 0.15},
        'hgo': {'yield': 0.35}
        }
    utilised_coker_cap = coker_capacity * 0.97
    coker_volume = np.minimum(utilised_coker_cap, coker_input)
    
    coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
    coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
    coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
    coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
    coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
    coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
    
    return coker_output

def coker():
    coker_output = {}
    coker_assay_standard = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lvn': {'yield': 0.09},
        'hvn': {'yield': 0.11},
        'lgo': {'yield': 0.15},
        'hgo': {'yield': 0.35}
        }
    utilised_coker_cap = coker_capacity * 0.97
    coker_volume = np.minimum(utilised_coker_cap, coker_input)
    
    coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
    coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
    coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
    coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
    coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
    coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
    
    return coker_output

coker_output = coker()
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

if coker_capacity is None:
        coker_capacity = refinery_volume*0.24

coker_capacity = None
if coker_capacity is None:
        coker_capacity = refinery_volume*0.24

refinery_volume = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
coker_capacity = None
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

if coker_capacity is None:
        coker_capacity = refinery_volume*0.24

coker_output = coker()
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
reformer_output = reformer()
def reformer():
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output



def coker():
    coker_output = {}
    coker_assay_standard = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lvn': {'yield': 0.09},
        'hvn': {'yield': 0.11},
        'lgo': {'yield': 0.15},
        'hgo': {'yield': 0.35}
        }
    utilised_coker_cap = coker_capacity * 0.97
    coker_volume = np.minimum(utilised_coker_cap, coker_input)
    
    coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
    coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
    coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
    coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
    coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
    coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
    
    return coker_output



def fcc():
    fcc_output = {}
    fcc_assay_standard = {
        'c3': {'yield': 0.01},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    fcc_assay_usgc = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lco': {'yield': 0.12},
        'clo': {'yield': 0.08},
        'gasoline': {'yield': 0.75}
        }
    
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay_standard['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay_standard['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay_standard['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay_standard['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay_standard['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay_standard['gasoline']['yield'] * fcc_volume
    
    return fcc_output



def alkylation_unit():
    pass


def hydrocracker():
    pass


def fo_blend():
    residue = {}
    residue['volume'] = assay[crude]['RESIDUE'] * refinery_volume
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO'] + coker_output['lgo']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields




refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
reformer_output = reformer()
coker_output = coker()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
return yields
from scipy.optimize import minimize
import numpy as np 
import pandas as pd
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
reformer_output = reformer()
coker_output = coker()
coker_output
reformer_output
fcc_output
if coker_capacity is None and destination is 'Houston':
        coker_capacity = refinery_volume*0.24
else:
    coker_capacity = 0

destination
global_arb = make_arbs(crudes, destinations, *var)
global_arb
reformer_input
utilised_ref_cap
reformer_capacity
utilised_ref_cap = reformer_capacity * 0.97
np.minimum(reformer_input - utilised_ref_cap, kero_input)
reformer_input - utilised_ref_cap
kero_input
reformer_input
utilised_ref_cap
reformer_input - utilised_ref_cap
np.minimum(reformer_input - utilised_ref_cap, kero_input)
kero_input
hvn_input
reformer_input - utilised_ref_cap
kero_input
reformer_output['surplus_for_jet']
destination
ports[ports['Name'] == destination]['Subregion']
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2])
sub_to_ws
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
codes_missing
not_in_targo
no_flat_rates
var[4]
var[0]
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
codes_missing
not_in_targo
no_flat_rates
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Documents//crude list.xlsx')
crude_list = pd.read_excel(data, 'Crudes')
crude_list
crude_list['Monthly Average' not in crude_list['Name (or Code)']]
crude_list.loc['Monthly Average' not in crude_list['Name (or Code)']]
crude_list['Name (or Code)']
crude_list.loc['Monthly Average' in crude_list['Name (or Code)']]
crude_list[crude_list['Name (or Code)'].str.contains('Monthly Average')]
crude_list.loc[crude_list['Name (or Code)'].str.contains('Monthly Average')]
crude_list = crude_list.loc[~crude_list['Name (or Code)'].str.contains('Monthly Average')]
crude_list
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Documents//crude list.xlsx')
crude_list = pd.read_excel(data, 'Crudes')
crudes_no_ma = crude_list.loc[~crude_list['Name (or Code)'].str.contains('Monthly Average')]
crudes_only_ma = crude_list.loc[crude_list['Name (or Code)'].str.contains('Monthly Average')]
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Documents//crude list.xlsx')
crude_list = pd.read_excel(data, 'Crudes')

crude_list.loc['Monthly Average' in crude_list['Name (or Code)']]

crudes_no_ma = crude_list.loc[~crude_list['Name (or Code)'].str.contains('Monthly Average')]
crudes_only_ma = crude_list.loc[crude_list['Name (or Code)'].str.contains('Monthly Average')]

writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs6.xlsx')
crudes_no_ma.to_excel(writer, sheet_name='No Monthly Avg')
crudes_only_ma.to_excel(writer, sheet_name='Only Monthly Avg')
writer.save()
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Documents//crude list.xlsx')
crude_list = pd.read_excel(data, 'Crudes')


crudes_no_ma = crude_list.loc[~crude_list['Name (or Code)'].str.contains('Monthly Average')]
crudes_only_ma = crude_list.loc[crude_list['Name (or Code)'].str.contains('Monthly Average')]

writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs6.xlsx')
crudes_no_ma.to_excel(writer, sheet_name='No Monthly Avg')
crudes_only_ma.to_excel(writer, sheet_name='Only Monthly Avg')
writer.save()
import pandas as pd

data = pd.ExcelFile('C://Users//mima//Documents//crude list.xlsx')
crude_list = pd.read_excel(data, 'Crudes')


crudes_no_ma = crude_list.loc[~crude_list['Name (or Code)'].str.contains('Monthly Average')]
crudes_only_ma = crude_list.loc[crude_list['Name (or Code)'].str.contains('Monthly Average')]

writer = pd.ExcelWriter('C://Users//mima//Documents//crude codes.xlsx')
crudes_no_ma.to_excel(writer, sheet_name='No Monthly Avg')
crudes_only_ma.to_excel(writer, sheet_name='Only Monthly Avg')
writer.save()
sub_to_ws
sub_region
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
discharge_price_region
exceptions = {
        'Arab Extra Light':{'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'}},
        'Arab Extra Light':{'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'}},
        'Arab Extra Light':{'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'}},
        'Arab Extra Light':{'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':{'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'}},
        'Arab Light':{'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'}},
        'Arab Light':{'HOUSTON':{'Code':'AAIRA00','Index':'WTI'}},
        'Arab Light':{'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':{'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'}},
        'Arab Medium':{'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'}},
        'Arab Medium':{'HOUSTON':{'Code':'AAIRB00','Index':'WTI'}},
        'Arab Medium':{'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':{'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'}},
        'Arab Heavy':{'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'}},
        'Arab Heavy':{'HOUSTON':{'Code':'AAIRC00','Index':'WTI'}},
        'Arab Heavy':{'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':{'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'}},
        'Basrah Light':{'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'}},
        'Basrah Light':{'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'}},
        'Basrah Light':{'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':{'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'}},
        'Basrah Heavy':{'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'}},
        'Basrah Heavy':{'HOUSTON':{'Code':'AAXUE00','Index':'Mars'}},
        'Basrah Heavy':{'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':{'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'}},
        'Iranian Heavy':{'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'}},
        #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Iranian Heavy':{'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':{'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'}},
        'Iranian Light':{'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'}},
        #'Iranian Light':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Iranian Light':{'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':{'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'}},
        'Forozan':{'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'}},
        #'Forozan':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Forozan':{'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'}},
        'Isthmus':{'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'}},
        'Isthmus':{'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'}},
        'Isthmus':{'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'BWAVE'}},
        'Maya':{'AUGUSTA':{'Code':'AAIQB00','Index':'BWAVE'}},
        'Maya':{'HOUSTON':{'Code':'AAIPY00','Index':'WTI'}},
        'Maya':{'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }
assay[crude]['Code']
if assay[crude]['Code'] = 'multiple':     
    diff = exceptions[crude][discharge_price_region]['Code']
else:    
    diff = total[assay[crude]['Code']]

exceptions = {
        'Arab Extra Light':{'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'}},
        'Arab Extra Light':{'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'}},
        'Arab Extra Light':{'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'}},
        'Arab Extra Light':{'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':{'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'}},
        'Arab Light':{'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'}},
        'Arab Light':{'HOUSTON':{'Code':'AAIRA00','Index':'WTI'}},
        'Arab Light':{'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':{'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'}},
        'Arab Medium':{'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'}},
        'Arab Medium':{'HOUSTON':{'Code':'AAIRB00','Index':'WTI'}},
        'Arab Medium':{'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':{'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'}},
        'Arab Heavy':{'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'}},
        'Arab Heavy':{'HOUSTON':{'Code':'AAIRC00','Index':'WTI'}},
        'Arab Heavy':{'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':{'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'}},
        'Basrah Light':{'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'}},
        'Basrah Light':{'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'}},
        'Basrah Light':{'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':{'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'}},
        'Basrah Heavy':{'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'}},
        'Basrah Heavy':{'HOUSTON':{'Code':'AAXUE00','Index':'Mars'}},
        'Basrah Heavy':{'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':{'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'}},
        'Iranian Heavy':{'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'}},
        #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Iranian Heavy':{'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':{'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'}},
        'Iranian Light':{'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'}},
        #'Iranian Light':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Iranian Light':{'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':{'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'}},
        'Forozan':{'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'}},
        #'Forozan':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Forozan':{'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'}},
        'Isthmus':{'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'}},
        'Isthmus':{'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'}},
        'Isthmus':{'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'BWAVE'}},
        'Maya':{'AUGUSTA':{'Code':'AAIQB00','Index':'BWAVE'}},
        'Maya':{'HOUSTON':{'Code':'AAIPY00','Index':'WTI'}},
        'Maya':{'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }
crude
exceptions['Maya'][discharge_price_region]['Code']
discharge_price_region
exceptions['Maya']
discharge_price_region
exceptions['Maya'][discharge_price_region]
exceptions['Maya']['ROTTERDAM']
exceptions = {
        'Arab Extra Light':{'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'}},
        'Arab Extra Light':{'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'}},
        'Arab Extra Light':{'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'}},
        'Arab Extra Light':{'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':{'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'}},
        'Arab Light':{'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'}},
        'Arab Light':{'HOUSTON':{'Code':'AAIRA00','Index':'WTI'}},
        'Arab Light':{'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':{'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'}},
        'Arab Medium':{'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'}},
        'Arab Medium':{'HOUSTON':{'Code':'AAIRB00','Index':'WTI'}},
        'Arab Medium':{'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':{'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'}},
        'Arab Heavy':{'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'}},
        'Arab Heavy':{'HOUSTON':{'Code':'AAIRC00','Index':'WTI'}},
        'Arab Heavy':{'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':{'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'}},
        'Basrah Light':{'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'}},
        'Basrah Light':{'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'}},
        'Basrah Light':{'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':{'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'}},
        'Basrah Heavy':{'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'}},
        'Basrah Heavy':{'HOUSTON':{'Code':'AAXUE00','Index':'Mars'}},
        'Basrah Heavy':{'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':{'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'}},
        'Iranian Heavy':{'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'}},
        #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Iranian Heavy':{'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':{'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'}},
        'Iranian Light':{'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'}},
        #'Iranian Light':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Iranian Light':{'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':{'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'}},
        'Forozan':{'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'}},
        #'Forozan':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
        'Forozan':{'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'}},
        'Isthmus':{'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'}},
        'Isthmus':{'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'}},
        'Isthmus':{'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'BWAVE'}},
        'Maya':{'AUGUSTA':{'Code':'AAIQB00','Index':'BWAVE'}},
        'Maya':{'HOUSTON':{'Code':'AAIPY00','Index':'WTI'}},
        'Maya':{'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }
exceptions['Maya']['ROTTERDAM']['Code']
exceptions['Maya']['AUGUSTA']['Code']
exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
        #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
        #'Iranian Light':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':{'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                   'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
        #'Forozan':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                    'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                   'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                   'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                   'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }
diff = exceptions['Maya']['AUGUSTA']['Code']
exceptions
exceptions['Maya']['AUGUSTA']['Code']
exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }
if assay[crude]['Code'] == 'multiple':     
    diff = exceptions[crude][discharge_price_region]['Code']
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

crude
crude_vs
cfd8
cfd4
cfd_condition
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


#crudes = list(test_crudes.index.values)
crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

#destinations = ['Houston','Rotterdam','Augusta','Singapore']


destinations = ['Rotterdam','Augusta','Houston']
global_arb = make_arbs(crudes, destinations, *var)
global_arb = make_arbs(crudes, destinations, *var)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
global_arb = make_arbs(crudes, destinations, *var)
debugfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
debugfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
rude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df, 
                    refinery_volume = None,
                    lvn_gasolinepool = None,
                    kero_gasolinepool = None,
                    reformer_capacity = None,
                    fcc_capacity = None,
                    coker_capacity = None
crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df, 
                    refinery_volume = None,
                    lvn_gasolinepool = None,
                    kero_gasolinepool = None,
                    reformer_capacity = None,
                    fcc_capacity = None,
                    coker_capacity = None

## ---(Mon May 14 11:29:05 2018)---
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from ArbEcons2504 import import_data

#crude = 'Azeri'
#destination = 'Rotterdam'
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
from ArbEcons2504 import import_data
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
from ArbEcons2504 import import_data
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
rader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.x
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2017.xlsm')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
crude_diffs
crude_diffs.columns
crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
total.index
[crude_diffs.index.isin(total.index)]
crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs.columns
[name for name in crude_diffs.columns if 'Unnamed' in name]
crude_diffs
list(crude_diffs.columns)
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
crude_diffs.columns
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if ['Unnamed','WTI CMA M1','Nymex WTI Active month 1 (US close)'] in name], axis=1)
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
refinery_volume = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
coker_capacity = None
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

if coker_capacity is None and destination is 'Houston':
        coker_capacity = refinery_volume*0.24
else:
        coker_capacity = 0

crude = 'Azeri'
destination = 'Houston'
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

if coker_capacity is None and destination is 'Houston':
        coker_capacity = refinery_volume*0.24
else:
        coker_capacity = 0

hvn_input = assay[crude]['HVN'] * refinery_volume
kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
reformer_input = hvn_input + kero_input + lvn_input
vgo_input = assay[crude]['VGO'] * refinery_volume
coker_input = assay[crude]['RESIDUE'] * refinery_volume
reformer_output = {}
reformer_assay_standard = {
    'c3': {'yield': 0.025},
    'c4': {'yield': 0.025},
    'h2': {'yield': 0.0},
    'btx': {'yield': 0.0},
    'gasoline': {'yield': 0.95}
    }
utilised_ref_cap = reformer_capacity * 0.97
reformer_volume = np.minimum(utilised_ref_cap, reformer_input)

"""if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
of the surplus"""

reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
coker_output = {}
coker_assay_standard = {
    'c3': {'yield': 0.015},
    'c4': {'yield': 0.015},
    'lvn': {'yield': 0.09},
    'hvn': {'yield': 0.11},
    'lgo': {'yield': 0.15},
    'hgo': {'yield': 0.35}
    }
utilised_coker_cap = coker_capacity * 0.97
coker_volume = np.minimum(utilised_coker_cap, coker_input)

coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
fcc_output = {}

fcc_assay_standard = {
    'c3': {'yield': 0.01},
    'c4': {'yield': 0.01},
    'c3__': {'yield': 0.02},
    'lco': {'yield': 0.20},
    'clo': {'yield': 0.20},
    'gasoline': {'yield': 0.56}
    }

fcc_assay_usgc = {
    'c3': {'yield': 0.015},
    'c4': {'yield': 0.015},
    'lco': {'yield': 0.12},
    'clo': {'yield': 0.08},
    'gasoline': {'yield': 0.75}
    }

utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])

if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':
    fcc_assay = fcc_assay_usgc
else:
    fcc_assay = fcc_assay_standard



fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    residue = {}
    residue['volume'] = coker_output['surplus_resid']
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO'] + coker_output['lgo']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume

    residue = {}
    residue['volume'] = coker_output['surplus_resid']
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO'] + coker_output['lgo']) * refinery_volume) / refinery_volume
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields

residue = {}
residue['volume'] = coker_output['surplus_resid']

# HDS removing suplhur from the bbl before processing
low_HDS_factor = 0.5
high_HDS_factor = 0.8
if assay[crude]['RESIDUE_sulphur'] < 0.035:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
else:
    residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor


residue['density'] = assay[crude]['RESIDUE_density'] 
residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation

# certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
imported_vgo = 0  
diluent_used = 0       
diluent = {
        'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
        'sulphur': 0.001,
        'density': 0.897,
        'viscosity': 3,
        'lco_for_blending': 0,
        'target_sulphur': 0,
        'volume_used': diluent_used
        }

diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
def target_viscosity(diluent_used):
    return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8


# we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
         {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  

# save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
diluent['volume_used'] = res.x[0]
diluent['cst'] = res.fun

# =============================================================================
# Left blank for the addition of if vgo is needed to be imported
# Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
# 
# if target_viscosity(diluent_used) > 380:
#     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
#     
# =============================================================================

diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29

combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
yields = {}  
yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
      + reformer_output['propane'] 
      + fcc_output['propane'] 
      + fcc_output['c3__'] * 0.5
      ) / refinery_volume

yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
       + reformer_output['butane'] 
       + fcc_output['butane']
       + fcc_output['c3__'] * 0.5
       ) / refinery_volume

yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
        + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
        ) / refinery_volume

#yields['btx'] = (reformer_output['btx']) / refinery_volume

yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO'] + coker_output['lgo']) * refinery_volume) / refinery_volume
yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
reformer_output = reformer()
coker_output = coker()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
coker_output['lgo']
coker_assay_standard['lgo']['yield']
coker_volume
coker_capacity
reformer_capacity
coker_input
reformer_input
np.maximum(coker_input - utilised_coker_cap,0)
utilised_coker_cap
coker_input
utilised_coker_cap
coker_input
coker_volume
coker_assay_standard['c3']['yield']
utilised_coker_cap
coker_input
coker_volume = np.minimum(utilised_coker_cap, coker_input)
coker_volume
assay[crude]['LGO'
assay[crude]['LGO']
coker_assay_standard['lvn']['yield'] 
coker_volume
coker_output['lgo']
fcc_output['gasoline'] 
reformer_output['gasoline']
fcc_output['gasoline']
coker_output['hvn']
assay[crude]['LGO']
assay[crude]['HGO']
(assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume)
((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume)
yields = {}  
yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
      + reformer_output['propane'] 
      + fcc_output['propane'] 
      + fcc_output['c3__'] * 0.5
      ) / refinery_volume

yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
       + reformer_output['butane'] 
       + fcc_output['butane']
       + fcc_output['c3__'] * 0.5
       ) / refinery_volume

yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
        + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
        ) / refinery_volume

#yields['btx'] = (reformer_output['btx']) / refinery_volume

yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
yields
yield_codes = {}
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                  'base_blend':'BaseBlendNWE'},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                  'base_blend':'BaseBlendNWE'},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                  'base_blend':'BaseBlendMED'},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                  'base_blend':'BaseBlendASIA'}
                         }
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j

pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 
yield_codes = {}
for i, j in yields.items():
    yield_codes[price_codes[pricing_centre][i]['Code']] = j

yield_codes
yield_names = {}
for i, j in yields.items():
    yield_names[price_codes[pricing_centre][i]['Code']] = i

yield_names
conversion_codes = {}
for i, j in yields.items():
    conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']

conversion_codes
regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
regional_price_set
temp_df = regional_price_set.assign(**conversion_codes)
temp_df
gpw = regional_price_set.div(temp_df)
gpw
temp_df = regional_price_set.assign(**yield_codes)
temp_df = regional_price_set.assign(**yield_codes)
gpw = gpw.mul(temp_df)
gpw.rename(columns=yield_names, inplace=True)
gpw
gpw['GPW'] = gpw.sum(1)
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'],
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
def standard_ref(crude):
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        coker_input = assay[crude]['RESIDUE'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        
        """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
        which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
        kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
        the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
        of the surplus"""
        
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def coker():
        coker_output = {}
        coker_assay_standard = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lvn': {'yield': 0.09},
            'hvn': {'yield': 0.11},
            'lgo': {'yield': 0.15},
            'hgo': {'yield': 0.35}
            }
        utilised_coker_cap = coker_capacity * 0.97
        coker_volume = np.minimum(utilised_coker_cap, coker_input)
        
        coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
        coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
        coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
        coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
        coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
        coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
        coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
        
        return coker_output
    
    
    def fcc():
        fcc_output = {}
        
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        fcc_assay_usgc = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lco': {'yield': 0.12},
            'clo': {'yield': 0.08},
            'gasoline': {'yield': 0.75}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
        
        if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':
            fcc_assay = fcc_assay_usgc
        else:
            fcc_assay = fcc_assay_standard
        
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    
    def alkylation_unit():
        pass
    
    def hydrocracker():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = coker_output['surplus_resid']
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
    reformer_output = reformer()
    coker_output = coker()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    return yields


def calculate_margin(yields):
    
    """The below gets us our list of prroduct price codes"""
    #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
    
    #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
    
    yield_codes = {}
    for i, j in yields.items():
        yield_codes[price_codes[pricing_centre][i]['Code']] = j
    
    yield_names = {}
    for i, j in yields.items():
        yield_names[price_codes[pricing_centre][i]['Code']] = i
    
    conversion_codes = {}
    for i, j in yields.items():
        conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
    
    regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
    temp_df = regional_price_set.assign(**conversion_codes)
    gpw = regional_price_set.div(temp_df)
    
    temp_df = regional_price_set.assign(**yield_codes)
    gpw = gpw.mul(temp_df)
    gpw.rename(columns=yield_names, inplace=True)
    
    gpw['GPW'] = gpw.sum(1)
    return gpw



"""this is to create the look up neccessary for generating the table"""
price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                  'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                  'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                  'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                  'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                  'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                  'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                  'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                  'base_blend':'BaseBlendNWE'},
    'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                  'butane':{'Code':'PMAAK00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                  'kero':{'Code':'PJABA00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                  'base_blend':'BaseBlendNWE'},
    'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                  'butane':{'Code':'PMAAM00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                  'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                  'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                  'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                  'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                  'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                  'base_blend':'BaseBlendMED'},
    'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                  'butane':{'Code':'AAJTT00', 'Conversion':11},
                  'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                  'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                  'kero':{'Code':'PJABF00', 'Conversion':1},
                  'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                  'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                  'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                  'base_blend':'BaseBlendASIA'}
                         }

"""Choose which set of products we use based on discharge locations"""   
pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 
price_codes[pricing_centre]['base_blend']
crude
destination
pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 
pricing_centre
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'],
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'])
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'],
                 refinery_volume = None,
                 lvn_gasolinepool = None,
                 kero_gasolinepool = None,
                 reformer_capacity = None,
                 fcc_capacity = None)
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'])
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

if coker_capacity is None and destination is 'Houston':
        coker_capacity = refinery_volume*0.24
else:
        coker_capacity = 0

base_yields = standard_ref(price_codes[pricing_centre]['base_blend'])
refinery_volume = None
lvn_gasolinepool = None
kero_gasolinepool = None
reformer_capacity = None
fcc_capacity = None
coker_capacity = None
refinery_config = {'refinery_volume':refinery_volume,
                   'reformer_capacity':reformer_capacity,
                   'fcc_capacity':fcc_capacity,
                   'coker_capacity':coker_capacity,
                   'lvn_gasolinepool':lvn_gasolinepool,
                   'kero_gasolinepool':kero_gasolinepool
                   }
refinery_config
refinery_config[refinery_volume]
refinery_volume
refinery_config[refinery_volume]
refinery_config['refinery_volume']
if refinery_volume is None:
    refinery_config['refinery_volume'] = 200

refinery_config['refinery_volume']
lvn_gasolinepool
refinery_config
refinery_volume = None
refinery_config = {'refinery_volume':refinery_volume,
                   'reformer_capacity':reformer_capacity,
                   'fcc_capacity':fcc_capacity,
                   'coker_capacity':coker_capacity,
                   'lvn_gasolinepool':lvn_gasolinepool,
                   'kero_gasolinepool':kero_gasolinepool
                   }
refinery_config
refinery_volume
print(refinery_volume)
if refinery_volume is None:
    refinery_volume = 200

refinery_config
refinery_volume
refinery_config = {'refinery_volume':refinery_volume,
                   'reformer_capacity':reformer_capacity,
                   'fcc_capacity':fcc_capacity,
                   'coker_capacity':coker_capacity,
                   'lvn_gasolinepool':lvn_gasolinepool,
                   'kero_gasolinepool':kero_gasolinepool
                   }
refinery_config
**refinery_config
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'])
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

if coker_capacity is None and destination is 'Houston':
        coker_capacity = refinery_volume*0.24
else:
        coker_capacity = 0

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input + lvn_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    coker_input = assay[crude]['RESIDUE'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input


def reformer():
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    
    """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
    which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
    kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
    the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
    of the surplus"""
    
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def coker():
    coker_output = {}
    coker_assay_standard = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lvn': {'yield': 0.09},
        'hvn': {'yield': 0.11},
        'lgo': {'yield': 0.15},
        'hgo': {'yield': 0.35}
        }
    utilised_coker_cap = coker_capacity * 0.97
    coker_volume = np.minimum(utilised_coker_cap, coker_input)
    
    coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
    coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
    coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
    coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
    coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
    coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
    coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
    
    return coker_output



def fcc():
    fcc_output = {}
    
    fcc_assay_standard = {
        'c3': {'yield': 0.01},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    fcc_assay_usgc = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lco': {'yield': 0.12},
        'clo': {'yield': 0.08},
        'gasoline': {'yield': 0.75}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
    
    if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':
        fcc_assay = fcc_assay_usgc
    else:
        fcc_assay = fcc_assay_standard
    
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output



def alkylation_unit():
    pass


def hydrocracker():
    pass


def fo_blend():
    residue = {}
    residue['volume'] = coker_output['surplus_resid']
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent


def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'])
refinery_config = {'refinery_volume':refinery_volume,
    'reformer_capacity':reformer_capacity,
    'fcc_capacity':fcc_capacity,
    'coker_capacity':coker_capacity,
    'lvn_gasolinepool':lvn_gasolinepool,
    'kero_gasolinepool':kero_gasolinepool
    }
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config)
def standard_ref(crude, refinery_volume, reformer_capacity, fcc_capacity, coker_capacity, lvn_gasolinepool, kero_gasolinepool):
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        coker_input = assay[crude]['RESIDUE'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        
        """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
        which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
        kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
        the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
        of the surplus"""
        
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def coker():
        coker_output = {}
        coker_assay_standard = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lvn': {'yield': 0.09},
            'hvn': {'yield': 0.11},
            'lgo': {'yield': 0.15},
            'hgo': {'yield': 0.35}
            }
        utilised_coker_cap = coker_capacity * 0.97
        coker_volume = np.minimum(utilised_coker_cap, coker_input)
        
        coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
        coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
        coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
        coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
        coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
        coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
        coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
        
        return coker_output
    
    
    def fcc():
        fcc_output = {}
        
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        fcc_assay_usgc = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lco': {'yield': 0.12},
            'clo': {'yield': 0.08},
            'gasoline': {'yield': 0.75}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
        
        if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':
            fcc_assay = fcc_assay_usgc
        else:
            fcc_assay = fcc_assay_standard
        
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    
    def alkylation_unit():
        pass
    
    def hydrocracker():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = coker_output['surplus_resid']
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
    reformer_output = reformer()
    coker_output = coker()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    return yields

base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config)
base_yields
def standard_ref(crude, **refinery_config):
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        coker_input = assay[crude]['RESIDUE'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        
        """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
        which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
        kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
        the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
        of the surplus"""
        
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def coker():
        coker_output = {}
        coker_assay_standard = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lvn': {'yield': 0.09},
            'hvn': {'yield': 0.11},
            'lgo': {'yield': 0.15},
            'hgo': {'yield': 0.35}
            }
        utilised_coker_cap = coker_capacity * 0.97
        coker_volume = np.minimum(utilised_coker_cap, coker_input)
        
        coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
        coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
        coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
        coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
        coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
        coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
        coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
        
        return coker_output
    
    
    def fcc():
        fcc_output = {}
        
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        fcc_assay_usgc = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lco': {'yield': 0.12},
            'clo': {'yield': 0.08},
            'gasoline': {'yield': 0.75}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
        
        if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':
            fcc_assay = fcc_assay_usgc
        else:
            fcc_assay = fcc_assay_standard
        
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    
    def alkylation_unit():
        pass
    
    def hydrocracker():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = coker_output['surplus_resid']
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
    reformer_output = reformer()
    coker_output = coker()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    return yields

base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config)
def standard_ref(crude, refinery_volume, reformer_capacity, fcc_capacity, coker_capacity, lvn_gasolinepool, kero_gasolinepool):
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        coker_input = assay[crude]['RESIDUE'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        
        """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
        which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
        kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
        the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
        of the surplus"""
        
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def coker():
        coker_output = {}
        coker_assay_standard = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lvn': {'yield': 0.09},
            'hvn': {'yield': 0.11},
            'lgo': {'yield': 0.15},
            'hgo': {'yield': 0.35}
            }
        utilised_coker_cap = coker_capacity * 0.97
        coker_volume = np.minimum(utilised_coker_cap, coker_input)
        
        coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
        coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
        coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
        coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
        coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
        coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
        coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
        
        return coker_output
    
    
    def fcc():
        fcc_output = {}
        
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        fcc_assay_usgc = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lco': {'yield': 0.12},
            'clo': {'yield': 0.08},
            'gasoline': {'yield': 0.75}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
        
        if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':
            fcc_assay = fcc_assay_usgc
        else:
            fcc_assay = fcc_assay_standard
        
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    
    def alkylation_unit():
        pass
    
    def hydrocracker():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = coker_output['surplus_resid']
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields
    
    refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
    reformer_output = reformer()
    coker_output = coker()
    fcc_output = fcc()
    diluent = fo_blend()
    yields = calculate_yields()
    return yields

**refinery_config
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config)
fcc_assay['c4']['yield']
fcc_volume
fcc_output['butane']
fcc_assay['c3__']['yield']
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3])
if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':
    fcc_assay = fcc_assay_usgc
else:
    fcc_assay = fcc_assay_standard

fcc_assay
if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON'

if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]) is 'HOUSTON':

orts[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3])
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3])
ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).iat[0]
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config)
fcc_volume
fcc_assay['c4']['yield'] 
fcc_output['butane']
if refinery_volume is None:
    refinery_volume = 200

if lvn_gasolinepool is None:
    lvn_gasolinepool = 0.12

if kero_gasolinepool is None:
    kero_gasolinepool = 0.15

if reformer_capacity is None:
    reformer_capacity = refinery_volume*0.21

if fcc_capacity is None:
        fcc_capacity = refinery_volume*0.24

if coker_capacity is None and destination is 'Houston':
        coker_capacity = refinery_volume*0.24
else:
        coker_capacity = 0

refinery_config = {'refinery_volume':refinery_volume,
    'reformer_capacity':reformer_capacity,
    'fcc_capacity':fcc_capacity,
    'coker_capacity':coker_capacity,
    'lvn_gasolinepool':lvn_gasolinepool,
    'kero_gasolinepool':kero_gasolinepool
    }
refinery_config
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config)
base_yields
crude_yields = standard_ref(crude, **refinery_config)
crude_yields
base_yields
base_crude
base_crude = calculate_margin(base_yields)
base_crude
base_yields
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config)
base_crude = calculate_margin(base_yields)

crude_yields = standard_ref(crude, **refinery_config)

crude_gpw = calculate_margin(crude_yields)
crude_gpw['Base_GPW'] = base_crude['GPW']
crude_gpw['GPW_delta'] = crude_gpw['GPW'] - crude_gpw['Base_GPW']
crude_gpw
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
crudes = ['Brent','Azeri','CPC Blend']
crudes = ['Brent','Azeri','CPC Blend']
destinations = ['Rotterdam','Augusta','Houston']
global_arb = make_arbs(crudes, destinations, *var)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
destinations
def make_arbs(crudes,destinations, *var): 
    counter = 0
    
    base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
        'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4}}
        #'Houston':{'Canadian Heavy':0.1,'Castilla':0.15,'Basrah Light':0.125,'Arab Medium':0.125,'LLS':0.25,'Qua Iboe':0.125,'WTI Midland':0.125}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        margin_headers.append(name)
                        if 'base' in j:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    for x in range(len(margin_headers)-1):
                        name = margin_headers[x]+'_advantage'
                        arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       margin_headers.append(name)
                       if 'base' in j:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    for x in range(len(margin_headers)-1):
                       name = margin_headers[x]+'_advantage'
                       next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arb = make_arbs(crudes, destinations, *var)

#global_arb.columns
#print(global_arb.head())
#global_arb['Eagle Ford']['Rotterdam'].tail()


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs6.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
refinery_config = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}}
refinery_config['simple']
refinery_config = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 
refinery_config['simple']
refinery_config
base_yields = standard_ref(price_codes[pricing_centre]['base_blend'], **refinery_config['simple'])
    def standard_ref(crude, refinery_volume, reformer_capacity, fcc_capacity, coker_capacity, lvn_gasolinepool, kero_gasolinepool):
        
        def cdu():
            ### Fractions for gaosline pool       
            hvn_input = assay[crude]['HVN'] * refinery_volume
            kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
            lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
            reformer_input = hvn_input + kero_input + lvn_input
            vgo_input = assay[crude]['VGO'] * refinery_volume
            coker_input = assay[crude]['RESIDUE'] * refinery_volume
            
            return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
        
        def reformer():
            reformer_output = {}
            reformer_assay_standard = {
                'c3': {'yield': 0.025},
                'c4': {'yield': 0.025},
                'h2': {'yield': 0.0},
                'btx': {'yield': 0.0},
                'gasoline': {'yield': 0.95}
                }
            utilised_ref_cap = reformer_capacity * 0.97
            reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
            
            """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
            which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
            kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
            the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
            of the surplus"""
            
            reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
            reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
            reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
            reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
            reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
            reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
            reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
            
            return reformer_output
        
        def coker():
            coker_output = {}
            coker_assay_standard = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lvn': {'yield': 0.09},
                'hvn': {'yield': 0.11},
                'lgo': {'yield': 0.15},
                'hgo': {'yield': 0.35}
                }
            utilised_coker_cap = coker_capacity * 0.97
            coker_volume = np.minimum(utilised_coker_cap, coker_input)
            
            coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
            coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
            coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
            coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
            coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
            coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
            coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
            
            return coker_output
        
        
        def fcc():
            fcc_output = {}
            
            fcc_assay_standard = {
                'c3': {'yield': 0.01},
                'c4': {'yield': 0.01},
                'c3__': {'yield': 0.02},
                'lco': {'yield': 0.20},
                'clo': {'yield': 0.20},
                'gasoline': {'yield': 0.56}
                }
            
            fcc_assay_usgc = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lco': {'yield': 0.12},
                'clo': {'yield': 0.08},
                'gasoline': {'yield': 0.75}
                }
            
            utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
            fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
            
            if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).iat[0] is 'HOUSTON':
                fcc_assay = fcc_assay_usgc
            else:
                fcc_assay = fcc_assay_standard
            
            
            fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
            fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
            fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
            fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
            fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
            fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
            fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
            
            return fcc_output
        
        
        def alkylation_unit():
            pass
        
        def hydrocracker():
            pass
        
        def fo_blend():
            residue = {}
            residue['volume'] = coker_output['surplus_resid']
            
            # HDS removing suplhur from the bbl before processing
            low_HDS_factor = 0.5
            high_HDS_factor = 0.8
            if assay[crude]['RESIDUE_sulphur'] < 0.035:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
            else:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
            
            residue['density'] = assay[crude]['RESIDUE_density'] 
            residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
            residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
            
            # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
            imported_vgo = 0  
            diluent_used = 0       
            diluent = {
                    'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                    'sulphur': 0.001,
                    'density': 0.897,
                    'viscosity': 3,
                    'lco_for_blending': 0,
                    'target_sulphur': 0,
                    'volume_used': diluent_used
                    }
            
            diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
            diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
            
            # create the 'cost' function - what do we want to minimise?
            def target_viscosity(diluent_used):
                return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
            
            # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
            cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                     {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
            
            # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
            res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
            diluent['volume_used'] = res.x[0]
            diluent['cst'] = res.fun
            
            # =============================================================================
            # Left blank for the addition of if vgo is needed to be imported
            # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
            # 
            # if target_viscosity(diluent_used) > 380:
            #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
            #     
            # =============================================================================
            
            diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
            diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
            diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
            residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
            residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
            
            combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
            combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
            diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
            
            return diluent
        
        def calculate_yields():
            yields = {}  
            yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
                  + reformer_output['propane'] 
                  + fcc_output['propane'] 
                  + fcc_output['c3__'] * 0.5
                  ) / refinery_volume
            
            yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                   + reformer_output['butane'] 
                   + fcc_output['butane']
                   + fcc_output['c3__'] * 0.5
                   ) / refinery_volume
            
            yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                    + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                    ) / refinery_volume
            
            #yields['btx'] = (reformer_output['btx']) / refinery_volume
            
            yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
            yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
            yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
            yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
            yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
            
            return yields
        
        refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
        reformer_output = reformer()
        coker_output = coker()
        fcc_output = fcc()
        diluent = fo_blend()
        yields = calculate_yields()
        return yields
    
    def calculate_margin(yields):
        
        """The below gets us our list of prroduct price codes"""
        #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
        
        #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]['Code']] = j
        
        yield_names = {}
        for i, j in yields.items():
            yield_names[price_codes[pricing_centre][i]['Code']] = i
        
        conversion_codes = {}
        for i, j in yields.items():
            conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
        
        regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
        temp_df = regional_price_set.assign(**conversion_codes)
        gpw = regional_price_set.div(temp_df)
        
        temp_df = regional_price_set.assign(**yield_codes)
        gpw = gpw.mul(temp_df)
        gpw.rename(columns=yield_names, inplace=True)
        
        gpw['GPW'] = gpw.sum(1)
        return gpw
    
    """this is to create the look up neccessary for generating the table"""
    price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                      'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                      'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                      'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                      'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                      'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                      'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                      'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                      'base_blend':'BaseBlendNWE'},
        'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                      'butane':{'Code':'PMAAK00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                      'kero':{'Code':'PJABA00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                      'base_blend':'BaseBlendNWE'},
        'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                      'butane':{'Code':'PMAAM00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                      'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                      'base_blend':'BaseBlendMED'},
        'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                      'butane':{'Code':'AAJTT00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                      'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                      'kero':{'Code':'PJABF00', 'Conversion':1},
                      'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                      'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                      'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                      'base_blend':'BaseBlendASIA'}
                             }
    
    """Choose which set of products we use based on discharge locations"""   
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 

# =============================================================================
#     if refinery_volume is None:
#         refinery_volume = 200
#     if lvn_gasolinepool is None:
#         lvn_gasolinepool = 0.12
#     if kero_gasolinepool is None:
#         kero_gasolinepool = 0.15
#     if reformer_capacity is None:
#         reformer_capacity = refinery_volume*0.21
#     if fcc_capacity is None:
#             fcc_capacity = refinery_volume*0.24
# =============================================================================
    if coker_capacity is None and destination is 'Houston':
            coker_capacity = refinery_volume*0.24
    else:
            coker_capacity = 0
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from ArbEcons2504 import import_data

#crude = 'Azeri'
#destination = 'Rotterdam'
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
refinery_config = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 
    def standard_ref(crude, refinery_volume, reformer_capacity, fcc_capacity, coker_capacity, lvn_gasolinepool, kero_gasolinepool):
        
        def cdu():
            ### Fractions for gaosline pool       
            hvn_input = assay[crude]['HVN'] * refinery_volume
            kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
            lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
            reformer_input = hvn_input + kero_input + lvn_input
            vgo_input = assay[crude]['VGO'] * refinery_volume
            coker_input = assay[crude]['RESIDUE'] * refinery_volume
            
            return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
        
        def reformer():
            reformer_output = {}
            reformer_assay_standard = {
                'c3': {'yield': 0.025},
                'c4': {'yield': 0.025},
                'h2': {'yield': 0.0},
                'btx': {'yield': 0.0},
                'gasoline': {'yield': 0.95}
                }
            utilised_ref_cap = reformer_capacity * 0.97
            reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
            
            """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
            which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
            kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
            the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
            of the surplus"""
            
            reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
            reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
            reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
            reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
            reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
            reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
            reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
            
            return reformer_output
        
        def coker():
            coker_output = {}
            coker_assay_standard = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lvn': {'yield': 0.09},
                'hvn': {'yield': 0.11},
                'lgo': {'yield': 0.15},
                'hgo': {'yield': 0.35}
                }
            utilised_coker_cap = coker_capacity * 0.97
            coker_volume = np.minimum(utilised_coker_cap, coker_input)
            
            coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
            coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
            coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
            coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
            coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
            coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
            coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
            
            return coker_output
        
        
        def fcc():
            fcc_output = {}
            
            fcc_assay_standard = {
                'c3': {'yield': 0.01},
                'c4': {'yield': 0.01},
                'c3__': {'yield': 0.02},
                'lco': {'yield': 0.20},
                'clo': {'yield': 0.20},
                'gasoline': {'yield': 0.56}
                }
            
            fcc_assay_usgc = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lco': {'yield': 0.12},
                'clo': {'yield': 0.08},
                'gasoline': {'yield': 0.75}
                }
            
            utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
            fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
            
            if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).iat[0] is 'HOUSTON':
                fcc_assay = fcc_assay_usgc
            else:
                fcc_assay = fcc_assay_standard
            
            
            fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
            fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
            fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
            fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
            fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
            fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
            fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
            
            return fcc_output
        
        
        def alkylation_unit():
            pass
        
        def hydrocracker():
            pass
        
        def fo_blend():
            residue = {}
            residue['volume'] = coker_output['surplus_resid']
            
            # HDS removing suplhur from the bbl before processing
            low_HDS_factor = 0.5
            high_HDS_factor = 0.8
            if assay[crude]['RESIDUE_sulphur'] < 0.035:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
            else:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
            
            residue['density'] = assay[crude]['RESIDUE_density'] 
            residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
            residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
            
            # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
            imported_vgo = 0  
            diluent_used = 0       
            diluent = {
                    'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                    'sulphur': 0.001,
                    'density': 0.897,
                    'viscosity': 3,
                    'lco_for_blending': 0,
                    'target_sulphur': 0,
                    'volume_used': diluent_used
                    }
            
            diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
            diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
            
            # create the 'cost' function - what do we want to minimise?
            def target_viscosity(diluent_used):
                return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
            
            # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
            cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                     {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
            
            # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
            res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
            diluent['volume_used'] = res.x[0]
            diluent['cst'] = res.fun
            
            # =============================================================================
            # Left blank for the addition of if vgo is needed to be imported
            # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
            # 
            # if target_viscosity(diluent_used) > 380:
            #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
            #     
            # =============================================================================
            
            diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
            diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
            diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
            residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
            residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
            
            combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
            combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
            diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
            
            return diluent
        
        def calculate_yields():
            yields = {}  
            yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
                  + reformer_output['propane'] 
                  + fcc_output['propane'] 
                  + fcc_output['c3__'] * 0.5
                  ) / refinery_volume
            
            yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                   + reformer_output['butane'] 
                   + fcc_output['butane']
                   + fcc_output['c3__'] * 0.5
                   ) / refinery_volume
            
            yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                    + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                    ) / refinery_volume
            
            #yields['btx'] = (reformer_output['btx']) / refinery_volume
            
            yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
            yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
            yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
            yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
            yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
            
            return yields
        
        refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
        reformer_output = reformer()
        coker_output = coker()
        fcc_output = fcc()
        diluent = fo_blend()
        yields = calculate_yields()
        return yields
    
    def calculate_margin(yields):
        
        """The below gets us our list of prroduct price codes"""
        #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
        
        #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]['Code']] = j
        
        yield_names = {}
        for i, j in yields.items():
            yield_names[price_codes[pricing_centre][i]['Code']] = i
        
        conversion_codes = {}
        for i, j in yields.items():
            conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
        
        regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
        temp_df = regional_price_set.assign(**conversion_codes)
        gpw = regional_price_set.div(temp_df)
        
        temp_df = regional_price_set.assign(**yield_codes)
        gpw = gpw.mul(temp_df)
        gpw.rename(columns=yield_names, inplace=True)
        
        gpw['GPW'] = gpw.sum(1)
        return gpw
    
    """this is to create the look up neccessary for generating the table"""
    price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                      'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                      'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                      'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                      'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                      'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                      'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                      'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                      'base_blend':'BaseBlendNWE'},
        'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                      'butane':{'Code':'PMAAK00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                      'kero':{'Code':'PJABA00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                      'base_blend':'BaseBlendNWE'},
        'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                      'butane':{'Code':'PMAAM00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                      'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                      'base_blend':'BaseBlendMED'},
        'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                      'butane':{'Code':'AAJTT00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                      'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                      'kero':{'Code':'PJABF00', 'Conversion':1},
                      'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                      'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                      'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                      'base_blend':'BaseBlendASIA'}
                             }
    
    """Choose which set of products we use based on discharge locations"""   
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 

# =============================================================================
#     if refinery_volume is None:
#         refinery_volume = 200
#     if lvn_gasolinepool is None:
#         lvn_gasolinepool = 0.12
#     if kero_gasolinepool is None:
#         kero_gasolinepool = 0.15
#     if reformer_capacity is None:
#         reformer_capacity = refinery_volume*0.21
#     if fcc_capacity is None:
#             fcc_capacity = refinery_volume*0.24
# =============================================================================
    if coker_capacity is None and destination is 'Houston':
            coker_capacity = refinery_volume*0.24
    else:
            coker_capacity = 0
crude = 'Azeri'
destination = 'Houston'
    def standard_ref(crude, refinery_volume, reformer_capacity, fcc_capacity, coker_capacity, lvn_gasolinepool, kero_gasolinepool):
        
        def cdu():
            ### Fractions for gaosline pool       
            hvn_input = assay[crude]['HVN'] * refinery_volume
            kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
            lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
            reformer_input = hvn_input + kero_input + lvn_input
            vgo_input = assay[crude]['VGO'] * refinery_volume
            coker_input = assay[crude]['RESIDUE'] * refinery_volume
            
            return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
        
        def reformer():
            reformer_output = {}
            reformer_assay_standard = {
                'c3': {'yield': 0.025},
                'c4': {'yield': 0.025},
                'h2': {'yield': 0.0},
                'btx': {'yield': 0.0},
                'gasoline': {'yield': 0.95}
                }
            utilised_ref_cap = reformer_capacity * 0.97
            reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
            
            """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
            which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
            kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
            the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
            of the surplus"""
            
            reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
            reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
            reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
            reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
            reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
            reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
            reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
            
            return reformer_output
        
        def coker():
            coker_output = {}
            coker_assay_standard = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lvn': {'yield': 0.09},
                'hvn': {'yield': 0.11},
                'lgo': {'yield': 0.15},
                'hgo': {'yield': 0.35}
                }
            utilised_coker_cap = coker_capacity * 0.97
            coker_volume = np.minimum(utilised_coker_cap, coker_input)
            
            coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
            coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
            coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
            coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
            coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
            coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
            coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
            
            return coker_output
        
        
        def fcc():
            fcc_output = {}
            
            fcc_assay_standard = {
                'c3': {'yield': 0.01},
                'c4': {'yield': 0.01},
                'c3__': {'yield': 0.02},
                'lco': {'yield': 0.20},
                'clo': {'yield': 0.20},
                'gasoline': {'yield': 0.56}
                }
            
            fcc_assay_usgc = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lco': {'yield': 0.12},
                'clo': {'yield': 0.08},
                'gasoline': {'yield': 0.75}
                }
            
            utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
            fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
            
            if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).iat[0] is 'HOUSTON':
                fcc_assay = fcc_assay_usgc
            else:
                fcc_assay = fcc_assay_standard
            
            
            fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
            fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
            fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
            fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
            fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
            fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
            fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
            
            return fcc_output
        
        
        def alkylation_unit():
            pass
        
        def hydrocracker():
            pass
        
        def fo_blend():
            residue = {}
            residue['volume'] = coker_output['surplus_resid']
            
            # HDS removing suplhur from the bbl before processing
            low_HDS_factor = 0.5
            high_HDS_factor = 0.8
            if assay[crude]['RESIDUE_sulphur'] < 0.035:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
            else:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
            
            residue['density'] = assay[crude]['RESIDUE_density'] 
            residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
            residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
            
            # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
            imported_vgo = 0  
            diluent_used = 0       
            diluent = {
                    'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                    'sulphur': 0.001,
                    'density': 0.897,
                    'viscosity': 3,
                    'lco_for_blending': 0,
                    'target_sulphur': 0,
                    'volume_used': diluent_used
                    }
            
            diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
            diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
            
            # create the 'cost' function - what do we want to minimise?
            def target_viscosity(diluent_used):
                return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
            
            # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
            cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                     {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
            
            # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
            res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
            diluent['volume_used'] = res.x[0]
            diluent['cst'] = res.fun
            
            # =============================================================================
            # Left blank for the addition of if vgo is needed to be imported
            # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
            # 
            # if target_viscosity(diluent_used) > 380:
            #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
            #     
            # =============================================================================
            
            diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
            diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
            diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
            residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
            residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
            
            combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
            combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
            diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
            
            return diluent
        
        def calculate_yields():
            yields = {}  
            yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
                  + reformer_output['propane'] 
                  + fcc_output['propane'] 
                  + fcc_output['c3__'] * 0.5
                  ) / refinery_volume
            
            yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                   + reformer_output['butane'] 
                   + fcc_output['butane']
                   + fcc_output['c3__'] * 0.5
                   ) / refinery_volume
            
            yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                    + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                    ) / refinery_volume
            
            #yields['btx'] = (reformer_output['btx']) / refinery_volume
            
            yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
            yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
            yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
            yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
            yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
            
            return yields
        
        refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
        reformer_output = reformer()
        coker_output = coker()
        fcc_output = fcc()
        diluent = fo_blend()
        yields = calculate_yields()
        return yields
    
    def calculate_margin(yields):
        
        """The below gets us our list of prroduct price codes"""
        #[price_codes[pricing_centre][i]['Code'] for i,j in yields.items()]  
        
        #{price_codes[pricing_centre][i]['Code']: [price_codes[pricing_centre][i]['Conversion'] for (i, j) in yields.items()}
        
        yield_codes = {}
        for i, j in yields.items():
            yield_codes[price_codes[pricing_centre][i]['Code']] = j
        
        yield_names = {}
        for i, j in yields.items():
            yield_names[price_codes[pricing_centre][i]['Code']] = i
        
        conversion_codes = {}
        for i, j in yields.items():
            conversion_codes[price_codes[pricing_centre][i]['Code']] = price_codes[pricing_centre][i]['Conversion']
        
        regional_price_set = total[list(yield_codes.keys())].dropna(axis=0)
        temp_df = regional_price_set.assign(**conversion_codes)
        gpw = regional_price_set.div(temp_df)
        
        temp_df = regional_price_set.assign(**yield_codes)
        gpw = gpw.mul(temp_df)
        gpw.rename(columns=yield_names, inplace=True)
        
        gpw['GPW'] = gpw.sum(1)
        return gpw
    
    """this is to create the look up neccessary for generating the table"""
    price_codes = {'HOUSTON':{'propane': {'Code':'PMAAY00', 'Conversion':(100/42)},
                      'butane':{'Code':'PMAAI00', 'Conversion':(100/42)},
                      'naphtha':{'Code':'AAXJP00', 'Conversion':(100/42)},
                      'gasoline':{'Code':'AAVKS00', 'Conversion':(100/42)},
                      'kero':{'Code':'PJABP00', 'Conversion':(100/42)},
                      'ulsd':{'Code':'AATGX00', 'Conversion':(100/42)},
                      'gasoil':{'Code':'POAED00', 'Conversion':(100/42)},
                      'lsfo':{'Code':'PUAAO00', 'Conversion':1},
                      'base_blend':'BaseBlendNWE'},
        'ROTTERDAM':{'propane': {'Code':'PMABA00', 'Conversion':11},
                      'butane':{'Code':'PMAAK00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAM00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAQZV00', 'Conversion':8.34},
                      'kero':{'Code':'PJABA00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAJUS00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAYWT00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAP00', 'Conversion':6.32},
                      'base_blend':'BaseBlendNWE'},
        'AUGUSTA':{'propane': {'Code':'PMABC00', 'Conversion':11},
                      'butane':{'Code':'PMAAM00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAH00', 'Conversion':8.9},
                      'gasoline':{'Code':'AAWZB00', 'Conversion':8.34},
                      'kero':{'Code':'AAZBN00', 'Conversion':7.87},
                      'ulsd':{'Code':'AAWYZ00', 'Conversion':7.45},
                      'gasoil':{'Code':'AAVJJ00', 'Conversion':7.45},
                      'lsfo':{'Code':'PUAAJ00', 'Conversion':6.32},
                      'base_blend':'BaseBlendMED'},
        'SINGAPORE':{'propane': {'Code':'AAJTQ00', 'Conversion':11},
                      'butane':{'Code':'AAJTT00', 'Conversion':11},
                      'naphtha':{'Code':'PAAAP00', 'Conversion':1},
                      'gasoline':{'Code':'PGAEY00', 'Conversion':1},
                      'kero':{'Code':'PJABF00', 'Conversion':1},
                      'ulsd':{'Code':'AAFEX00', 'Conversion':1},
                      'gasoil':{'Code':'AAFEX00', 'Conversion':1},
                      'lsfo':{'Code':'PUADV00', 'Conversion':6.32},
                      'base_blend':'BaseBlendASIA'}
                             }
    
    """Choose which set of products we use based on discharge locations"""   
    pricing_centre = ports[ports['Name'].str.lower() == destination.lower()]['Subregion'].map(sub_to_ws[3]).to_string(index = False) 

# =============================================================================
#     if refinery_volume is None:
#         refinery_volume = 200
#     if lvn_gasolinepool is None:
#         lvn_gasolinepool = 0.12
#     if kero_gasolinepool is None:
#         kero_gasolinepool = 0.15
#     if reformer_capacity is None:
#         reformer_capacity = refinery_volume*0.21
#     if fcc_capacity is None:
#             fcc_capacity = refinery_volume*0.24
# =============================================================================
    if coker_capacity is None and destination is 'Houston':
            coker_capacity = refinery_volume*0.24
    else:
            coker_capacity = 0
global_arb = make_arbs(crudes, destinations, *var)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


#crudes = list(test_crudes.index.values)
crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

#destinations = ['Houston','Rotterdam','Augusta','Singapore']


destinations = ['Rotterdam','Augusta']
#crudes = ['Brent','Azeri','CPC Blend']

def make_arbs(crudes,destinations, *var): 
    counter = 0
    
    base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
        'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4}}
        #'Houston':{'Canadian Heavy':0.1,'Castilla':0.15,'Basrah Light':0.125,'Arab Medium':0.125,'LLS':0.25,'Qua Iboe':0.125,'WTI Midland':0.125}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        margin_headers.append(name)
                        if 'base' in j:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    for x in range(len(margin_headers)-1):
                        name = margin_headers[x]+'_advantage'
                        arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       margin_headers.append(name)
                       if 'base' in j:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    for x in range(len(margin_headers)-1):
                       name = margin_headers[x]+'_advantage'
                       next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
refinery_config
*refinery_config['simple']
**refinery_config['simple']
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
refinery_configurations
refinery_config = refinery_configurations['simple']
refinery_config
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arb = make_arbs(crudes, destinations, *var)
refinery_config['refinery_volume']
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
from GPW2504 import gpw_calculation
global_arb = make_arbs(crudes, destinations, *var)
refinery_config
refinery_config['refinery_volume']
refinery_config
refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 


refinery_config = refinery_configurations['simple']
def make_arbs(crudes,destinations, *var, **refinery_config): 
    counter = 0
    
    base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
        'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4}}
        #'Houston':{'Canadian Heavy':0.1,'Castilla':0.15,'Basrah Light':0.125,'Arab Medium':0.125,'LLS':0.25,'Qua Iboe':0.125,'WTI Midland':0.125}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for i in crudes:
        for k in destinations:
            try:
                if counter == 0:
                    arb_values = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    arb_values[base] = base_costs[k]
                    gpw = gpw_calculation(i,k, *var, **refinery_config)
                    arb_values = pd.concat([arb_values,gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in arb_values.columns if '_landed' in index]:
                        name = j[:4]+"_margin"
                        margin_headers.append(name)
                        if 'base' in j:
                            arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                        else:
                            arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                    for x in range(len(margin_headers)-1):
                        name = margin_headers[x]+'_advantage'
                        arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                    arb_values.columns = pd.MultiIndex.from_product([[i],[k], arb_values.columns])
                    counter +=1
                else:
                    next_arb = arb(i,k, *var)
                    base = k[:4]+'_base_landed'
                    next_arb[base] = base_costs[k]
                    next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                    next_arb = pd.concat([next_arb,next_gpw], axis=1)
                    margin_headers = []
                    for j in [index for index in next_arb.columns if '_landed' in index]:
                       name = j[:4]+"_margin"
                       margin_headers.append(name)
                       if 'base' in j:
                           next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                       else:
                           next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                    for x in range(len(margin_headers)-1):
                       name = margin_headers[x]+'_advantage'
                       next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                    next_arb.columns = pd.MultiIndex.from_product([[i],[k], next_arb.columns])
                    arb_values = pd.concat([arb_values,next_arb], axis=1)
            
            except Exception as e:
                print(e)
                print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arb = make_arbs(crudes, destinations, *var, **refinery_config)

#global_arb.columns
#print(global_arb.head())
#global_arb['Eagle Ford']['Rotterdam'].tail()


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs6.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
global_arb = make_arbs(crudes, destinations, *var, **refinery_config)

#global_arb.columns
#print(global_arb.head())
#global_arb['Eagle Ford']['Rotterdam'].tail()


writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs6.xlsx')
global_arb.to_excel(writer, sheet_name='Arbs')
writer.save()
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
refinery_configurations.keys() 
global_arbs = pd.DataFrame()
for m in refinery_configurations.keys():
    refinery_config = refinery_configurations[m]
    temporary = make_arbs(crudes, destinations, *var, **refinery_config)
    temporary.columns = pd.MultiIndex.from_product([[m], temporary.columns])
    global_arbs = pd.concat([global_arbs,temporary], axis=1)

refinery_config = refinery_configurations['simple']
temporary = make_arbs(crudes, destinations, *var, **refinery_config)
temporary
temporary.columns
pd.MultiIndex.from_product([['simple'], temporary.columns])
temporary.columns = pd.MultiIndex.from_product([['simple'], temporary.columns])
temporary.columns
next_arb.columns
temporary.columns
pd.MultiIndex.from_product([['simple'], temporary.columns])
temporary.columns
refinery_config = refinery_configurations['simple']
temporary = make_arbs(crudes, destinations, *var, **refinery_config)
pd.MultiIndex.from_product([['simple'], temporary.columns])
temporary.columns = pd.MultiIndex.from_product(['simple', temporary.columns])
global_arbs = pd.DataFrame()
for m in refinery_configurations.keys():
    refinery_config = refinery_configurations[m]
    temporary = make_arbs(crudes, destinations, *var, **refinery_config)
    temporary.columns = pd.MultiIndex.from_product([[m], temporary.columns])
    global_arbs = pd.concat([global_arbs,temporary], axis=1, keys=refinery_configurations.keys())

global_arbs = pd.DataFrame()
for m in refinery_configurations.keys():
    refinery_config = refinery_configurations[m]
    temporary = make_arbs(crudes, destinations, *var, **refinery_config)
    #temporary.columns = pd.MultiIndex.from_product([[m], temporary.columns])
    global_arbs = pd.concat([global_arbs,temporary], axis=1, keys=refinery_configurations.keys())

global_arbs = []
for m in refinery_configurations.keys():
    refinery_config = refinery_configurations[m]
    temporary = make_arbs(crudes, destinations, *var, **refinery_config)
    #temporary.columns = pd.MultiIndex.from_product([[m], temporary.columns])
    global_arbs = pd.concat([global_arbs,temporary], axis=1, keys=refinery_configurations.keys())

refinery_config = refinery_configurations['simple']
temporary = make_arbs(crudes, destinations, *var, **refinery_config)
temporary
res = pd.concat(dict('x':global_arbs, 'y':temporary))
res = pd.concat(dict('x'= global_arbs, 'y'= temporary), axis=1)
res = pd.concat(dict(x= global_arbs, y= temporary), axis=1)
global_arbs = pd.DataFrame()
res = pd.concat(dict(x= global_arbs, y= temporary), axis=1)
refinery_config = refinery_configurations['simple']
global_arbs = make_arbs(crudes, destinations, *var, **refinery_config)
global_arbs.set_index('simple', append=True, inplace=True)
global_arbs.set_index(['simple'], append=True, inplace=True)
global_arbs
global_arbs.set_index(['simple'], append=True, inplace=True, axis=1)
pd.concat(dict(x= global_arbs, y= temporary), axis=1)
pd.concat(dict('simple'= global_arbs, y= temporary), axis=1)
pd.concat(dict(simple = global_arbs, y= temporary), axis=1)
temporary
dict(simple = global_arbs)
mike = {}
refinery_config = refinery_configurations['simple']
global_arbs = make_arbs(crudes, destinations, *var, **refinery_config)
mike['simple'] = global_arbs
mike
refinery_configurations.keys()
refinery_configurations
by_ref_type = {}
for m in refinery_configurations.keys():  
    refinery_config = refinery_configurations[m]
    global_arbs = make_arbs(crudes, destinations, *var, **refinery_config)
    by_ref_type[m] = global_arbs

by_ref_type
refinery_configurations
refinery_configurations.keys()
by_ref_type
by_ref_type.keys()       
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs7.xlsx')
by_ref_type.to_excel(writer, sheet_name='Arbs')
writer.save()
pd.from_dict(by_ref_type)  
pd.DataFrame.from_dict(by_ref_type)       
make_arbs(crudes, destinations, *var, **refinery_config)
by_ref_type = {}
for m in refinery_configurations.keys():  
    refinery_config = refinery_configurations[m]
    global_arbs = make_arbs(crudes, destinations, *var, **refinery_config)
    by_ref_type[m] = global_arbs

    m='simple'
refinery_config = refinery_configurations[m]
global_arbs = make_arbs(crudes, destinations, *var, **refinery_config)
by_ref_type[m] = global_arbs
    m='complex'
refinery_config = refinery_configurations[m]
global_arbs = make_arbs(crudes, destinations, *var, **refinery_config)
by_ref_type[m] = global_arbs
by_ref_type
    m='complex'
refinery_config = refinery_configurations[m]
refinery_config
global_arbs = make_arbs(crudes, destinations, *var, **refinery_config)
by_ref_type
by_ref_type.head()
by_ref_type.keys()
by_ref_type.items()
by_ref_type['simple']
by_ref_type['complex']
by_ref_type['simple'].head()
by_ref_type['simple'].head()
by_ref_type['complex'].head()
by_ref_type['simple'].head()
by_ref_type['complex'].head()
by_ref_type['complex']['Azeri'].head()
by_ref_type['simple']['Azeri'].head()
by_ref_type['complex']['Azeri'].head()
pd.DataFrame(by_ref_type)
def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    
    base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
        'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4}}
        #'Houston':{'Canadian Heavy':0.1,'Castilla':0.15,'Basrah Light':0.125,'Arab Medium':0.125,'LLS':0.25,'Qua Iboe':0.125,'WTI Midland':0.125}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values

global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs7.xlsx')
global_arbs.to_excel(writer, sheet_name='Arbs')
writer.save()
def gpw_calculation(crude, destination, assay, ws, ports, total, rate_data, sub_to_ws, df, **refinery_config):
    
    def standard_ref(crude, refinery_volume, reformer_capacity, fcc_capacity, coker_capacity, lvn_gasolinepool, kero_gasolinepool):
        
        def cdu():
            ### Fractions for gaosline pool       
            hvn_input = assay[crude]['HVN'] * refinery_volume
            kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
            lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
            reformer_input = hvn_input + kero_input + lvn_input
            vgo_input = assay[crude]['VGO'] * refinery_volume
            coker_input = assay[crude]['RESIDUE'] * refinery_volume
            
            return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
        
        def reformer():
            reformer_output = {}
            reformer_assay_standard = {
                'c3': {'yield': 0.025},
                'c4': {'yield': 0.025},
                'h2': {'yield': 0.0},
                'btx': {'yield': 0.0},
                'gasoline': {'yield': 0.95}
                }
            utilised_ref_cap = reformer_capacity * 0.97
            reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
            
            """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
            which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
            kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
            the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
            of the surplus"""
            
            reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
            reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
            reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
            reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
            reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
            reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
            reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
            
            return reformer_output
        
        def coker():
            coker_output = {}
            coker_assay_standard = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lvn': {'yield': 0.09},
                'hvn': {'yield': 0.11},
                'lgo': {'yield': 0.15},
                'hgo': {'yield': 0.35}
                }
            utilised_coker_cap = coker_capacity * 0.97
            coker_volume = np.minimum(utilised_coker_cap, coker_input)
            
            coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
            coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
            coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
            coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
            coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
            coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
            coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
            
            return coker_output
        
        
        def fcc():
            fcc_output = {}
            
            fcc_assay_standard = {
                'c3': {'yield': 0.01},
                'c4': {'yield': 0.01},
                'c3__': {'yield': 0.02},
                'lco': {'yield': 0.20},
                'clo': {'yield': 0.20},
                'gasoline': {'yield': 0.56}
                }
            
            fcc_assay_usgc = {
                'c3': {'yield': 0.015},
                'c4': {'yield': 0.015},
                'lco': {'yield': 0.12},
                'clo': {'yield': 0.08},
                'gasoline': {'yield': 0.75}
                }
            
            utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
            fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
            
            if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).iat[0] is 'HOUSTON':
                fcc_assay = fcc_assay_usgc
            else:
                fcc_assay = fcc_assay_standard
            
            
            fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
            fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
            fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
            fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
            fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
            fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
            fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
            
            return fcc_output
        
        
        def alkylation_unit():
            pass
        
        def hydrocracker():
            pass
        
        def fo_blend():
            residue = {}
            residue['volume'] = coker_output['surplus_resid']
            
            # HDS removing suplhur from the bbl before processing
            low_HDS_factor = 0.5
            high_HDS_factor = 0.8
            if assay[crude]['RESIDUE_sulphur'] < 0.035:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
            else:
                residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
            
            residue['density'] = assay[crude]['RESIDUE_density'] 
            residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
            residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
            
            # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
            imported_vgo = 0  
            diluent_used = 0       
            diluent = {
                    'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                    'sulphur': 0.001,
                    'density': 0.897,
                    'viscosity': 3,
                    'lco_for_blending': 0,
                    'target_sulphur': 0,
                    'volume_used': diluent_used
                    }
            
            diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
            diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
            
            # create the 'cost' function - what do we want to minimise?
            def target_viscosity(diluent_used):
                return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
            
            # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
            cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                     {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
            
            # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
            res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
            diluent['volume_used'] = res.x[0]
            diluent['cst'] = res.fun
            
            # =============================================================================
            # Left blank for the addition of if vgo is needed to be imported
            # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
            # 
            # if target_viscosity(diluent_used) > 380:
            #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
            #     
            # =============================================================================
            
            diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
            diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
            diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
            residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
            residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
            
            combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
            combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
            diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
            
            return diluent
        
        def calculate_yields():
            yields = {}  
            yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
                  + reformer_output['propane'] 
                  + fcc_output['propane'] 
                  + fcc_output['c3__'] * 0.5
                  ) / refinery_volume
            
            yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
                   + reformer_output['butane'] 
                   + fcc_output['butane']
                   + fcc_output['c3__'] * 0.5
                   ) / refinery_volume
            
            yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                    + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                    ) / refinery_volume
            
            #yields['btx'] = (reformer_output['btx']) / refinery_volume
            
            yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
            yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
            yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
            yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
            yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
            
            return yields

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
def standard_ref(crude, refinery_volume, reformer_capacity, fcc_capacity, coker_capacity, lvn_gasolinepool, kero_gasolinepool):
    
    def cdu():
        ### Fractions for gaosline pool       
        hvn_input = assay[crude]['HVN'] * refinery_volume
        kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
        lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
        reformer_input = hvn_input + kero_input + lvn_input
        vgo_input = assay[crude]['VGO'] * refinery_volume
        coker_input = assay[crude]['RESIDUE'] * refinery_volume
        
        return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input
    
    def reformer():
        reformer_output = {}
        reformer_assay_standard = {
            'c3': {'yield': 0.025},
            'c4': {'yield': 0.025},
            'h2': {'yield': 0.0},
            'btx': {'yield': 0.0},
            'gasoline': {'yield': 0.95}
            }
        utilised_ref_cap = reformer_capacity * 0.97
        reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
        
        """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
        which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
        kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
        the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
        of the surplus"""
        
        reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
        reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
        reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
        reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
        reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
        reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
        reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
        
        return reformer_output
    
    def coker():
        coker_output = {}
        coker_assay_standard = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lvn': {'yield': 0.09},
            'hvn': {'yield': 0.11},
            'lgo': {'yield': 0.15},
            'hgo': {'yield': 0.35}
            }
        utilised_coker_cap = coker_capacity * 0.97
        coker_volume = np.minimum(utilised_coker_cap, coker_input)
        
        coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
        coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
        coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
        coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
        coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
        coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
        coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
        
        return coker_output
    
    
    def fcc():
        fcc_output = {}
        
        fcc_assay_standard = {
            'c3': {'yield': 0.01},
            'c4': {'yield': 0.01},
            'c3__': {'yield': 0.02},
            'lco': {'yield': 0.20},
            'clo': {'yield': 0.20},
            'gasoline': {'yield': 0.56}
            }
        
        fcc_assay_usgc = {
            'c3': {'yield': 0.015},
            'c4': {'yield': 0.015},
            'lco': {'yield': 0.12},
            'clo': {'yield': 0.08},
            'gasoline': {'yield': 0.75}
            }
        
        utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
        fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
        
        if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).iat[0] is 'HOUSTON':
            fcc_assay = fcc_assay_usgc
        else:
            fcc_assay = fcc_assay_standard
        
        
        fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
        fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
        fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
        fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
        fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
        fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
        fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
        
        return fcc_output
    
    
    def alkylation_unit():
        pass
    
    def hydrocracker():
        pass
    
    def fo_blend():
        residue = {}
        residue['volume'] = coker_output['surplus_resid']
        
        # HDS removing suplhur from the bbl before processing
        low_HDS_factor = 0.5
        high_HDS_factor = 0.8
        if assay[crude]['RESIDUE_sulphur'] < 0.035:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
        else:
            residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
        
        residue['density'] = assay[crude]['RESIDUE_density'] 
        residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
        residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
        
        # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
        imported_vgo = 0  
        diluent_used = 0       
        diluent = {
                'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
                'sulphur': 0.001,
                'density': 0.897,
                'viscosity': 3,
                'lco_for_blending': 0,
                'target_sulphur': 0,
                'volume_used': diluent_used
                }
        
        diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
        diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
        
        # create the 'cost' function - what do we want to minimise?
        def target_viscosity(diluent_used):
            return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
        
        # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
        cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
                 {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
        
        # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
        res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
        diluent['volume_used'] = res.x[0]
        diluent['cst'] = res.fun
        
        # =============================================================================
        # Left blank for the addition of if vgo is needed to be imported
        # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
        # 
        # if target_viscosity(diluent_used) > 380:
        #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
        #     
        # =============================================================================
        
        diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
        diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
        diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
        residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
        residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
        
        combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
        combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
        diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
        
        return diluent
    
    def calculate_yields():
        yields = {}  
        yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
              + reformer_output['propane'] 
              + fcc_output['propane'] 
              + fcc_output['c3__'] * 0.5
              ) / refinery_volume
        
        yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
               + reformer_output['butane'] 
               + fcc_output['butane']
               + fcc_output['c3__'] * 0.5
               ) / refinery_volume
        
        yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
                + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
                ) / refinery_volume
        
        #yields['btx'] = (reformer_output['btx']) / refinery_volume
        
        yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
        yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
        yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
        yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
        yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
        
        return yields

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input + lvn_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    coker_input = assay[crude]['RESIDUE'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input

def cdu():
    ### Fractions for gaosline pool       
    hvn_input = assay[crude]['HVN'] * refinery_volume
    kero_input = assay[crude]['KERO'] * kero_gasolinepool * refinery_volume
    lvn_input = assay[crude]['LVN'] * lvn_gasolinepool * refinery_volume
    reformer_input = hvn_input + kero_input + lvn_input
    vgo_input = assay[crude]['VGO'] * refinery_volume
    coker_input = assay[crude]['RESIDUE'] * refinery_volume
    
    return refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input


def reformer():
    reformer_output = {}
    reformer_assay_standard = {
        'c3': {'yield': 0.025},
        'c4': {'yield': 0.025},
        'h2': {'yield': 0.0},
        'btx': {'yield': 0.0},
        'gasoline': {'yield': 0.95}
        }
    utilised_ref_cap = reformer_capacity * 0.97
    reformer_volume = np.minimum(utilised_ref_cap, reformer_input)
    
    """if the reformer input is greater than the capacity, say 50kbd vs 48kbd, this means 2kbd is surplus. This 2kbs is then compared to the kero input
    which is part of the feed for the reformer. We can't have more surplus jet than the volume put in so we compare the additional amount to the
    kero_input. If the kero input is larger than the surplus, we assume all additional goes into the jet pool, however, if the ref_input vs the cap is higher than
    the kero input, then the minimum clause says just take the kero input as it will be the smaller of the two and then the max means we take that as the volume 
    of the surplus"""
    
    reformer_output['surplus_for_jet'] = np.maximum(np.minimum(reformer_input - utilised_ref_cap, kero_input), 0)
    reformer_output['surplus_for_naphtha'] = np.maximum(hvn_input - utilised_ref_cap, 0)
    reformer_output['propane'] = reformer_assay_standard['c3']['yield'] * reformer_volume 
    reformer_output['butane'] = reformer_assay_standard['c4']['yield'] * reformer_volume 
    reformer_output['h2'] = reformer_assay_standard['h2']['yield'] * reformer_volume
    reformer_output['btx'] = reformer_assay_standard['btx']['yield'] * reformer_volume
    reformer_output['gasoline'] = reformer_assay_standard['gasoline']['yield'] * reformer_volume
    
    return reformer_output


def coker():
    coker_output = {}
    coker_assay_standard = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lvn': {'yield': 0.09},
        'hvn': {'yield': 0.11},
        'lgo': {'yield': 0.15},
        'hgo': {'yield': 0.35}
        }
    utilised_coker_cap = coker_capacity * 0.97
    coker_volume = np.minimum(utilised_coker_cap, coker_input)
    
    coker_output['surplus_resid'] = np.maximum(coker_input - utilised_coker_cap,0)
    coker_output['propane'] = coker_assay_standard['c3']['yield'] * coker_volume
    coker_output['butane'] = coker_assay_standard['c4']['yield'] * coker_volume
    coker_output['lvn'] = coker_assay_standard['lvn']['yield'] * coker_volume
    coker_output['hvn'] = coker_assay_standard['hvn']['yield'] * coker_volume
    coker_output['lgo'] = coker_assay_standard['lgo']['yield'] * coker_volume
    coker_output['hgo'] = coker_assay_standard['hgo']['yield'] * coker_volume
    
    return coker_output



def fcc():
    fcc_output = {}
    
    fcc_assay_standard = {
        'c3': {'yield': 0.01},
        'c4': {'yield': 0.01},
        'c3__': {'yield': 0.02},
        'lco': {'yield': 0.20},
        'clo': {'yield': 0.20},
        'gasoline': {'yield': 0.56}
        }
    
    fcc_assay_usgc = {
        'c3': {'yield': 0.015},
        'c4': {'yield': 0.015},
        'lco': {'yield': 0.12},
        'clo': {'yield': 0.08},
        'gasoline': {'yield': 0.75}
        }
    
    utilised_fcc_cap = fcc_capacity * 0.97 # this is 97% ulitilisation of total FCC capacity
    fcc_volume = np.minimum(utilised_fcc_cap, vgo_input + coker_output['hgo'])
    
    if  ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).iat[0] is 'HOUSTON':
        fcc_assay = fcc_assay_usgc
    else:
        fcc_assay = fcc_assay_standard
    
    
    fcc_output['surplus_vgo'] = np.maximum(vgo_input + coker_output['hgo'] - utilised_fcc_cap, 0)
    fcc_output['propane'] = fcc_assay['c3']['yield'] * fcc_volume
    fcc_output['butane'] = fcc_assay['c4']['yield'] * fcc_volume
    fcc_output['c3__'] = fcc_assay['c3__']['yield'] * fcc_volume
    fcc_output['lco']  = fcc_assay['lco']['yield'] * fcc_volume
    fcc_output['clo']  = fcc_assay['clo']['yield'] * fcc_volume
    fcc_output['gasoline'] = fcc_assay['gasoline']['yield'] * fcc_volume
    
    return fcc_output



def alkylation_unit():
    pass


def hydrocracker():
    pass


def fo_blend():
    residue = {}
    residue['volume'] = coker_output['surplus_resid']
    
    # HDS removing suplhur from the bbl before processing
    low_HDS_factor = 0.5
    high_HDS_factor = 0.8
    if assay[crude]['RESIDUE_sulphur'] < 0.035:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * high_HDS_factor
    else:
        residue['sulphur'] = assay[crude]['RESIDUE_sulphur'] * low_HDS_factor
    
    residue['density'] = assay[crude]['RESIDUE_density'] 
    residue['sulphurW'] = residue['volume'] * residue['sulphur'] * residue['density'] * 1000 / 6.29 # convert the bbls volume to tonnes
    residue['viscosity_index'] = 23.1 + np.log10(np.log10(assay[crude]['RESIDUE_v50']+0.8))*33.47 # this is a set conversion calculation
    
    # certain vis and FO sulphur % avail. We want to use the low sulphur slurry oil or if need the LCO to blend down the resid
    imported_vgo = 0  
    diluent_used = 0       
    diluent = {
            'volume': fcc_output['surplus_vgo'] + fcc_output['clo'] + imported_vgo,
            'sulphur': 0.001,
            'density': 0.897,
            'viscosity': 3,
            'lco_for_blending': 0,
            'target_sulphur': 0,
            'volume_used': diluent_used
            }
    
    diluent['viscosity_index'] = 23.1 + np.log10(np.log10(diluent['viscosity'] + 0.8)) * 33.47
    diluent['sulphurW'] = diluent['volume'] * diluent['sulphur'] * diluent['density'] * 1000 / 6.29
    
    # create the 'cost' function - what do we want to minimise?
    def target_viscosity(diluent_used):
        return 10 ** (10 ** (((diluent_used / (diluent_used + residue['volume']) * diluent['viscosity_index'] + residue['volume'] / (diluent_used + residue['volume']) * residue['viscosity_index'])-23.1)/33.47)) -0.8
    
    # we want to minimise the target viscosity subject to i) the amount of diluent we have avliable to blend and ii) giving away as little as possible so cst at 375
    cons = ({'type': 'ineq', 'fun': lambda diluent_used: diluent['volume'] - diluent_used},
             {'type': 'ineq', 'fun': lambda diluent_used: target_viscosity(diluent_used) - 380})  
    
    # save the output from the optimisation function to res then take the optimized value and pass it into diluent_used
    res = minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)     
    diluent['volume_used'] = res.x[0]
    diluent['cst'] = res.fun
    
    # =============================================================================
    # Left blank for the addition of if vgo is needed to be imported
    # Essentially need to take what the vicosity etc is after the first minimise function and then minimised again without the first constraint 
    # 
    # if target_viscosity(diluent_used) > 380:
    #     minimize(lambda diluent_used: target_viscosity(diluent_used), [0], method = 'COBYLA', constraints=cons)
    #     
    # =============================================================================
    
    diluent['surplus_after_blend'] = diluent['volume'] - diluent['volume_used'] # need to have an optimizer function of some sort                        
    diluent['sulphur_per_bbl'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29 * diluent['sulphur']
    diluent['weight'] = diluent['volume_used'] * diluent['density'] * 1000 / 6.29
    residue['sulphur_per_bbl'] = residue['volume'] * residue['density'] * 1000 / 6.29 * residue['sulphur']
    residue['weight'] = residue['volume']  * residue['density'] *1000 / 6.29
    
    combined_fo_sulphur = diluent['sulphur_per_bbl'] + residue['sulphur_per_bbl'] 
    combined_fo_sulphur_weight = diluent['weight'] + residue['weight']
    diluent['target_sulphur'] = combined_fo_sulphur / combined_fo_sulphur_weight
    
    return diluent

cdu()
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
refinery_config
refinery_volume = refinery_config['refinery_volume']
reformer_capacity = refinery_config['reformer_capacity']
fcc_capacity = refinery_config['fcc_capacity']
coker_capacity = refinery_config['coker_capacity']
lvn_gasolinepool = refinery_config['lvn_gasolinepool']
kero_gasolinepool = refinery_config['kero_gasolinepool']
yields = {}  
yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
      + reformer_output['propane'] 
      + fcc_output['propane'] 
      + fcc_output['c3__'] * 0.5
      ) / refinery_volume

yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
       + reformer_output['butane'] 
       + fcc_output['butane']
       + fcc_output['c3__'] * 0.5
       ) / refinery_volume

yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
        + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
        ) / refinery_volume

#yields['btx'] = (reformer_output['btx']) / refinery_volume

yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
reformer_output = reformer()
coker_output = coker()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
def calculate_yields():
    yields = {}  
    yields['propane'] = (assay[crude]['LPG'] * refinery_volume * 0.5 
          + reformer_output['propane'] 
          + fcc_output['propane'] 
          + fcc_output['c3__'] * 0.5
          ) / refinery_volume
    
    yields['butane'] = (assay[crude]['LPG'] * refinery_volume * 0.5
           + reformer_output['butane'] 
           + fcc_output['butane']
           + fcc_output['c3__'] * 0.5
           ) / refinery_volume
    
    yields['naphtha'] = (assay[crude]['LVN'] * refinery_volume * (1-lvn_gasolinepool)
            + reformer_output['surplus_for_naphtha'] + coker_output['lvn']
            ) / refinery_volume
    
    #yields['btx'] = (reformer_output['btx']) / refinery_volume
    
    yields['gasoline'] = (reformer_output['gasoline'] + fcc_output['gasoline'] + coker_output['hvn']) / refinery_volume
    yields['kero'] = (assay[crude]['KERO'] * (1-kero_gasolinepool) * refinery_volume + reformer_output['surplus_for_jet']) / refinery_volume
    yields['ulsd'] = ((assay[crude]['LGO'] + assay[crude]['HGO']) * refinery_volume + coker_output['lgo']) / refinery_volume 
    yields['gasoil'] = (fcc_output['lco'] - diluent['lco_for_blending'] + diluent['surplus_after_blend']) / refinery_volume
    yields['lsfo'] = (diluent['volume_used'] + assay[crude]['RESIDUE'] * refinery_volume) / refinery_volume
    
    return yields

refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
reformer_output = reformer()
reformer_output
coker_output = coker()
coker_output
fcc_output = fcc()
fcc_output
diluent = fo_blend()
diluent
yields = calculate_yields()
yields
coker_capacity
yields
refinery_config = refinery_configurations['simple']
refinery_volume = refinery_config['refinery_volume']
reformer_capacity = refinery_config['reformer_capacity']
fcc_capacity = refinery_config['fcc_capacity']
coker_capacity = refinery_config['coker_capacity']
lvn_gasolinepool = refinery_config['lvn_gasolinepool']
kero_gasolinepool = refinery_config['kero_gasolinepool']
coker_capacity
refinery_volume, reformer_input, kero_input, hvn_input, vgo_input, lvn_gasolinepool, kero_gasolinepool, coker_input = cdu()
reformer_output = reformer()
coker_output = coker()
fcc_output = fcc()
diluent = fo_blend()
yields = calculate_yields()
yields
refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':20,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram with configs in loop.py', wdir='C:/Users/mima/.spyder-py3')
coker_input
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
from GPW2504 import gpw_calculation
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
from GPW2504 import gpw_calculation
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

#destinations = ['Houston','Rotterdam','Augusta','Singapore']


destinations = ['Rotterdam','Augusta']
#crudes = ['Brent','Azeri','CPC Blend']

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs['simple']['Azeri']['Rotterdam']
global_arbs['simple']['Azeri']['Rotterdam']['Suez_margin_advantage']
global_arbs['simple']['Azeri']['Rotterdam']['Afra_margin_advantage']
global_arbs['simple']['Azeri']['Rotterdam']['Suez_margin_advantage'].tail()
global_arbs['complex']['Azeri']['Rotterdam']['Suez_margin_advantage'].tail()
global_arbs['simple']['Azeri']['Rotterdam']['Suez_margin_advantage'].tail()
global_arbs['complex']['Azeri']['Rotterdam']['Suez_margin_advantage'].tail()
global_arbs['complex']['Azeri']['Rotterdam'].tail()
global_arbs['simple']['Azeri']['Rotterdam'].tail()
global_arbs['simple']['Azeri']['Rotterdam'].tail()
global_arbs['complex']['Azeri']['Rotterdam'].tail()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs.iloc[:, global_arbs.columns.get_level_values(0)=='simple']
global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
crudes
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
afra_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Afra_margin_advantage']

#global_arbs['simple']['Azeri']['Rotterdam'].tail()
#global_arbs['complex']['Azeri']['Rotterdam'].tail()

writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs8.xlsx')
global_arbs.to_excel(writer, sheet_name='Arbs')
suez_margins.to_excel(writer, sheet_name='Suez Adv')
afra_margins.to_excel(writer, sheet_name='Afra Adv')
writer.save()
global_arbs
import numpy as np 
import pandas as pd
from scipy.optimize import minimize
from ArbEcons2504 import import_data

crude = 'Azeri'
destination = 'Rotterdam'
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()


np.seterr(divide='ignore', invalid='ignore')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
var = import_data()
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    #ws_table = pd.read_excel(data, 'ws_table', header = 1)
    #rate_data = pd.read_excel(data, 'flat_rate')
    #prices = pd.read_excel(data, 'prices', header = 1)
    #paper_prices = pd.read_excel(data, 'paper prices', header = 1)
    expiry_table = pd.read_excel(data, 'expiry')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
    total.index = pd.to_datetime(total.index)
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    def apply_expiry(x):
        return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                                 &(expiry_table['Month'].dt.year == x[0]),
                                 'Expiry']).iat[0]
    
    index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
    df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df

import_data()
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx'
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)
df = pd.DataFrame(index=total.index)
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]

index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
expiry_table
x
apply_expiry
apply_along_axis
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
expiry_table['Month'].dt.month
index_for_func
np.apply_along_axis(apply_expiry,1,index_for_func)
apply_expiry
index_for_func
df
total
df.index.year
df.index.month
df = pd.DataFrame(index=total.index)
df['x'] = pd.DataFrame(index=total.index)
df = pd.DataFrame(index=total.index)
expiry_table
index_for_func

## ---(Tue May 15 19:34:49 2018)---
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
expiry_table
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)

## ---(Tue May 15 19:50:06 2018)---
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
df = pd.DataFrame(index=total.index, columns = ['Expiry'])
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index, columns = ['Expiry'])
df = pd.DataFrame(index=total.index, columns = ['Expiry'])
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
index_for_func
len(index_for_func)
len(df)
expiry_table
(expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                         &(expiry_table['Month'].dt.year == x[0]),
                         'Expiry']).iat[0]

## ---(Tue May 22 11:22:33 2018)---
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index, columns = ['Expiry'])
def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()

df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)

## ---(Tue May 22 11:49:53 2018)---
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index, columns = ['Expiry'])

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()

df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)


""" USE NP WHERE AND DO WHEN YOU GET BACK FROM HOL"""

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index, columns = ['Expiry'])

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])
                             &(expiry_table['Month'].dt.year == x[0]),
                             'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()

df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)


""" USE NP WHERE AND DO WHEN YOU GET BACK FROM HOL"""

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df
df = pd.DataFrame(index=total.index)
df
df['Date'] = df.index
df
df['Date'] 
df['Date'][0]
f['Date'][0][0]
df['Date'][0][0]
df['Date'].month
x = df['Date'].to_datetime()
df['Date'].describe
df['Date'].dt.month
x = datetime(2015,1,1)
expiry_table.loc[(df['Date'].dt.month == x[1])&(df['Date'].dt.year == x[0])]
x = dt(2015,1,1)
expiry_table.loc[(df['Date'].dt.month == x[1])&(df['Date'].dt.year == x[0])]
x
x.dt.year
x.year
x.month
expiry_table.loc[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
expiry_table
expiry_table.iloc[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
expiry_table
expiry_table[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
df['Date'].dt.month
x.month
df['Date'].dt.year
x.year
(df['Date'].dt.year == x.year)
(df['Date'].dt.month == x.month)
x = dt(2015,5,1)
expiry_table[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
expiry_table
expiry_table[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]['Expiry']
expiry_table.loc[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
expiry_table
expiry_table['Month']
expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&(expiry_table['Month'].dt.year == x.year)]
df['Expiry'] = expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&(expiry_table['Month'].dt.year == x.year)]['Expiry']
df['Expiry']
expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&(expiry_table['Month'].dt.year == x.year)]['Expiry'].iat[0]
df['Expiry'] = expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&(expiry_table['Month'].dt.year == x.year)]['Expiry'].iat[0]
df
expiry_table['Month'].dt.month
df['Date'].dt.month
expiry_table
df['Expiry'] = np.where(((df['Date'].dt.month == expiry_table['Month'].dt.month)&(df['Date'].dt.year == expiry_table['Month'].dt.year)), expiry_table['Expiry'])
expiry_table
expiry_dict = expiry_table.to_dict()
expiry_dict
expiry_dict = expiry_table.to_dict(orient='records')
expiry_dict
expiry_dict[0]
expiry_dict = expiry_table.to_dict(orient='list')
expiry_dict[0]
expiry_dict
expiry_dict = expiry_table.to_dict(orient='series')
expiry_dict
expiry_dict = expiry_table.to_dict(orient='series')
expiry_dict
expiry_dict[0]
expiry_dict = expiry_table.to_dict(orient='series')
expiry_dict = expiry_table.to_dict()
expiry_table = pd.read_excel(data, index_col = 'month').to_dict('index')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'month').to_dict('index')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month').to_dict('index')
expiry_table
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month').to_dict()
expiry_table
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month').to_dict('records')
expiry_table
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month').to_dict('index')
expiry_table
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month').to_dict('list')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month').to_dict()
pd.read_excel(data, 'expiry', index_col = 'Month')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month', header=None).to_dict()
pd.read_excel(data, 'expiry', index_col = 'Month', header=None)
pd.read_excel(data, 'expiry', header=None)
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month').to_dict()
pd.read_excel(data, 'expiry', index_col = 'Month').to_dict()
expiry_table[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
expiry_table
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
expiry_table
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
expiry_table[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
df['Date'].dt.month
expiry_table[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]

expiry_table[(df['Date'].dt.month == x.month)&(df['Date'].dt.year == x.year)]
expiry_table.loc[(expiry_table['Month'].dt.month == x.month)&(expiry_table['Month'].dt.year == x.year)]['Expiry'].iat[0]
expiry_table
expiry_table.index.dt.month
expiry_table.index.month
expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry'].iat[0]
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
expiry_table
expiry_table.loc[(expiry_table['Month'].dt.month == x.month)
                         &(expiry_table['Month'].dt.year == x.year),
                         'Expiry']).iat[0]
expiry_table.loc[(expiry_table['Month'].dt.month == x.month)
                         &(expiry_table['Month'].dt.year == x.year)]
                         ['Expiry']).iat[0]
expiry_table
(expiry_table.loc[(expiry_table.index.month == x.month)
                         &(expiry_table.index.year == x.year)]['Expiry']).iat[0]
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
for_dates
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
df['Expiry'] = df.index.apply(for_dates
df['Expiry'] = df.index.apply(for_dates)
df['Expiry'] = df['Date'].apply(for_dates)
(expiry_table.loc[(expiry_table.index.month == x.month)
                         &(expiry_table.index.year == x.year)]['Expiry'])
(expiry_table.loc[(expiry_table.index.month == x.month)
                         &(expiry_table.index.year == x.year)]['Expiry']).value
(expiry_table.loc[(expiry_table.index.month == x.month)
                         &(expiry_table.index.year == x.year)]['Expiry']).values
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).values
df['Expiry'] = df['Date'].apply(for_dates)
df
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
df['Expiry'] = df['Date'].apply(for_dates)
expiry_table
df['Date'].dt.year
atetime(df['Date'].dt.year, df['Date'].dt.month, 1)
datetime(df['Date'].dt.year, df['Date'].dt.month, 1)
dt(df['Date'].dt.year, df['Date'].dt.month, 1)
dt(2015,1,1)
(expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).values
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).values()
(expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).values()
(expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).values
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
#ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
#prices = pd.read_excel(data, 'prices', header = 1)
#paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
df['Expiry'] = df['Date'].apply(for_dates)
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
df['Expiry'] = df['Date'].apply(for_dates)
df = pd.DataFrame(index=total.index)
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
pd.read_excel(data, 'price_warehouse', header = 4)
total = pd.read_excel(data, 'price_warehouse', header = 4).drop('Timestamp', axis=0).fillna(method='ffill')
total.index
total = pd.read_excel(data, 'price_warehouse', header = 4).fillna(method='ffill')
total.iloc[:2,:]
total.iloc[2:,:]
total.iloc[:,0]
total.index = total.iloc[:,0]
total
total = pd.read_excel(data, 'price_warehouse', header = 4).fillna(method='ffill')
total = total.iloc[2:,:]
total
total.set_index([[0]])
data = pd.ExcelFile('C://Users//mima//Desktop//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
pd.read_excel(data, 'price_warehouse', header = 4).fillna(method='ffill')
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')
total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')
total
total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
df['Expiry'] = df['Date'].apply(for_dates)
df['Expiry']
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Desktop//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Desktop//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df
df.drop(['Date'], inplace=True)
df.drop(['Date'], inplace=True, axis=1)
df
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
calculate_flat_rates
i
k
j
base_blends.keys()
df
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Desktop//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Desktop//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)

"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
total['CLc1']
data = pd.ExcelFile('C://Users//mima//Desktop//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pypyodbc
import pandas as pd


cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''Select * From view_CargoAnalysis_External_Crude_Kb'''

flows_data = pd.read_sql(cxn, query)
import pyodbc
import pandas as pd


cxn = pypyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''Select * From view_CargoAnalysis_External_Crude_Kb'''

flows_data = pd.read_sql(cxn, query)
import pandas as pd


cxn = pyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''Select * From view_CargoAnalysis_External_Crude_Kb'''

flows_data = pd.read_sql(cxn, query)
import pyodbc
import pandas as pd


cxn = pyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = '''Select * From view_CargoAnalysis_External_Crude_Kb'''

flows_data = pd.read_sql(cxn, query)
runfile('C:/Users/mima/.spyder-py3/Vessel Tracking.py', wdir='C:/Users/mima/.spyder-py3')
flows_data.head()
flows_data.columns
flows_data.LoadDate
flows_data.LoadMonth
flows_data['LoadCountry'].unique()
flows_data['LoadCountry'].unique().sort()
flows_data['LoadCountry'].unique()
flows_data[flows_data['LoadCountry'] == 'Iran']
iranian_flows = flows_data[flows_data['LoadCountry'] == 'Iran']
iran_f = flows_data[flows_data['LoadCountry'] == 'Iran']
iran_f.columns
iran_p = iran_f.pivot(index='LoadDate', columns=['DischargeRegion','DischargeSubRegion','DischargeCountry','CurrentOwner'], values = 'CleanQuantity')
iran_f
flows_data['DischargeRegion'].unique()
iran_p = iran_f.pivot_table(index='LoadDate', columns=['DischargeRegion','DischargeSubRegion','DischargeCountry','CurrentOwner'], values = 'CleanQuantity')
iran_p.head()
iran_p = iran_f.pivot_table(index='LoadDate', columns=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
iran_p = iran_f.pivot_table(index='LoadDate', columns=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
iran_p.head()
iran_p.loc[:,('ASIA','CHINA')]
iran_p
iran_p.loc[:,('ASIA','China')]
iran_p.loc[:,('ASIA','China')].columns
iran_p.head()
iran_p.loc[:,('ASIA','China')].head()
test = iran_p.loc[:,('ASIA','China')].head()
test = iran_p.loc[:,('ASIA','China')]
test.columns
test = iran_p.loc[:,('ASIA')]
test.columns
test.groupby(level=0)
iran_p.groupby(level=0, axis=0)
mike = iran_p.groupby(level=0, axis=0)
mike
print(mike)
mike = iran_p.groupby(level=0, axis=0).mean()
print(mike)
mike = iran_p.groupby(level=[0,1,2], axis=0).mean()
print(mike)
iran_p = iran_f.pivot_table(index='LoadDate', columns=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
mike = iran_p.groupby(level=[0,1,2], axis=1).mean()
print(mike)
iran_p.columns
mike.columns
iran_p.get_level_values(0)
iran_p.index.get_level_values(0)
iran_p.index.get_level_values(0, axis=1)
iran_p.index
iran_p.set_index(['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], inplace=True)
reindexed = iran_p.set_index(['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'])
iran_p = iran_f.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
iran_p.get_level_values(0)
iran_p.index.get_level_values(0)
iran_p.index.get_level_values([0,1])
iran_p.index.get_level_values(1)
iran_p.index.to_list()
iran_p.index.tolist()
[(a,b,c) for a,b,c,d in iran_p.index.tolist()]
[(a,b,c) for a,b,c,d in iran_p.index.tolist().unique()]
[(a,b,c) for a,b,c,d in iran_p.index.tolist().unique]
[(a,b,c) for a,b,c,d in set(iran_p.index.tolist())]
iran_p.index.get_level_values(1)
iranian_buyers = [(a,b,c) for a,b,c,d in set(iran_p.index.tolist())]
iranian_buyers[0]
i = iranian_buyers[0]
iran_p.loc[:,i]
iran_p
i = iranian_buyers[0]
iran_p.loc[i,:]
iranian_buyers
iran_p.index.tolist()
iranian_buyers_2 = [[a,b,c] for a,b,c,d in set(iran_p.index.tolist())]
iranian_buyers_2 = [[a],[b],[c] for a,b,c,d in set(iran_p.index.tolist())]
iranian_buyers_2 = [[a][b][c] for a,b,c,d in set(iran_p.index.tolist())]
iranian_buyers_2 = [a for a,b,c,d in set(iran_p.index.tolist())]
i = iranian_buyers[0]
iran_p.loc[i,:]
iran_p.loc[i,:]
iranian_buyers = [(a,b,c) for a,b,c,d in set(iran_p.index.tolist())]
iranian_buyers
iran_p.loc[i,:]
filtered_list = iran_p.loc[i,:]
i
pd.concat([filtered_list], keys=[i[0],i[1],i[2]], names='mike']
pd.concat([filtered_list], keys=[i[0],i[1],i[2]])
filtered_list = pd.concat([iran_p.loc[i,:]], keys=[i[0],i[1],i[2]])
i[0]
i[1]
i[2]
filtered_list
filtered_list = pd.concat([iran_p.loc[i,:]], keys=[str(i[0]),str(i[1]),str(i[2])])
i
iranian_buyers = [(a,b,c) for a,b,c,d in set(iran_p.index.tolist())]
i = iranian_buyers[0]

filtered_list = iran_p.loc[i,:]
filtered_list = pd.concat([iran_p.loc[i,:]], keys=[i])
iranian_buyers
filtered_list = pd.concat([iran_p.loc[i,:]], keys=[i])
filtered_list
existing = pd.DataFrame()
filtered_list = pd.concat([iran_p.loc[i,:]], keys=[i])
existing = pd.concat([existing, filtered_list])
iran_p
iran_p = iran_f.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
#iran_p.index.get_level_values(1)


"""generate a list of tuples to pass into larger dataframe"""
iranian_buyers = [(region,country,buyer) for region, country, buyer, grade in set(iran_p.index.tolist())]


#i = iranian_buyers[0]
existing = pd.DataFrame()
for i in iranian_buyers:
    filtered_list = pd.concat([iran_p.loc[i,:]], keys=[i])
    existing = pd.concat([existing, filtered_list])

flows = flows_data.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')


"""pivot table the data on a multiindex colun structure"""
iran_f = flows_data[flows_data['LoadCountry'] == 'Iran']
iran_p = iran_f.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')

#iran_p.index.get_level_values(1)


"""generate a list of tuples to pass into larger dataframe"""
iranian_buyers = [(region,country,buyer) for region, country, buyer, grade in set(iran_p.index.tolist())]


#i = iranian_buyers[0]
existing = pd.DataFrame()
for i in iranian_buyers:
    filtered_list = pd.concat([flows.loc[i,:]], keys=[i])
    existing = pd.concat([existing, filtered_list])

flows['Sulphur'].groupby([flows['CleanGrade']]).mean()
flows_data['Sulphur'].groupby([flows_data['CleanGrade']]).mean()
flows_data['Sulphur'].groupby([flows_data['CleanGrade']]).mean().to_dict()
iran_p
iran_p['CleanGrade']
iran_f
iran_f['CleanGrade'].unique()
iranian_grades
iranian_grades = flows_data[flows_data['LoadCountry'] == 'Iran']['CleanGrade'].unique()
iranian_grades
sulphur_dict
sulphur_dict = flows_data['Sulphur'].groupby([flows_data['CleanGrade']]).mean().to_dict()
i = iranian_grades[0]
sulphur_dict[i]
grades_dict = {}
for i in iranian_grades:
    grades_dict[i] = sulphur_dict[i]

sulphur_dict
grades_dict
sulphur_dict.items()
for i in sulphur_dict.items():
    if i[1] > 1.0:
        substitutes[i[0]] = i[1]

substitutes = {}
for i in sulphur_dict.items():
    if i[1] > 1.0:
        substitutes[i[0]] = i[1]

substitutes
substitutes.keys()
subs = [substitutes.keys()]
subs = list(substitutes.keys())
flows.loc[(slice(None),slice(None),slice(None),subs),:]    
iran_subs_sour = existing.loc[(slice(None),slice(None),slice(None),subs),:]    
iran_subs_sour
iran_subs_sour.T
iran_subs_sour.T.index
iran_subs_sour.T.index.to_datetime()
iran_subs_sour.T.index.pd.to_datetime()
iran_subs_sour.T.index.to_datetime
pd.to_datetime(iran_subs_sour.T.index)
iran_subs_sour = iran_subs_sour.T
iran_subs_sour.index = pd.to_datetime(iran_subs_sour.index)
iran_subs_sour.resample('W', how='mean')
iran_subs_sour.resample('W', how='sum')
iran_subs_sour.get_level_values(2)
iran_subs_sour = iran_subs_sour.resample('W', how='sum')
iran_subs_sour = iran_subs_sour.T
iran_subs_sour
iran_subs_sour.index.get_level_values(2)
iranian_buyers
iran_subs_sour = iran_subs_sour.T
iran_subs_sour
iran_subs_sour.resample('M').sum()

## ---(Thu May 24 10:45:07 2018)---
""" Aim is see for each taker of Iranian:
    1) what volumes they take 
    2) what they subbed with during sanctions
    3) what I think they will replace the volumes with
    """


import pyodbc
import pandas as pd


cxn = pyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = "Select * From view_CargoAnalysis_External_Crude_Kb"
flows_data = pd.read_sql(query, cxn)
flows = flows_data.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')


"""pivot table the data on a multiindex colun structure"""
iran_f = flows_data[flows_data['LoadCountry'] == 'Iran']
iran_p = iran_f.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')

#iran_p.index.get_level_values(1)


"""generate a list of tuples to pass into larger dataframe"""
iranian_buyers = [(region,country,buyer) for region, country, buyer, grade in set(iran_p.index.tolist())]

"""this creates an empty dataframe for use in the loop. We then go through the list we know of iranian buyers and use that to slice the total clipper data and
and then append to the existing. This creates a matrix, with time in days as columns and looking only at the customers we are concerned with"""
#i = iranian_buyers[0]
existing = pd.DataFrame()
for i in iranian_buyers:
    filtered_list = pd.concat([flows.loc[i,:]], keys=[i])
    existing = pd.concat([existing, filtered_list])


""" figure out the average sulphur countent for the grades and the unique Iranian grades"""
sulphur_dict = flows_data['Sulphur'].groupby([flows_data['CleanGrade']]).mean().to_dict()
iranian_grades = flows_data[flows_data['LoadCountry'] == 'Iran']['CleanGrade'].unique()

"""create a dictionary with only the grades we are concerned with just to double check sulphur content"""
grades_dict = {}
for i in iranian_grades:
    grades_dict[i] = sulphur_dict[i]


""" see which grades in our larger dictionary are close to the conditions""" 
substitutes = {}
for i in sulphur_dict.items():
    if i[1] > 1.0:
        substitutes[i[0]] = i[1]


"""with this list of substitutes, we can filter our main list based on who has taken and the possible crudes of substitution we are concerned about"""
subs = list(substitutes.keys())
iran_subs_sour = existing.loc[(slice(None),slice(None),slice(None),subs),:]    
iran_subs_sour = iran_subs_sour.T
iran_subs_sour.index = pd.to_datetime(iran_subs_sour.index)

"""weekly totals based on load volumes"""
iran_subs_sour = iran_subs_sour.resample('W', how='sum')
iran_subs_sour = iran_subs_sour.T
import pyodbc
import pandas as pd


cxn = pyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = "Select * From view_CargoAnalysis_External_Crude_Kb"
flows_data = pd.read_sql(query, cxn)
flows = flows_data.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
iran_exports = flows_data[flows_data['LoadCountry'] == 'Iran']
iran_raw_exports = flows_data[flows_data['LoadCountry'] == 'Iran']
iran_raw_exports = flows_data[flows_data['LoadCountry'] == 'Iran']
iran_exports = iran_raw_exports.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
set(iran_exports.index.tolist())
iran_exports.loc[('ASIA','Japan'),:]
iran_exports.loc[('ASIA','Japan'),:].groupby('CurrentOwner')
iran_exports.loc[('ASIA','Japan'),:].groupby('CurrentOwner').sum()
japan_customers = iran_exports.loc[('ASIA','Japan'),:].groupby('CurrentOwner').sum()
japan_customers.values
japan_customers.columns
japan_customers['sum'] = japan_customers[japan_customers.columns.values].sum(axis=1)
japan_customers['sum']
japan_customers.describe()
japan_customers.T.describe()
japan_customers = iran_exports.loc[('ASIA','Japan'),:].groupby('CurrentOwner').sum
japan_customers.T.describe()
japan_customers = iran_exports.loc[('ASIA','Japan'),:].groupby('CurrentOwner').sum()
japan_customers.T.describe()
iran_countries = iran_exports.groupby('DischargeCountry').sum()
iran_countries
iran_countries = iran_exports.groupby(['DischargeRegion','DischargeCountry']).sum()
iran_countries
iran_countries = iran_exports.groupby(['DischargeRegion','DischargeCountry']).sum().T
iran_countries
iran_countries_montlhy = iran_countries.resample('M').sum()
iran_countries = iran_exports.groupby(['DischargeRegion','DischargeCountry']).sum().T
iran_countries.index = pd.to_datetime(iran_countries.index)
iran_countries_montlhy = iran_countries.resample('M').sum()
iran_countries_montlhy
iran_countries_monthly = iran_countries.resample('M').sum()
iran_countries_monthly
iran_countries_monthly.index.dt.daysinmonth
iran_countries_monthly.index.daysinmonth
iran_countries = iran_exports.groupby(['DischargeRegion','DischargeCountry']).sum().T
iran_countries.index = pd.to_datetime(iran_countries.index)
iran_countries_monthly = iran_countries.resample('M').sum().div(iran_countries_monthly.index.daysinmonth)
iran_exports = iran_raw_exports.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
iran_countries = iran_exports.groupby(['DischargeRegion','DischargeCountry']).sum().T
iran_countries.index = pd.to_datetime(iran_countries.index)
iran_countries_monthly_kbd = iran_countries_monthly.div(iran_countries_monthly.index.daysinmonth)
iran_exports = iran_raw_exports.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
iran_countries = iran_exports.groupby(['DischargeRegion','DischargeCountry']).sum().T
iran_countries.index = pd.to_datetime(iran_countries.index)
iran_countries_monthly = iran_countries.resample('M').sum()
iran_countries_monthly.index.daysinmonth
iran_countries_monthly_kbd = iran_countries_monthly.div(iran_countries_monthly.index.daysinmonth.values)
iran_countries_monthly.
iran_countries_monthly
iran_countries_monthly.index.daysinmonth.values
len(iran_countries_monthly.index.daysinmonth.values)
iran_countries_monthly_kbd = iran_countries_monthly.div(iran_countries_monthly.index.daysinmonth.values, axis=1)
iran_countries_monthly_kbd = iran_countries_monthly.div(iran_countries_monthly.index.daysinmonth.values, axis=0)
iran_countries_monthly_kbd
import pyodbc
import pandas as pd


cxn = pyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = "Select * From view_CargoAnalysis_External_Crude_Kb"

flows_data = pd.read_sql(query, cxn)
flows = flows_data.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
db_file = 'L://TRADING//NSea//Lloyds APEX//APEXDB_V4.mdb'
user = 'admin'
password = ''

#odbc_conn_str = 'DRIVER={Microsoft Access Driver (*.mdb)};DBQ=%s;UID=%s;PWD=%s' %\
#                (db_file, user, password)
# Or, for newer versions of the Access drivers:
odbc_conn_str = 'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=%s;UID=%s;PWD=%s' %\
                (db_file, user, password)

conn = pyodbc.connect(odbc_conn_str)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total.index = pd.to_datetime(total.index)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4, index_col = 'Date').drop(['Timestamp']).fillna(method='ffill')    
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')   
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
calculate_flat_rate()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')   
    total.index = pd.to_datetime(total.index)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



#import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()  
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }

if assay[crude]['Code'] == 'multiple':     
    diff = exceptions[crude][discharge_price_region]['Code']
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

crude = 'Azeri'
destination = 'Rotterdam'
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }

if assay[crude]['Code'] == 'multiple':     
    diff = exceptions[crude][discharge_price_region]['Code']
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight

def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
        return df_freight

calculate_flat_rate()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight

calculate_flat_rate()
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

calculate_flat_rate()
rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
          (rate_data['DischargePort'] == destination)]
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')   
    total.index = pd.to_datetime(total.index)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



#import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()  
crude = 'Azeri'
destination = 'Rotterdam'
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2

exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }

if assay[crude]['Code'] == 'multiple':     
    diff = exceptions[crude][discharge_price_region]['Code']
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

flat_rate_table
flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
          (rate_data['DischargePort'] == destination)]
flat_rate_table
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])

v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
ports_with_rates
var
var[4]
var[3]
var[3]['AAVMR00']
var[3]['AAVMR00'][20]
var[3]['AAVMR00'][20:]
var[3]
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp']).fillna(method='ffill')   
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
total = total.fillna(method='ffill')
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
total = total.sort_values(by=total.index).fillna(method='ffill')
total.sort_values(by=total.index)
total.sort_values(index)
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True).fillna(method='ffill')
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
    
    total.index = pd.to_datetime(total.index)
    total.sort_index(inplace=True)
    total.fillna(method='ffill', inplace=True)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



#import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()    
    crude = 'Azeri'
    destination = 'Rotterdam'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    if assay[crude]['Code'] == 'multiple':     
        diff = exceptions[crude][discharge_price_region]['Code']
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['diff'] = diff
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            elif crude_vs in index_dub:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    def create_landed_values():
        temp = pd.concat([df_prices,df_freight], axis=1)
        price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
        freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
        #for i in df_prices.columns:
        for k in freight_list:
            name = k[:4]+"_landed_"+price_index
            temp[name] = df_prices[price_index] + df_freight[k]   
        return temp
df_freight = construct_freight()
df_freight
df_prices = convert_prices()
df_prices
df_prices = create_landed_values()
df_prices
temp = pd.concat([df_prices,df_freight], axis=1)
temp
[price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
price_index
df_prices.columns
[freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
df_freight.columns
freight_list
freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
price_index
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
price_index
for k in freight_list:

freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
freight_list
k[:4]
freight_list
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
sub_region_2
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes
ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
df_prices = create_landed_values()
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
destination
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
    
    total.index = pd.to_datetime(total.index)
    total.sort_index(inplace=True)
    total.fillna(method='ffill', inplace=True)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



#import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()  
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
#len(test_crudes)
"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()


#var[3]['AAVMR00'][20:]

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Brent','Azeri','CPC Blend']
#destinations = ['Houston']

#destinations = ['Houston','Rotterdam','Augusta','Singapore']


destinations = ['Rotterdam','Augusta','Houston']
#crudes = ['Brent','Azeri','CPC Blend']

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    
    base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
        'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
        #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
crudes
destinations
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
crudes
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()   
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
    
    total.index = pd.to_datetime(total.index)
    total.sort_index(inplace=True)
    total.fillna(method='ffill', inplace=True)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



#import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()
index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
crude = 'Azeri'
destination = 'Rotterdam'
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
df_landed = pd.DataFrame(index=df.index)
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']
days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }

if assay[crude]['Code'] == 'multiple':     
    diff = exceptions[crude][discharge_price_region]['Code']
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight

    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['diff'] = diff
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
destination
sub_to_ws[2]
index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
index_region
func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
    return df_freight

calculate_flat_rate()
calculate_freight()
def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
        return df_freight
    
    calculate_flat_rate()
    calculate_freight()
    return df_freight

def convert_wti():
    df_prices['outright'] = wtim1
    if crude_vs in ['wti cma']:
        df_prices['diff'] = diff
        df_prices['wti_cma_m1'] = wti_cma_m1
        df_prices['wtim1'] = wtim1
        df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
    
    elif crude_vs in ['mars']:
        df_prices['diff'] = diff
        df_prices['mars_wti2'] = mars_wti2
        df_prices['vs_wti'] = diff + mars_wti2
    
    elif crude_vs in index_wti:
        df_prices['diff'] = diff
        df_prices['wtim1_m2'] = wtim1_m2
        df_prices['vs_wti'] = np.where(expiry_condition,
                 diff + wtim1_m2,
                 diff)
    
    elif crude_vs in index_dtd:
        cfd_condition = cfd4 > cfd8
        df_prices['diff'] = diff
        df_prices['cfd4'] = cfd4
        df_prices['cfd8'] = cfd8
        df_prices['efpm2'] = efpm2
        df_prices['wti_br_m1'] = wti_br_m1
        df_prices['vs_wti']  = np.where(cfd_condition,
                 diff + cfd8 +  efpm2 - (wti_br_m1),
                 diff + cfd4 +  efpm2 - (wti_br_m1))
    
    elif crude_vs in index_dub:
        """ This is because all the eastern crudes heading here have diffs against BWAVE"""
        pass
    else:
        df_prices['vs_wti'] = diff
    return df_prices

        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
def convert_dub():
    df_prices['outright'] = dub
    if crude_vs in ['wti cma']:
        """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
        df_prices['diff'] = diff
        df_prices['wti_cma_m1'] = wti_cma_m1
        df_prices['wtim1'] = wtim1
        df_prices['wti_br_m1'] = wti_br_m1
        df_prices['wtim1_m2'] = wtim1_m2
        df_prices['efs2'] = efs2
        df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
    
    elif crude_vs in index_wti:
        df_prices['diff'] = diff
        df_prices['wtim1_m2'] = wtim1_m2
        df_prices['wti_br_m1'] = wti_br_m1
        df_prices['brentm2'] = brentm2
        df_prices['efs2'] = efs2
        df_prices['vs_dub'] = np.where(expiry_condition,
                 diff + wtim1_m2 + wti_br_m1 + efs2,
                 diff + wti_br_m1 + efs2)
    
    elif crude_vs in index_dtd:
        df_prices['diff'] = diff
        df_prices['cfd6'] = cfd6
        df_prices['efpm2'] = efpm2
        df_prices['efs2'] = efs2
        df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
    
    else:
        df_prices['vs_dub'] = diff
    
    return df_prices

index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
func_list
index
index_region
[f() for index, f in func_list.items() if index == index_region][0]
[f() for index, f in func_list.items() if index == index_region][0]
total.describe()
total.dtype()
total.type()
type(total)
total.dtypes
total.info()
def create_landed_values():
    temp = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
    #for i in df_prices.columns:
    for k in freight_list:
        name = str(k[:4])+'_landed_'+str(price_index)
        temp[name] = df_prices[price_index] + df_freight[k]   
    return temp

df_freight = construct_freight()
df_freight
convert_prices()
df_prices
df_prices = convert_prices()
df_prices = create_landed_values()
df_prices
df_prices = convert_prices()
df_prices
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['diff'] = diff
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
df_prices = convert_prices()
total
df_prices = convert_prices()
func_list
index_region
func_list.items()
index_region
[f() for index, f in func_list.items() if index == index_region][0]
dtd
df_prices['outright'] = dtd
df_prices
df_prices = pd.DataFrame(index=df.index)
df_prices = convert_prices()
df_prices
df_landed = create_landed_values()
df_landed
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
df_freight
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
    
    total.index = pd.to_datetime(total.index)
    total.sort_index(inplace=True)
    total.fillna(method='ffill', inplace=True)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



#import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data() 
df_freight = construct_freight()
df_prices = convert_prices()
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    if assay[crude]['Code'] == 'multiple':     
        diff = exceptions[crude][discharge_price_region]['Code']
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['diff'] = diff
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
crude = 'Azeri'
destination = 'Rotterdam'
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    if assay[crude]['Code'] == 'multiple':     
        diff = exceptions[crude][discharge_price_region]['Code']
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['diff'] = diff
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
df_freight = construct_freight()
df_freight
df_prices = convert_prices()
df_prices
temp = pd.concat([df_prices,df_freight], axis=1)
temp
[price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
[freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
freight_list
k='Aframax'
str(k[:4])
'_landed_'
str(price_index)
name = str(k[:4])+'_landed_'+str(price_index)
freight_list
df_freight[k]
df_prices[price_index]
temp[name] = df_prices[price_index] + df_freight[k]
temp
return temp
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time

def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
    
    total.index = pd.to_datetime(total.index)
    total.sort_index(inplace=True)
    total.fillna(method='ffill', inplace=True)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df



#import_data()
assay, ws, ports, total, rate_data, sub_to_ws, df = import_data()    
    crude = 'Azeri'
    destination = 'Rotterdam'
    
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    df_landed = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI M2'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    if assay[crude]['Code'] == 'multiple':     
        diff = exceptions[crude][discharge_price_region]['Code']
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
            return df_freight
        
        calculate_flat_rate()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['diff'] = diff
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['diff'] = diff
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['diff'] = diff
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['diff'] = diff
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['diff'] = diff
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    
    df_freight = construct_freight()
    df_prices = convert_prices()
    
    temp = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
        #for i in df_prices.columns:
    for k in freight_list:
        k='Aframax'
        name = str(k[:4])+'_landed_'+str(price_index)
        temp[name] = df_prices[price_index] + df_freight[k] 
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
crudes = 'Azeri'
destinations = ['Houston','Rotterdam','Augusta']


#destinations = ['Rotterdam']
#crudes = ['Azeri']

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    
    base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
        'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
        #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    
    """generate the landed base blends for each region"""
    base_costs = pd.DataFrame()
    for j in base_blends.keys():    
        for i in base_blends[j].keys():
            base_landed = pd.DataFrame()
            arb_frame = arb(i,j,*var)
            freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
            base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():    
            for i in base_blends[j].keys():
                base_landed = pd.DataFrame()
                arb_frame = arb(i,j,*var)
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
                base_costs[j] = base_landed.sum(1)
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
destinations = ['Rotterdam','Augusta']
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
var[3]
var[3]['AAIPG00']
var[3]['AAIQC00']
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

## ---(Fri May 25 11:29:14 2018)---
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
base_blends[j].keys()
base_blends.keys()
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
j = 'Rotterdam'
i = 'Urals Nth'
base_landed = pd.DataFrame()
arb_frame = arb(i,j,*var)
freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
base_costs[j] = base_landed.sum(1)
base_costs = pd.DataFrame()
j = 'Rotterdam'
i = 'Urals Nth'
base_landed = pd.DataFrame()
arb_frame = arb(i,j,*var)
freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
base_costs[j] = base_landed.sum(1)
base_costs
base_landed
arb_frame
arb_frame.head()
freight_list
arb_frame[freight_list]
arb_frame[freight_list].mean(axis=1)
base_blends
base_blends[j][i]
base_costs
base_landed
base_landed.sum(1)
arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        
        arb_frame = arb(i,j,*var)
        arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
    
    base_costs[j] = base_landed.sum(1)

base_costs
base_costs = pd.DataFrame()
for j in base_blends.keys():    
    for i in base_blends[j].keys():
        base_landed = pd.DataFrame()
        arb_frame = arb(i,j,*var)
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
        base_costs[j] = base_landed.sum(1)

base_costs
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
import seaborn as sns
global_arbs['simple']['Azeri']
suez_margins['simple']['Azeri']
sns.tsplot(suez_margins['simple']['Azeri'])
suez_margins['simple']['Azeri'].plot()
suez_margins['simple']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)]
from datetime import datetime as dt
suez_margins['simple']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)]
suez_margins['simple']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['Azeri'][['diff','Suezmax','Aframax']].iloc[suez_margins.index > dt(2016,1,1)]
global_arbs['simple']['Azeri']['Rotterdam']['diff','Suezmax','Aframax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['Azeri']['Rotterdam']['diff','Suezmax','Aframax']]
global_arbs['simple']['Azeri']['Rotterdam'][['diff','Suezmax','Aframax']]
global_arbs['simple']['Azeri']['Rotterdam'][['diff','Suezmax','Aframax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['simple']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)].plot()

global_arbs['simple']['Azeri']['Rotterdam'][['diff','Suezmax','Aframax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['Azeri']['Houston'][['diff','Suezmax','Aframax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['simple']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)].plot()

global_arbs['simple']['Azeri']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['complex']['Azeri']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['complex']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['semi']['CPC Belnd'].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['simple']['CPC Blend'].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['CPC Blend']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][['BaseBlendUSGC','CPC Blend']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][['BaseBlendUSGC','CPC Blend']]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][['BaseBlendUSGC','CPC Blend']]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][['BaseBlendUSGC','CPC Blend']]
global_arbs['simple'][['BaseBlendUSGC','CPC Blend']][['Houston']][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['CPC Blend']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['BaseBlendUSGC']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][['BaseBlendUSGC','CPC Blend']]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['BaseBlendUSGC']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['BaseBlendUSGC','Maya']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][['BaseBlendUSGC','Maya']]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['simple']['CPC Blend'].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][['Maya']]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['Maya']
global_arbs['simple']['Maya']['Houston']
global_arbs['simple']['Maya']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
base_blends['Houston'].keys()
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
base_blends['Houston'].keys()
for i in base_blends['Houston'].keys():
    global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()

runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
i = 'Basrah Light'
j = 'Houston'
arb_frame = arb(i,j,*var)
arb_frame.head()
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
for i in base_blends['Houston'].keys():
    global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()




for i in base_blends['Houston'].keys():
    global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()

i = 'Basrah Light'
j = 'Houston'
arb_frame = arb(i,j,*var)
arb_frame.head()
global_arbs['simple'][i]
base_blends['Houston'].keys()
i = 'Basrah Light'
j = 'Houston'
arb_frame = arb(i,j,*var)
arb_frame.head()
i = 'Azeri'
j = 'Houston'
arb_frame = arb(i,j,*var)
arb_frame.head()
i = 'Azeri'
j = 'Basrah Light'
arb_frame = arb(i,j,*var)
arb_frame.head()
i = 'Basrah Light'
j = 'Houston'
arb_frame = arb(i,j,*var)
arb_frame.head()
crudes = ['Basrah Light']
destinations = ['Houston']
#destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
suez_margins['simple']['Basrah Light'].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['simple']
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
suez_margins['simple']
suez_margins['simple']['Basrah Light'].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][i]
base_blends['Houston'].keys()
global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Levels are 0: ref type, 1: crude, 2: destination, 3: price series"""
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
afra_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Afra_margin_advantage']
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
afra_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Afra_margin_advantage']
suez_margins['simple']['Basrah Light'].iloc[suez_margins.index > dt(2016,1,1)].plot()
suez_margins['simple']['Azeri'].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs.head()
for i in base_blends['Houston'].keys():
    global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()

runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
for i in base_blends['Rotterdam'].keys():
    global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()

base_blends['Rotterdam'].keys()
global_arbs['simple']['Gullfaks']['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple']['Gullfaks']['Houston']
global_arbs['simple']['Forties']['Houston'][['diff','Suezmax']]
global_arbs['simple']['Forties']['Houston']
global_arbs['simple']['Forties']['Houston'].head()
global_arbs['simple']['Gullfaks']['Houston'].head()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Levels are 0: ref type, 1: crude, 2: destination, 3: price series"""
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
afra_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Afra_margin_advantage']
base_blends['Rotterdam'].keys()
global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()
global_arbs['simple'][i]['Houston']
for i in base_blends['Rotterdam'].keys():
    global_arbs['simple'][i]['Houston'][['diff','Suezmax']].iloc[suez_margins.index > dt(2016,1,1)].plot()

suez_margins
for i in base_blends['Rotterdam'].keys():
    suez_margins['simple'][i].iloc[suez_margins.index > dt(2016,1,1)].plot()

for i in base_blends['Rotterdam'].keys():
    suez_margins['simple'][i].iloc[suez_margins.index > dt(2016,1,1)].plot(title=i)

for i in base_blends['Houston'].keys():
    suez_margins['simple'][i].iloc[suez_margins.index > dt(2016,1,1)].plot(title=i)

for i in base_blends['Houston'].keys():
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2016,1,1)].plot(title=i)
    except Exception as e: print('{} passed').format(i)

for i in base_blends['Houston'].keys():
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2016,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

for i in base_blends['Houston'].keys():
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2017,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

var[3]['AAIRB00']
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""
test_crudes = test_crudes.loc[test_crudes['Code'].isin(crudes_with_codes)]
codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]
crude_check
ports_with_rates
crudes_with_codes
ports_in_targo
test_crudes
no_flat_rates
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
crude_check
ports_with_rates
crudes_with_codes
crude_check
crude_check['LoadPort']
rude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
test_crudes
ports_with_rates
crude_check['LoadPort']
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)
crude_check
crude_check['LoadPort'].isin(ports_with_rates)
crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
test_crudes
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
test_crudes
test_crudes2 = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')]
test_crudes2
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')]
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes
test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
test_crudes
crudes = list(test_crudes.index.values)
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Levels are 0: ref type, 1: crude, 2: destination, 3: price series"""
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
afra_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Afra_margin_advantage']
for i in base_blends['Houston'].keys():
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2017,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

for i in ['Saharan','CPC Blend','Azeri']:
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2017,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

for i in [crudes]:
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2017,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

for i in crudes:
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2017,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

suez_margins
suez_weekly = suez_margins.resample('W').mean()
for i in crudes:
    try:
        suez_weekly['simple'][i].iloc[suez_margins.index > dt(2017,1,1)].plot(title=i)

for i in crudes:
    try:
        suez_weekly['simple'][i].iloc[suez_weekly.index > dt(2017,1,1)].plot(title=i)

for i in crudes:
    try:
        suez_weekly['simple'][i].iloc[suez_weekly.index > dt(2017,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

for i in crudes:
    try:
        suez_margins['simple'][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

for i in crudes:
    try:
        suez_margins['complex'][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

suez_weekly = suez_margins.resample('W').mean()
for i in crudes:
    try:
        suez_weekly['complex'][i].iloc[suez_weekly.index > dt(2018,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

global_arbs
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
runfile('C:/Users/mima/.spyder-py3/InitiationProgram.py', wdir='C:/Users/mima/.spyder-py3')
for i in crudes:
    try:
        suez_margins['complex'][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i)
    except Exception as e: print('{} passed'.format(i))

refinery_configurations.keys()
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

global_arbs.head()
import pyodbc
import pandas as pd


cxn = pyodbc.connect('Driver=SQL Server Native Client 11.0;'
                                'Server=STCHGS126;'
                                'Database=STG_Targo;'
                                'uid=mima;'
                                'Trusted_Connection=Yes;')

query = "Select * From view_CargoAnalysis_External_Crude_Kb"

flows_data = pd.read_sql(query, cxn)
flows = flows_data.pivot_table(columns='LoadDate', index=['DischargeRegion','DischargeCountry','CurrentOwner','CleanGrade'], values = 'CleanQuantity')
flows_data
flows_data.dtypes
global_arbs.head()
global_arbs.index.get_level_values
global_arbs.index.names
global_arbs.columns.names
global_arbs.columns.names = ['RefineryConfig','Grade','Region','Prices']
global_arbs
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Prices'], ['Date']
global_arbs
global_arbs.iloc[:, global_arbs.columns.get_level_values('Prices')=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
global_arbs.iloc[:, global_arbs.columns.get_level_values('Prices')=='Suez_margin_advantage']
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                     (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                     (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].stack()
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Grade')== ['Urals Nth','Basrah Light'])&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Grade') in ['Urals Nth','Basrah Light'])&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light'])&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light','CPC Blend']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light','CPC Blend']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot()
global_arbs.iloc[:, (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light','CPC Blend']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,15))
global_arbs.iloc[:, (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light','CPC Blend']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,15))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light','CPC Blend']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,15))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Urals Nth','Basrah Light','CPC Blend']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (10,10))
crudes
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin([crudes]))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (10,10))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(crudes))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (10,10))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(crudes))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (20,200))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(crudes))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (100,50))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(crudes))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (20,20))
var
crudes
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin('Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars'))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,20))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,20))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')].plot(figsize = (15,20))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')]
global_arbs.iloc[:-5,(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[-5:,(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
random = global_arbs.iloc[-5:,(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
random.rolling(2, min_periods=1).sum()
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')].rolling(5, min_periods=1).mean().plot(figsize = (15,20))
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')].rolling(5, min_periods=1).mean().plot(figsize = (15,20))
analysis[-1]
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')].rolling(5, min_periods=1).mean()


analysis_picture = analysis.plot(figsize = (15,20))
analysis_picture[0]
analysis[0]
analysis[0,:]
analysis
analysis[1,:]
analysis.iloc[1,:]
analysis.iloc[5,:]
analysis.iloc[-1,:]
analysis.iloc[-3,:] - analysis.iloc[-5,:]
analysis.pct_change()
analysis.pct_change().plot()
analysis.pct_change().head()
analysis.pct_change().iloc[-10:,:]
analysis.pct_change().iloc[-10:,:].plot()
analysis.pct_change().iloc[-10:,:]
analysis.pct_change(period=5).iloc[-10:,:].plot()
analysis.pct_change(periods=5).iloc[-10:,:]
analysis.pct_change(periods=10).iloc[-10:,:]
analysis.pct_change(periods=10).iloc[-5]
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,20))
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')].rolling(5, min_periods=1).mean()
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')].rolling(5, min_periods=1).mean().plot()
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean().plot()
analysis.pct_change(periods=10).iloc[-5]
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean()
analysis.pct_change(periods=10).iloc[-10:,:].plot()
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,20))
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean()
analysis.plot(figsize = (15,20))
analysis.pct_change(periods=10).iloc[-30:,:].plot()
analysis.pct_change(periods=10).iloc[-100:,:].plot()
analysis.iloc[-100:,:].plot()
(analysis.iloc[-3,:] - analysis.iloc[-5,:]) / analysis.iloc[-5,:]
analysis.pct_change(periods=10).iloc[-5]
analysis
analysis.pct_change(periods=10).loc[(analysis["Suez_margin_advantage"].shift() < 0) & (analysis["Suez_margin_advantage"] > 0)] *= -1
analysis
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
    base_costs[j] = base_landed.sum(1) 

base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}

"""generate the landed base blends for each region"""
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
    base_costs[j] = base_landed.sum(1)       

base_costs
freight_list
    arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
base_costs[j] = base_landed.sum(1)          
arb_frame[freight_list]
arb_frame[freight_list].mean(axis=1)
sub_to_ws
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""
Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison
"""
prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})


"""
Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column
"""
total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
total = total.iloc[total.index > dt(2015,12,31)]


"""
Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned
"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers


"""
This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018
"""    

rates = []

for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:46,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""
Also initialise the temp df with index of total.
Temp df is tol hold the dataseries needed to calculate the freight
"""
df = pd.DataFrame(index=total.index)

"""
This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA

Have tried timing and slight improvment with the blow of 0.2seconds....
"""

t = time.process_time()

def apply_expiry(x):
    return (expiry_table.loc[(expiry_table['Month'].dt.month == x[1])&
                                  (expiry_table['Month'].dt.year == x[0]), 'Expiry']).iat[0]


index_for_func = np.array((list(df.index.year),list(df.index.month))).transpose()
df['Expiry'] = np.apply_along_axis(apply_expiry,1,index_for_func)
print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
sub_to_ws
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
ws_table = pd.read_excel(data, 'ws_table', header = 1)
#rate_data = pd.read_excel(data, 'flat_rate')
prices = pd.read_excel(data, 'prices', header = 1)
paper_prices = pd.read_excel(data, 'paper prices', header = 1)
expiry_table = pd.read_excel(data, 'expiry')
ports = pd.read_excel(data, 'ports')
#products = pd.read_excel(data, 'rott products')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
sub_to_ws
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws
sub_to_ws.set_index([0])
sub_to_ws.set_index([0]).to_dict()
sub_to_ws['1']
sub_region.map(sub_to_ws[1])
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
crude= 'Azeri'
destination = 'Augusta'
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'
crude= 'Azeri'
destination = 'Augusta'
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region
sub_region.map(sub_to_ws[1])
crude= 'Azeri'
destination = 'Augusta'
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_to_ws[1]
sub_to_ws
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws
sub_to_ws.set_index([0])
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
sub_to_ws
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None
sub_to_ws
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws
sub_to_ws.set_index([0])
sub_to_ws.set_index([0]).to_dict()
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
type(sub_to_ws)
sub_to_ws
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()
crude= 'Azeri'
destination = 'Augusta'
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region
sub_to_ws
sub_region.map(sub_to_ws[1])
sub_to_ws
sub_to_ws[1]
sub_region
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
ports[ports['Name'] == assay[crude]['LoadPort']]
base_costs
base_landed
base_costs
base_landed
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}

"""generate the landed base blends for each region"""
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]
    base_costs[j] = base_landed.sum(1)  

base_costs
base_landed
arb_frame
j
i
arb_frame
arb_frame[freight_list].mean(axis=1)
base_landed[i]
base_landed
base_costs[j] = base_landed.sum(0)           
base_costs[j]
base_landed
base_landed.sum(1)    
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]

arb_frame[freight_list]
base_landed = pd.DataFrame()
for i in base_blends[j].keys():
    #i = 'Basrah Light'
    #j = 'Houston'
    arb_frame = arb(i,j,*var)
    #arb_frame.head()
    freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
    base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i]

base_landed
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}

"""generate the landed base blends for each region"""
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.03
    base_costs[j] = base_landed.sum(1)     

base_costs
base_blends[j][i]
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']                    
    return df_freight

calculate_flat_rate()
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
calculate_flat_rate()
crude = 'Azeri'
destination = 'Rotterdam'
calculate_flat_rate()
crude = 'Azeri'
destination = 'Rotterdam'

#crude = 'Azeri'
#destination = 'Rotterdam'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

calculate_flat_rate()
calculate_freight()
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', header = None)
basrah_ws_base = basrah_ws_base.set_index([0]).to_dict()
total['PFAOH00']
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', header = None)
basrah_ws_base = basrah_ws_base.set_index([0]).to_dict()

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), list(crude_diffs.columns)] = crude_diffs[list(crude_diffs.columns)]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
total['PFAOH00'] 
df_freight
total['PFAOH00'].reasmple('M').mean()  
total['PFAOH00'].resample('M').mean()  
df_freight.index
df_freight.index.month
total['PFAOH00'].dt.month
total['PFAOH00']
np.where(total['PFAOH00'].index.month == df_freight.index.month)
total['PFAOH00'].resample('M').mean().resample('D') 
total['PFAOH00'].resample('M').mean().resample('D').mean()  
total['PFAOH00']
total['PFAOH00'].resample('M').mean().resample('D').interpolate().fillna(method='bfill')
total['PFAOH00'].resample('M').mean().resample('D').fillna(method='bfill')
total['PFAOH00'].resample('M').mean().resample('D').fillna(method='bfill').loc[total.index,:] 
total.index
total['PFAOH00'].resample('M').mean().resample('D').fillna(method='bfill').iloc[total.index,:] 
total['PFAOH00'].resample('M').mean().resample('D').fillna(method='bfill')
total['PFAOH00']
monthly_averages = total['PFAOH00'].resample('M').mean()
monthly_averages
df_freight
df_freight.index
df_freight['Date'] = df_freight.index
monthly_averages = total['PFAOH00'].resample('M').mean()
averages_month = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
df_freight['Date'] = df_freight.index
df_freight['WS for Esc'] = df_freight['Date'].apply(averages_month)
df_freight
df_freight.drop(['Date'])
df_freight
monthly_averages
df_freight['Date']
df_freight.drop(['Date'], axis = 1)
total['PFAOH00'].resample('M')
total['PFAOH00'].resample('M').mean()
total['PFAOH00']
total['PFAOH00'].resample('M').mean()
total['PFAOH00'].iloc[dt(2018,4,1) < total.index < dt(2018,4,31),:]
total['PFAOH00'].iloc[(dt(2018,4,1) < total.index < dt(2018,4,31)),:]
(dt(2018,4,1) < total.index
total.index
dt(2018,4,1) < total.index
(dt(2018,4,1) < total.index)&(total.index < dt(2018,4,31))
total.index < dt(2018,4,31)
(dt(2018,4,1) < total.index < dt(2018,4,30))
total.index < dt(2018,4,30)
(dt(2018,4,1) < total.index
dt(2018,4,1) < total.index
(dt(2018,4,1) <= total.index <= dt(2018,4,30))
total['PFAOH00'].iloc[(dt(2018,4,1) <= total.index,:]
total['PFAOH00'].iloc[(dt(2018,4,1) <= total.index]
total['PFAOH00'].iloc[dt(2018,4,1) <= total.index]
total['PFAOH00'].iloc[dt(2018,4,1) <= total.index].resample('M').mean()
total['PFAOH00'].iloc[dt(2018,3,1) <= total.index].resample('M').mean()
total['PFAOH00'].iloc[dt(2018,3,1) <= total.index]
monthly_averages = total['PFAOH00'].resample('M').mean()
monthly_averages
monthly_averages = total['PFAOH00'].resample('M').mean()
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
df_freight['Date'] = df_freight.index
df_freight['WS for Esc'] = df_freight['Date'].apply(func_ma_on_days)
df_freight.drop(['Date'], axis = 1)
monthly_averages = total['PFAOH00'].resample('M').mean()
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
df_freight['Date'] = df_freight.index
df_freight['WS for Esc'] = df_freight['Date'].apply(func_ma_on_days)
from pandas.tseries.offsets import BMonthEnd
BMonthEnd()
BMonthEnd(1)
df_freight['Date'].apply(func_ma_on_days)
func_ma_on_days = 
total['PFAOH00'].resample('BMS').mean(
total['PFAOH00'].resample('BMS').mean()
monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
df_freight['Date'] = df_freight.index
df_freight['WS for Esc'] = df_freight['Date'].apply(func_ma_on_days)
df_freight
basrah_ws_base
basrah_ws_base[0]
basrah_ws_base
basrah_ws_base[1]
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', header = None)
basrah_ws_base
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base')
basrah_ws_base
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
basrah_ws_base
basrah_ws_base = basrah_ws_base.set_index([0]).to_dict()
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
basrah_ws_base
basrah_ws_base.index.year
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
basrah_ws_base
basrah_ws_base.index.year
df_freight['Date'].apply(func_ws_base)
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]).iat[0]
df_freight['Date'].apply(func_ws_base)
basrah_ws_base
monthly_averages
basrah_ws_base
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]).iat[0]
df_freight['Date'].apply(func_ws_base)
monthly_averages
basrah_ws_base
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base')
basrah_ws_base
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]).iat[0]
monthly_averages
basrah_ws_base
df_freight['Date']
df_freight['Date'].apply(func_ws_base)
basrah_ws_base
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
df_freight['Date'].apply(func_ws_base)
df_freight['WS for Esc'] = df_freight['Date'].apply(func_ma_on_days) - df_freight['Date'].apply(func_ws_base)
df_freight
df_freight['WS for Esc'] = (df_freight['Date'].apply(func_ma_on_days) - df_freight['Date'].apply(func_ws_base)) * 23.59 / 7.3 / 100
df_freight['WS for Esc']
(df_freight['Date'].apply(func_ma_on_days) - df_freight['Date'].apply(func_ws_base))
df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'].iloc[df_freight.index => dt(2018,4,1)]
df_freight['SOMO Base WS'].iloc[df_freight.index >= dt(2018,4,1)]
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
df_freight['SOMO Base WS'].iloc[df_freight.index >= dt(2018,4,1)]
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))]
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
df_freight['SOMO Base WS']
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))]
df_freight['WS for Esc']
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]

"""Create func to handle basrah base and flat rate values"""
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]

df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)

# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25

df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]

"""Create func to handle basrah base and flat rate values"""
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]

df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)

# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25

df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
df_freight['WS for Esc']
df_freight.drop(['Date'], axis = 1, inplace=True)
df_freight
ws_codes['Code']
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion']
sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes['Code']
size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
for i in list(ws_codes['Code']):
    size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
    name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]

size
name
vessel_size = []
for i in list(ws_codes['Code']):
    size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
    vessel_size.append(size)
    name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
    if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
        df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
        df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
        df_freight.drop(['Rate'], axis=1)
    else:
        df_freight[name] = total[i]
        df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']

vessel_size
'WS for Esc' in ('WS for Esc')
df_freight.columns
df_freight.columns.values
'WS for Esc' in df_freight.columns.values
crude = 'Azeri'
destination = 'Rotterdam'

#crude = 'Azeri'
#destination = 'Rotterdam'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

crude = 'Basrah Light'
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

def freight_and_quality_exceptions():
    if crude in ('Forties'):
        pass
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create func to handle basrah base and flat rate values"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
        
        df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
    
    if destination in ('South Korea'):
        pass
    
    return df_freight

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    vessel_size = []
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        vessel_size.append(size)
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    
    if 'WS for Esc' in df_freight.columns.values:
        for i in vessel_size:
            df_freight[i] = df_freight[i] + df_freight['WS for Esc']

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    vessel_size = []
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        vessel_size.append(size)
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    
    if 'WS for Esc' in df_freight.columns.values:
        for i in vessel_size:
            df_freight[i] = df_freight[i] + df_freight['WS for Esc']
    
    return df_freight

calculate_flat_rate()
freight_and_quality_exceptions()
calculate_freight()
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight


def freight_and_quality_exceptions():
    if crude in ('Forties'):
        pass
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
        func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
        df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
        
        if crude == 'Basrah Light':
            df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
        else:
            df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
        
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
    
    if destination in ('South Korea'):
        pass
    
    return df_freight


def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
    sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion']
    sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    vessel_size = []
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        vessel_size.append(size)
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
    
    if 'WS for Esc' in df_freight.columns.values:
        for i in vessel_size:
            df_freight[i] = df_freight[i] + df_freight['WS for Esc'] + df_freight['API Esc']
    
    return df_freight

calculate_flat_rate()
freight_and_quality_exceptions()
calculate_flat_rate()
freight_and_quality_exceptions()
calculate_freight()
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 1

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

crude_diffs.index
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index
pd.notnull(crude_diffs.index)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')
[name for name in crude_diffs.columns if 'Unnamed' in name]
crude_diffs
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('D').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs
crude_diffs.columns
crude_diffs = crude_diffs.resample('BD').interpolate().fillna(method='bfill')
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
list(crude_diffs.columns)
total.loc[total.index.isin(crude_diffs.index), codes_list]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy')]
codes_list
total.loc[total.index.isin(crude_diffs.index), codes_list] 
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
crude_diffs
crude_diffs['Basrah Light']
crude
crude in ('Basrah Light','Basrah Heavy'
crude = 'Basrah Light'
if crude in ('Basrah Light','Basrah Heavy'):

crude in ('Basrah Light','Basrah Heavy')
crude_diffs
crude_diffs[crude]
diff
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
crude = 'Basrah Light'
destination = 'Rotterdam'

#crude = 'Azeri'
#destination = 'Rotterdam'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2
df_prices
df_prices['diff'] = diff + crude_diffs[crude]
crude = 'Basrah Light'
destination = 'Rotterdam'

#crude = 'Azeri'
#destination = 'Rotterdam'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

df_prices['diff'] = diff + crude_diffs[crude]
df_prices['diff']
diff
diff + crude_diffs[crude]
crude_diffs[crude].fillna(method = 'ffill')
diff
crude_diffs[crude]
crude_diffs
crude_diffs[crude]
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
crude_diffs[crude]
df_prices
df_prices['diff'] = diff + crude_diffs[crude].fillna(method = 'ffill')
df_prices['diff'] 
diff
crude_diffs[crude].fillna(method = 'ffill'
crude_diffs[crude].fillna(method = 'ffill')
df_prices['diff']
crude = 'Basrah Light'
destination = 'Rotterdam'

#crude = 'Azeri'
#destination = 'Rotterdam'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

df_prices
df_prices['diff'] = diff + crude_diffs[crude].fillna(method = 'ffill')
df_prices
diff
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs.reindex(total.index)
crude_diffs = crude_diffs.reindex(total.index).resample('B').interpolate().fillna(method='bfill')
crude_diffs
df_prices['diff'] = diff + crude_diffs[crude]
df_prices['diff']
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 1

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 1

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
ports_with_rates
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time


def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """table containing the basrah base worldscale that they fix their freight against"""
    basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
    
    total.index = pd.to_datetime(total.index)
    total.sort_index(inplace=True)
    total.fillna(method='ffill', inplace=True)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.reindex(total.index).resample('B').interpolate().fillna(method='bfill')
    
    """Give me the columns with codes against them"""
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    
    """Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
    crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
    codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
    
    """Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
    total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
    
    """Also need to adjust the cfds to take into account the inter month BFOE spread"""   
    cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
    temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
    temp = temp[temp.index > dt(2017,6,30)]
    total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]
    
    """This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
    rates = []
    for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
        f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
        lplen = len(f.iloc[:,1])
        dplen = len(f.iloc[1,:])
        for j in range(1, dplen):
            for i in range(1,lplen):
                LoadPort = f.iloc[i,0]
                DischargePort = f.iloc[0,j]
                Year = y
                Rate = f.iloc[i,j]
                rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})
    
    rate_data = pd.DataFrame(rates)
    
    """Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
    df = pd.DataFrame(index=total.index)
    df['Date'] = df.index
    
    """This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
    Have tried timing and slight improvment with the blow of 0.2seconds...."""
    
    t = time.process_time()
    
    for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]
    
    df['Expiry'] = df['Date'].apply(for_dates)
    df.drop(['Date'], inplace=True, axis=1)
    
    print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
    print("Temp DataFrame created successfully")
    print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
    
    return assay, ws, ports, total, rate_data, sub_to_ws, df, basrah_ws_base, crude_diffs

assay, ws, ports, total, rate_data, sub_to_ws, df, basrah_ws_base, crude_diffs = import_data()    
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.reindex(total.index).resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
codes_list
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 1

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.reindex(total.index).resample('B').interpolate().fillna(method='bfill')

"""Give me the columns with codes against them"""
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 1

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
    crude = 'Basrah Light'
    destination = 'Rotterdam'
    
    #crude = 'Azeri'
    #destination = 'Rotterdam'
    
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    
    
    if assay[crude]['Code'] == 'multiple':     
        diff = total[exceptions[crude][discharge_price_region]['Code']]
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            return df_freight
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                pass
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
                df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
            
            if destination in ('South Korea'):
                pass
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
            sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
            sub_region_2 = ports[ports['Name'] == destination]['Subregion']
            sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] - df_freight['API Esc']
            
            return df_freight
        
        calculate_flat_rate()
        freight_and_quality_exceptions()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        if crude in ('Basrah Light','Basrah Heavy'):
            df_prices['diff'] = diff + crude_diffs[crude]
        else:
            df_prices['diff'] = diff
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    try:
        df_freight = construct_freight()
    except Exception as e: print(e), print('df_freight')    
    
    try:
        df_prices = convert_prices()
    except Exception as e: print(e), print('df_prices') 
    
    temp = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
    
    try:
        for k in freight_list:
            try:
                name = str(k[:4]) + str('_landed_') + str(price_index)
            except Exception as e: print('name fails') 
            try:
                """test to see if nan's present
                print('this is the price index')
                print(price_index)
                print(df_prices)
                print(df_prices[price_index].isnull().sum().sum())
                print('this is the freight')
                print(k)
                print(df_freight[k].isnull().sum().sum())"""
                temp[name] = df_prices[price_index].add(df_freight[k])
            except Exception as e: print('temp fails')    
    except Exception as e: print('check')
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 1

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

global_arbs
refinery_configurations.keys()
m = 'complex'
i = 'Basrah Light'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
base = k[:4]+'_base_landed'
arb_values[base] = base_costs[k]
gpw = gpw_calculation(i,k, *var, **refinery_config)
arb_values = pd.concat([arb_values,gpw], axis=1)
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
    base_costs[j] = base_landed.sum(1)  

base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}

"""generate the landed base blends for each region"""
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
    base_costs[j] = base_landed.sum(1)           

base_costs
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}

"""generate the landed base blends for each region"""
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
    base_costs[j] = base_landed.sum(1)      

base_costs
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
    'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
    #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
    'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}

"""generate the landed base blends for each region"""
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
    base_costs[j] = base_landed.sum(1)

base_costs
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.reindex(total.index).resample('B').interpolate().fillna(method='ffill')
crude_diffs
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs.index.isin(total.index)
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs
codes_list
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
total
crude_diffs
df_freight
calculate_flat_rate()
freight_and_quality_exceptions()
calculate_freight()
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()



def construct_freight():
    def calculate_flat_rate():
        """create the flat rates table for the rates calculations and column creation"""    
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                  (rate_data['DischargePort'] == destination)]
        
        def calculate_flat_rates(x):
            return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
        
        """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
        return df_freight
    
    def freight_and_quality_exceptions():
        if crude in ('Forties'):
            pass
        
        if crude in ('Basrah Light','Basrah Heavy'):
            """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
            monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
            func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
            
            """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
            func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
            func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
            func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
            func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
            df_freight['Date'] = df_freight.index
            df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
            df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
            # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
            # only valid for 2018
            df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
            df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
            
            if crude == 'Basrah Light':
                df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
            else:
                df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
            
            df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
            df_freight.drop(['Date'], axis = 1, inplace=True)
        
        if destination in ('South Korea'):
            pass
        
        return df_freight
    
    def calculate_freight():
        
        """This finds the correct worldscale rate and adjusts if it is lumpsum"""
        sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion']
        sub_region = sub_region.map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
        sub_region_2 = ports[ports['Name'] == destination]['Subregion']
        sub_region_2 = sub_region_2.map(sub_to_ws[1]).to_string(index = False)
        ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
        
        vessel_size = []
        for i in list(ws_codes['Code']):
            size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
            vessel_size.append(size)
            name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
            if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
                df_freight.drop(['Rate'], axis=1)
            else:
                df_freight[name] = total[i]
                df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion']
        
        if 'WS for Esc' in df_freight.columns.values:
            for i in vessel_size:
                df_freight[i] = df_freight[i] - df_freight['WS for Esc'] - df_freight['API Esc']
        
        return df_freight
    
    calculate_flat_rate()
    freight_and_quality_exceptions()
    calculate_freight()

total['PFAOH00']
calculate_flat_rate()
m = 'complex'
i = 'Basrah Light'
k = 'Rotterdam'
arb_values = arb(i,k, *var)
arb_values
base = k[:4]+'_base_landed'
base
base_costs[k]
base_costs = pd.DataFrame()
for j in base_blends.keys():
    base_landed = pd.DataFrame()
    for i in base_blends[j].keys():
        #i = 'Basrah Light'
        #j = 'Houston'
        arb_frame = arb(i,j,*var)
        #arb_frame.head()
        freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
        base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
    base_costs[j] = base_landed.sum(1)   

base_costs
i = 'Basrah Light'
j = 'Houston'
arb_frame = arb(i,j,*var)
arb_frame
[column_header for column_header in arb_frame.columns if 'landed' in column_header]
freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
base_landed
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
crude_diffs = crude_diffs.reindex(total.index).resample('B').interpolate().fillna(method='ffill')
crude_diffs
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
total
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
total
total[codes_list].tail()
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
pd.notnull(crude_diffs.index)
crude_diffs
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
crude_diffs.reindex(total.index)
[name for name in crude_diffs.columns if 'Unnamed' in name]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
interpolate().
crude_diffs.reindex(total.index).interpolate()
crude_diffs.reindex(total.index).fillna(method='ffill')
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')
crude_diffs[crude_diffs.index.isin(total.index)]
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
total
total[codes_list]
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                i = 'Basrah Light'
                j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
global_arbs.iloc[:, global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage']
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (15,20))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean()
analysis_picture = analysis.plot(figsize = (7.5,7.5))
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
global_arbs.iloc[:, global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage']

global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='complex')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['simple','complex'])&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['complex']))&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['complex','simple']))&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
global_arbs
global_arbs,iloc[:, global_arbs.columns.get_level_values('Grade')=='BaseBlendUSGC']
global_arbs.iloc[:, global_arbs.columns.get_level_values('Grade')=='BaseBlendUSGC']
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['complex','simple']))&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].plot(figsize = (7.5,7.5))
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean()


analysis_picture = analysis.plot(figsize = (7.5,7.5))
global_arbs.iloc[:, global_arbs.columns.get_level_values('Grade')=='BaseBlendUSGC']
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['complex','simple']))&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean().plot(figsize = (7.5,7.5))
analysis = global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['LLS']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean()


analysis_picture = analysis.plot(figsize = (7.5,7.5))
analysis.pct_change(periods=10).iloc[-100:,:].plot()
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

crudes
refinery_configurations.keys()
crudes
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

total.index.isin(crude_diffs.index)
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
total[codes_list]
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]
total.index.isin(crude_diffs.index)
total.index
crude_diffs.index
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')
crude_diffs
crude_diffs.index.isin(total.index)
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
total.update(crude_diffs[codes_list])
total = total.update(crude_diffs[codes_list])
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total = total.update(crude_diffs[codes_list])
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
crude_diffs[codes_list]
total.update(crude_diffs[codes_list])
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
#global_arbs.iloc[:, global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage']

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
total = total.update(crude_diffs[codes_list])
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
mike = total.update(crude_diffs[codes_list])
total.update(crude_diffs[codes_list], inplace=True)
total.update(crude_diffs[codes_list])
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
total.update(crude_diffs[codes_list])
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
#global_arbs.iloc[:, global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage']

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

global_arbs.columns.get_level_values(3)
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,11,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

global_arbs.columns
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Hous_margin']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,11,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin(['Hous_margin','Augu_margin','Rott_margin'])
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin(['Hous_margin','Augu_margin','Rott_margin'])]
base_margins
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin(['Hous_margin','Augu_margin','Rott_margin'])].groupby(level=['RefineryConfig','Series'], axis=1).mean()
base_margins
base_margins.plot(figsize = (7.5,7.5))
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['complex','simple']))&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean().plot(figsize = (7.5,7.5)) 
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['complex','simple']))&
                 (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')]
global_arbs.iloc[global_arbs.index > dt(2018,1,1) , (global_arbs.columns.get_level_values('RefineryConfig').isin(['complex','simple']))&
                                  (global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage')].rolling(5, min_periods=1).mean().plot(figsize = (7.5,7.5))
global_arbs.get_level_values(3)
global_arbs.columns.get_level_values(3)
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin([
        'Hous_margin','Augu_margin','Rott_margin'])].groupby(level=['RefineryConfig','Series'], axis=1).mean()
    
    
    base_margins.plot(figsize = (7.5,7.5))
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin([
        'Hous_margin','Augu_margin','Rott_margin'])].groupby(level=['RefineryConfig','Series'], axis=1).mean()
base_margins.plot(figsize = (7.5,7.5))
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin([
        'Hous_margin','Augu_margin','Rott_margin'])].groupby(level=['RefineryConfig','Series'], axis=1).rolling(5, min_periods=1).mean()
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin([
        'Hous_margin','Augu_margin','Rott_margin'])].groupby(level=['RefineryConfig','Series'], axis=1).rolling(5, min_periods=1).mean()


base_margins.plot(figsize = (7.5,7.5))
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin([
        'Hous_margin','Augu_margin','Rott_margin'])].rolling(5, min_periods=1).groupby(level=['RefineryConfig','Series'], axis=1).mean()
base_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin([
        'Hous_margin','Augu_margin','Rott_margin'])].rolling(5, min_periods=1).mean().groupby(level=['RefineryConfig','Series'], axis=1).mean()
base_margins.plot(figsize = (7.5,7.5))
df_freight['Port_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
import numpy as np
df_freight['Port_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
df_freight = pd.DataFrame(index=df.index)
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
import numpy as np
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
crude = 'Basrah Light'
destination = 'Rotterdam'
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    return df_freight

calculate_flat_rate()
np.where(df_freight.index > dt(2018,2,28),0.09,0)
sub_region
ports[ports['Name'] == assay[crude]
ports[ports['Name'] == assay[crude]['LoadPort']]
temp_costs = pd.DataFrame(index = df_freight.index)
temp_costs
ports[ports['Name'] == destination]
df_freight['Costs'] += df_freight['Houston_Load_Costs']
df_freight['Costs'] = pd.DataFrame(0,index = df_freight.index)
df_freight['Costs'] = 0
df_freight['Costs']
df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
df_freight
df_freight['Costs'] += df_freight['Houston_Load_Costs']
df_freight
ws_codes
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes
rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
          (rate_data['DischargePort'] == destination)]
rate_data
flat_rate_table
flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
          (rate_data['DischargePort'] == destination)]
flat_rate_table
rate_data
rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
      (rate_data['DischargePort'] == 'Singapore')
rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
      (rate_data['DischargePort'] == 'Singapore')]
flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
      (rate_data['DischargePort'] == 'Singapore')]
v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['SK_Fr_Rebate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
def calculate_flat_rates(x):
    return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])

flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
      (rate_data['DischargePort'] == 'Singapore')]
v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight['SK_Fr_Rebate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
df_freight
ws_codes
(ws_codes[ws_codes['Code'] == i]['Size'].values * 1000)
ws_codes[ws_codes['Code'] == i]['Size'].value
ws_codes[ws_codes['Code'] == i]['Size'].values
ws_codes
(ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000)
sub_region
ports
ports[ports['Name'] == assay[crude]['LoadPort']]
total['AASLA00']
total['AASLA00'] * 1000000 / 2000000
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = 0)
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col 7, header = 22)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = 7, header = 22)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = 7)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc')
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = 'I')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = "I")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = 7)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = [7])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = [7:9])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = ['I'])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = [7:9])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = [7,9])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols = [7,9], header = [23])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = [7], usecols = [6,7], header = [23])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = [7], usecols = [7,8], header = [23])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = [7], usecols = [8], header = [23])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = [7], header = [23])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = [7], header = [22])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', index_col = [7], header = [22])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])['buzzard content']
forties_sulphur
pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])['buzzard content'].index
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])['buzzard content'].droplevel(level=0)
pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])['buzzard content'].index
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])['buzzard content']
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])['buzzard content'][0]
forties_sulphur
pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur.head()
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.head()
forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur.loc[:,['week ending','buzzard content']].set_index(forties_sulphur['week ending'])
forties_sulphur.loc[:,['week ending','buzzard content']].set_index(forties_sulphur['week ending']).drop(forties_sulphur['week ending'], axis=1)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.loc[:,['week ending','buzzard content']].set_index(forties_sulphur['week ending'])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
mike = forties_sulphur.loc[:,['week ending','buzzard content']].set_index(forties_sulphur['week ending'])
mike = mike.drop(forties_sulphur['week ending'], axis=1)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
mike = forties_sulphur.loc[:,['week ending','buzzard content']].set_index(forties_sulphur['week ending'])
mike
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
mike = forties_sulphur.loc[:,['week ending','buzzard content']].set_index(forties_sulphur['week ending'])
mike
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
mike = forties_sulphur.loc[:,['week ending','buzzard content']]
mike
mike.index
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [[7,8]])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [7,8])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [7,8])
mike = forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
mike = forties_sulphur.loc[:,['week ending','buzzard content']]
mike
mike.index
mike.index = mike['week_ending']
mike.index = mike['week ending']
mike
mike.index = mike['week ending'].drop(mike.loc[:,0], axis = 1)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6]
forties_sulphur.index.droplevel()
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur.index = forties_sulphur.index.droplevel()
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur.index = forties_sulphur.index.droplevel(2)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur.index = forties_sulphur.index.droplevel(1)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur.index = forties_sulphur.index.droplevel(0)
forties_sulphur
forties_sulphur.index.droplevel(0)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur[0]
forties_sulphur[:,0]
forties_sulphur.loc[:,0]
forties_sulphur.iloc[:,0]
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur
forties_sulphur = forties_sulphur.reindex(forties_sulphur['week ending'])
forties_sulphur
forties_sulphur['week ending']
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur = forties_sulphur.index.droplevel(0)
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur.index.droplevel()
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols = [5,6])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = [5], usecols = [5,6])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 5, usecols = [5,6])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 5, usecols = [5,6])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 5)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = [5])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index()
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur
forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur
forties_sulphur.set_index('week ending')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur = forties_sulphur.set_index(total.index)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur
forties_sulphur.reindex(total.index)
forties_sulphur = forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,['week ending','buzzard content']]
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,['week ending','buzzard content']].set_index('week ending')
forties_sulphur
forties_sulphur.index 
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')
forties_sulphur.reindex(total.index)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,'buzzard content'].set_index('week ending')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur = forties_sulphur.loc[:,'buzzard content']
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True)
forties_sulphur
forties_sulphur.reset_index(inplace=True).set_index('week ending')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur.reset_index(inplace=True).set_index('week ending')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur
forties_sulphur.set_index('week ending', inplace=True)
forties_sulphur
forties_sulphur = forties_sulphur.loc[:,'buzzard content']
forties_sulphur
forties_sulphur = forties_sulphur.loc[forties_sulphur.index > dt(2015,1,1),'buzzard content']
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols=['week ending', 'buzzard content'])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = ['week ending'], usecols=['week ending', 'buzzard content'])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 'week ending', usecols=['week ending', 'buzzard content'])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 0, usecols=['week ending', 'buzzard content'])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 7, usecols=['week ending', 'buzzard content'])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 6, usecols=['week ending', 'buzzard content'])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 1, usecols=['week ending', 'buzzard content'])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 4, usecols=['week ending', 'buzzard content'])
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 2, usecols=['week ending', 'buzzard content'])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 2)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], index_col = 0)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22])
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], usecols=['week ending', 'buzzard content'], index_col = 2)
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols=['week ending', 'buzzard content'], index_col = 2)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols=['H', 'J'], index_col = 2)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols=['H'], index_col = 2)
'H'
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols=["A:E"], index_col = 2)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E", index_col = 2)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', usecols="A:E")
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', parse_cols="A:E")
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', parse_cols="H:I")
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I")
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week wending')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur
forties_sulphur.reindex(total.index).fillna(method='ffill')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending').sort_index(inplace=True)
forties_sulphur
forties_sulphur.sort_index(inplace=True).reindex(total.index).fillna(method='ffill')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur
forties_sulphur.reindex(total.index).fillna(method='ffill')
forties_sulphur.reindex(crude_diffs.index).fillna(method='ffill')
forties_sulphur.reset_index(inplace=True)
forties_sulphur
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
forties_sulphur.index
[pd.notnull(forties_sulphur.index)
pd.notnull(forties_sulphur.index)
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur
forties_sulphur.reindex(total.index).fillna(method='ffill')
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')
forties_sulphur
forties_sulphur['buzzard content']
df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
df_freight['Implied_Sulphur']
df_freight['De-Escalator_Threshold'] = np.around(df_freight['Implied_Sulphur'], 3)
df_freight['De-Escalator_Threshold']
df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['De-Escalator_Threshold'])
df_freight['De-Escalator_Counts']
df_freight['De-Escalator_Counts'] = np.,a(0, 6-df_freight['De-Escalator_Threshold']*1000)
df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['De-Escalator_Threshold']*1000)
df_freight['De-Escalator_Counts']
6-df_freight['De-Escalator_Threshold']*1000
df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
df_freight['De-Escalator_Threshold']
df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
df_freight['De-Escalator_Counts']
df_freight['Platts_De_Esc'] = total['AAUXL00']
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
df_freight['Platts_De_Esc'] = total['AAUXL00']
df_freight
df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
df_freight['Forties_Margin_Impact']
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
sub_region
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018TEST.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
crude = 'Forties'
destination = 'Houston'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']

def freight_and_quality_exceptions():
    if crude in ('Forties'):
        df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
        df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
        df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
        df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
        df_freight['Platts_De_Esc'] = total['AAUXL00']
        df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
        df_freight['Costs'] += df_freight['Forties_Margin_Impact']
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
        func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
        df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
        
        if crude == 'Basrah Light':
            df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
        else:
            df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
        
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
     
     # South Korean particulars
    if ports[ports['Name'] == destination]['Country'] == 'South Korea':
        # Freight rebate on imported crudes
        df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
        df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
        df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
        df_freight['Costs'] -= df_freight['Freight_Rebate']
        
        # Tax rebate on crudes out of Europe
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'] in (['NW EUROPE','MED']):
            df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Tax rebate on crudes out of the US
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'] in (['N AMERICA']):
            df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Costs ascociated with lifting CPC based on delays
        if crude == 'CPC Blend':
            df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
            df_freight['TS_Demur'] = total['AAPED00']
            df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
            df_freight['Costs'] += df_freight['TS_Demur_Costs']
        
        # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
        if crude in (['Urals Nth', 'Urals Med']):
            df_freight['Urals_Cif_Rebate'] = 0.11
            df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
        
        if crude == 'Forties':
            df_freight['Forties_Mkt_Discount'] = 0.5
            df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
    
    return df_freight

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    vessel_size = []
    for i in list(ws_codes['Code']):
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        vessel_size.append(size)
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) - df_freight['Costs']
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] - df_freight['Costs']
    
    if 'WS for Esc' in df_freight.columns.values:
        for i in vessel_size:
            df_freight[i] = df_freight[i] - df_freight['WS for Esc'] - df_freight['API Esc']
    
    return df_freight

calculate_flat_rate()
sub_region
if sub_region in (['US GULF (padd 3)']):

sub_region in (['US GULF (padd 3)'])
destination == 'Rotterdam'
destination == 'Houston'
def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    return df_freight  

calculate_flat_rate()
ports[ports['Name'] == destination]['Country']
ports[ports['Name'] == destination]['Country'].iat[0]
ports[ports['Name'] == assay[crude]['LoadPort']]['Region']
def freight_and_quality_exceptions():
    if crude in ('Forties'):
        df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
        df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
        df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
        df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
        df_freight['Platts_De_Esc'] = total['AAUXL00']
        df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
        df_freight['Costs'] += df_freight['Forties_Margin_Impact']
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
        func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
        df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
        
        if crude == 'Basrah Light':
            df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
        else:
            df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
        
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
     
     # South Korean particulars
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        # Freight rebate on imported crudes
        df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
        df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
        df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
        df_freight['Costs'] -= df_freight['Freight_Rebate']
        
        # Tax rebate on crudes out of Europe
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
            df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Tax rebate on crudes out of the US
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
            df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Costs ascociated with lifting CPC based on delays
        if crude == 'CPC Blend':
            df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
            df_freight['TS_Demur'] = total['AAPED00']
            df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
            df_freight['Costs'] += df_freight['TS_Demur_Costs']
        
        # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
        if crude in (['Urals Nth', 'Urals Med']):
            df_freight['Urals_Cif_Rebate'] = 0.11
            df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
        
        if crude == 'Forties':
            df_freight['Forties_Mkt_Discount'] = 0.5
            df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
    
    return df_freight

def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    return df_freight                

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

calculate_flat_rate()
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

calculate_flat_rate()
calculate_port_costs()
freight_and_quality_exceptions()
calculate_freight()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
#global_arbs.iloc[:, global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage']

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Hous_margin']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,11,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,11,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

global_arbs.columns
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin(['Suez_landed_vs_wti','Suez_landed_vs_dtd'])
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3).isin(['Suez_landed_vs_wti','Suez_landed_vs_dtd'])]
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2017,11,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

suez_margins['simple']['WTI Midland']  
suez_margins['simple']['WTI Midland'].loc[suez_margins.index > dt(2018,1,1),:]       
global_arbs['simple']['WTI Midland'].loc[suez_margins.index > dt(2018,1,1),:] 
global_arbs['simple']['WTI Midland']['Rotterdam'].loc[suez_margins.index > dt(2018,1,1),:]       
writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
global_arbs.to_excel(writer, sheet_name='Arbs')
writer.save()
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']
for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))

random = global_arbs.iloc[-5:,(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')]
random
random = global_arbs.iloc[-5:,(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(['Grade']).mean()
random = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(['Grade']).mean()
random = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby('Grade').mean()
random = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(levels=['Grade']).mean()
random = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level=['Grade']).mean()
random = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')]
random = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(['Grade']).mean()
random = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level = ['Grade'], axis=1).mean()
(global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk']))&
random
random.plot()
runfile('C:/Users/mima/.spyder-py3/DOE DECADE CONVERSION.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
#global_arbs.iloc[:, global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage']

for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))


suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']

dtd_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level = ['Grade'], axis=1).mean()

dtd_landed_comparison.plot(title='Rotterdam Landed')

wti_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level = ['Grade'], axis=1).mean()

wti_landed_comparison.plot(title='Houston Landed')
global_arbs
suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']

for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))




dtd_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level = ['Grade'], axis=1).mean()

dtd_landed_comparison.plot(title='Rotterdam Landed')

wti_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level = ['Grade'], axis=1).mean()

wti_landed_comparison.plot(title='Houston Landed')
wti_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level = ['Grade'], axis=1).mean()

wti_landed_comparison.plot(title='Houston Landed')
wti_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')].groupby(level = ['Grade'], axis=1).mean()
wti_landed_comparison.plot(title='Houston Landed')
wti_landed_comparison
wti_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Houston')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_wti')].groupby(level = ['Grade'], axis=1).mean()

wti_landed_comparison.plot(title='Houston Landed')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs.xlsx')
global_arbs.to_excel(writer, sheet_name='Arbs')
suez_margins.to_excel(writer, sheet_name='Suez Adv')
afra_margins.to_excel(writer, sheet_name='Afra Adv')
writer.save()
dtd_landed_comparison
dtd_landed_comparison.plot(title='Rotterdam Landed')
dtd_landed_comparison
dtd_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
         (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
         (global_arbs.columns.get_level_values('Region')=='Rotterdam')
         ].groupby(level = ['Grade'], axis=1).mean()
dtd_landed_comparison
global_arbs
dtd_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')&
                 (global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')]
dtd_landed_comparison
&
(global_arbs.columns.get_level_values('Series')=='Suez_landed_vs_dtd')
dtd_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')]
dtd_landed_comparison
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//SaharanRott.xlsx')
dtd_landed_comparison.to_excel(writer, sheet_name='Arbs')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
#global_arbs.iloc[:, global_arbs.columns.get_level_values('Series')=='Suez_margin_advantage']

suez_margins = global_arbs.iloc[:, global_arbs.columns.get_level_values(3)=='Suez_margin_advantage']

for i in crudes:
    for j in refinery_configurations.keys():
        try:
            suez_margins[j][i].iloc[suez_margins.index > dt(2018,1,1)].rolling(5, min_periods=1).mean().plot(title=i+' '+j)
        except Exception as e: print('{} passed'.format(i))



dtd_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')]
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//SaharanRott1.xlsx')
dtd_landed_comparison.to_excel(writer, sheet_name='Arbs')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//SaharanRott2.xlsx')
dtd_landed_comparison.to_excel(writer, sheet_name='Arbs')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
dtd_landed_comparison = global_arbs.iloc[global_arbs.index > dt(2018,1,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Bakken','BaseBlendUSGC','Eagleford 45','LLS','Mars','Forties','Ekofisk','Saharan']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam')]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//SaharanRott2.xlsx')
dtd_landed_comparison.to_excel(writer, sheet_name='Arbs')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region')=='Rotterdam','Augusta')]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//SaharanRott2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin['Rotterdam','Augusta'])]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//SaharanRott2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# ==========================================================
basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin['Rotterdam','Augusta'])]
basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//SaharanRott2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

crude = 'Basrah Light'
destination = 'Rotterdam'

#crude = 'Forties'
#destination = 'Houston'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    return df_freight  

def freight_and_quality_exceptions():
    if crude in ('Forties'):
        df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
        df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
        df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
        df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
        df_freight['Platts_De_Esc'] = total['AAUXL00']
        df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
        df_freight['Costs'] += df_freight['Forties_Margin_Impact']
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
        func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
        df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
        
        if crude == 'Basrah Light':
            df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
        else:
            df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
        
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
     
     # South Korean particulars
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        # Freight rebate on imported crudes
        df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
        df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
        df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
        df_freight['Costs'] -= df_freight['Freight_Rebate']
        
        # Tax rebate on crudes out of Europe
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
            df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Tax rebate on crudes out of the US
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
            df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Costs ascociated with lifting CPC based on delays
        if crude == 'CPC Blend':
            df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
            df_freight['TS_Demur'] = total['AAPED00']
            df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
            df_freight['Costs'] += df_freight['TS_Demur_Costs']
        
        # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
        if crude in (['Urals Nth', 'Urals Med']):
            df_freight['Urals_Cif_Rebate'] = 0.11
            df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
        
        if crude == 'Forties':
            df_freight['Forties_Mkt_Discount'] = 0.5
            df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
    
    return df_freight

calculate_flat_rate()
freight_and_quality_exceptions()
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes
vessel_size = []
i = 'PFAGN10'
size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
size
vessel_size.append(size)
name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
name
df_freight[name] = total[i]
df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
calculate_port_costs()
df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
df_freight[size]
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
df_prices = convert_prices()
    def convert_prices():
        if crude in ('Basrah Light','Basrah Heavy'):
            df_prices['diff'] = diff + crude_diffs[crude]
        else:
            df_prices['diff'] = diff
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/7) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
df_prices = convert_prices()
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
price_index
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    
    
    if assay[crude]['Code'] == 'multiple':     
        diff = total[exceptions[crude][discharge_price_region]['Code']]
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            
            
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
                      (rate_data['DischargePort'] == 'Singapore')]
                v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
                df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            return df_freight
        
        def calculate_port_costs():
            """These are for the odd costs, tax rebates, etc"""
            df_freight['Costs'] = 0
            
            # This is the export cost out of Houston
            if sub_region in (['US GULF (padd 3)']):
                df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
                df_freight['Costs'] += df_freight['Houston_Load_Costs']
            
            # Port costs to discharge in Rotterdam               
            if destination == 'Rotterdam':
                df_freight['Rott_Discharge_Costs'] = 0.15
                df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
            
            # Port costs to discharge in Houston
            if destination == 'Houston':
                df_freight['Hous_Discharge_Costs'] = 0.25
                df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
            
            if assay[crude]['LoadPort'] == 'Basrah':
                df_freight['Basrah_Costs'] = 0.76
                df_freight['Costs'] += df_freight['Basrah_Costs']
            
            if assay[crude]['LoadPort'] == 'Ras Tanura':
                df_freight['Saudi_Costs'] = 0.66
                df_freight['Costs'] += df_freight['Saudi_Costs']
            
            return df_freight                
        
        
        
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
                df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
                df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
                df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
                df_freight['Platts_De_Esc'] = total['AAUXL00']
                df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
                df_freight['Costs'] += df_freight['Forties_Margin_Impact']
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
                df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
             
             # South Korean particulars
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                # Freight rebate on imported crudes
                df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
                df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
                df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
                df_freight['Costs'] -= df_freight['Freight_Rebate']
                
                # Tax rebate on crudes out of Europe
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
                    df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Tax rebate on crudes out of the US
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
                    df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Costs ascociated with lifting CPC based on delays
                if crude == 'CPC Blend':
                    df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
                    df_freight['TS_Demur'] = total['AAPED00']
                    df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
                    df_freight['Costs'] += df_freight['TS_Demur_Costs']
                
                # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
                if crude in (['Urals Nth', 'Urals Med']):
                    df_freight['Urals_Cif_Rebate'] = 0.11
                    df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
                
                if crude == 'Forties':
                    df_freight['Forties_Mkt_Discount'] = 0.5
                    df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                i = 'PFAGN10'
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
            
            return df_freight
        
        calculate_flat_rate()
        calculate_port_costs()
        freight_and_quality_exceptions()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        if crude in ('Basrah Light','Basrah Heavy'):
            df_prices['Basrah_OSP'] = diff
            df_prices['Basrah_vs_OSP'] = crude_diffs[crude]
            df_prices['diff'] = diff + crude_diffs[crude]
        else:
            df_prices['diff'] = diff
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/7) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    try:
        df_freight = construct_freight()
    except Exception as e: print(e), print('df_freight')    
    
    try:
        df_prices = convert_prices()
    except Exception as e: print(e), print('df_prices') 
temp = pd.concat([df_prices,df_freight], axis=1)
temp
temp = pd.concat([df_prices,df_freight], axis=1)
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]

try:
    for k in freight_list:
        try:
            name = str(k[:4]) + str('_landed_') + str(price_index)
        except Exception as e: print('name fails') 
        try:
            temp[name] = df_prices[price_index].add(df_freight[k])
        except Exception as e: print('temp fails')    
except Exception as e: print('check')

temp
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    
    
    if assay[crude]['Code'] == 'multiple':     
        diff = total[exceptions[crude][discharge_price_region]['Code']]
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            
            
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
                      (rate_data['DischargePort'] == 'Singapore')]
                v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
                df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            return df_freight
        
        def calculate_port_costs():
            """These are for the odd costs, tax rebates, etc"""
            df_freight['Costs'] = 0
            
            # This is the export cost out of Houston
            if sub_region in (['US GULF (padd 3)']):
                df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
                df_freight['Costs'] += df_freight['Houston_Load_Costs']
            
            # Port costs to discharge in Rotterdam               
            if destination == 'Rotterdam':
                df_freight['Rott_Discharge_Costs'] = 0.15
                df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
            
            # Port costs to discharge in Houston
            if destination == 'Houston':
                df_freight['Hous_Discharge_Costs'] = 0.25
                df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
            
            if assay[crude]['LoadPort'] == 'Basrah':
                df_freight['Basrah_Costs'] = 0.76
                df_freight['Costs'] += df_freight['Basrah_Costs']
            
            if assay[crude]['LoadPort'] == 'Ras Tanura':
                df_freight['Saudi_Costs'] = 0.66
                df_freight['Costs'] += df_freight['Saudi_Costs']
            
            return df_freight                
        
        
        
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
                df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
                df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
                df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
                df_freight['Platts_De_Esc'] = total['AAUXL00']
                df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
                df_freight['Costs'] += df_freight['Forties_Margin_Impact']
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
                df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
             
             # South Korean particulars
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                # Freight rebate on imported crudes
                df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
                df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
                df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
                df_freight['Costs'] -= df_freight['Freight_Rebate']
                
                # Tax rebate on crudes out of Europe
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
                    df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Tax rebate on crudes out of the US
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
                    df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Costs ascociated with lifting CPC based on delays
                if crude == 'CPC Blend':
                    df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
                    df_freight['TS_Demur'] = total['AAPED00']
                    df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
                    df_freight['Costs'] += df_freight['TS_Demur_Costs']
                
                # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
                if crude in (['Urals Nth', 'Urals Med']):
                    df_freight['Urals_Cif_Rebate'] = 0.11
                    df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
                
                if crude == 'Forties':
                    df_freight['Forties_Mkt_Discount'] = 0.5
                    df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                i = 'PFAGN10'
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
            
            return df_freight
        
        calculate_flat_rate()
        calculate_port_costs()
        freight_and_quality_exceptions()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        if crude in ('Basrah Light','Basrah Heavy'):
            df_prices['Basrah_OSP'] = diff
            df_prices['Basrah_current_diff'] = crude_diffs[crude]
            df_prices['diff'] = diff + crude_diffs[crude]
        else:
            df_prices['diff'] = diff
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/7) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    try:
        df_freight = construct_freight()
    except Exception as e: print(e), print('df_freight')    
    
    try:
        df_prices = convert_prices()
    except Exception as e: print(e), print('df_prices') 
    
    temp = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
    
    try:
        for k in freight_list:
            try:
                name = str(k[:4]) + str('_landed_') + str(price_index)
            except Exception as e: print('name fails') 
            try:
                temp[name] = df_prices[price_index].add(df_freight[k])
            except Exception as e: print('temp fails')    
    except Exception as e: print('check')
temp
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = total['WTCLc1-LCOc1']
    wtim1_m2 = total['CLc1-CLc2']
    brentm1_m2 = total['LCOc1-LCOc2']
    efpm2 = total['AAGVX00']
    brentm2 = total['LCOc2']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    
    
    if assay[crude]['Code'] == 'multiple':     
        diff = total[exceptions[crude][discharge_price_region]['Code']]
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            
            
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
                      (rate_data['DischargePort'] == 'Singapore')]
                v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
                df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            return df_freight
        
        def calculate_port_costs():
            """These are for the odd costs, tax rebates, etc"""
            df_freight['Costs'] = 0
            
            # This is the export cost out of Houston
            if sub_region in (['US GULF (padd 3)']):
                df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
                df_freight['Costs'] += df_freight['Houston_Load_Costs']
            
            # Port costs to discharge in Rotterdam               
            if destination == 'Rotterdam':
                df_freight['Rott_Discharge_Costs'] = 0.15
                df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
            
            # Port costs to discharge in Houston
            if destination == 'Houston':
                df_freight['Hous_Discharge_Costs'] = 0.25
                df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
            
            if assay[crude]['LoadPort'] == 'Basrah':
                df_freight['Basrah_Costs'] = 0.76
                df_freight['Costs'] += df_freight['Basrah_Costs']
            
            if assay[crude]['LoadPort'] == 'Ras Tanura':
                df_freight['Saudi_Costs'] = 0.66
                df_freight['Costs'] += df_freight['Saudi_Costs']
            
            return df_freight                
        
        
        
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
                df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
                df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
                df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
                df_freight['Platts_De_Esc'] = total['AAUXL00']
                df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
                df_freight['Costs'] += df_freight['Forties_Margin_Impact']
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
                df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
             
             # South Korean particulars
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                # Freight rebate on imported crudes
                df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
                df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
                df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
                df_freight['Costs'] -= df_freight['Freight_Rebate']
                
                # Tax rebate on crudes out of Europe
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
                    df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Tax rebate on crudes out of the US
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
                    df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Costs ascociated with lifting CPC based on delays
                if crude == 'CPC Blend':
                    df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
                    df_freight['TS_Demur'] = total['AAPED00']
                    df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
                    df_freight['Costs'] += df_freight['TS_Demur_Costs']
                
                # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
                if crude in (['Urals Nth', 'Urals Med']):
                    df_freight['Urals_Cif_Rebate'] = 0.11
                    df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
                
                if crude == 'Forties':
                    df_freight['Forties_Mkt_Discount'] = 0.5
                    df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                #i = 'PFAGN10'
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
            
            return df_freight
        
        calculate_flat_rate()
        calculate_port_costs()
        freight_and_quality_exceptions()
        calculate_freight()
        return df_freight
    
    def convert_prices():
        if crude in ('Basrah Light','Basrah Heavy'):
            df_prices['Basrah_OSP'] = diff
            df_prices['Basrah_current_diff'] = crude_diffs[crude]
            df_prices['diff'] = diff + crude_diffs[crude]
        else:
            df_prices['diff'] = diff
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                df_prices['cfd3'] = cfd3
                df_prices['cfd5'] = cfd5
                if sub_region == sub_region_2:
                    df_prices['vs_dtd'] = diff
                elif sub_region == 'WAF':
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/14) * days
                else:
                    df_prices['vs_dtd'] = diff + ((cfd3 - cfd5)/7) * days
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
    
    try:
        df_freight = construct_freight()
    except Exception as e: print(e), print('df_freight')    
    
    try:
        df_prices = convert_prices()
    except Exception as e: print(e), print('df_prices') 
    
    temp = pd.concat([df_prices,df_freight], axis=1)
    price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
    freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
    
    try:
        for k in freight_list:
            try:
                name = str(k[:4]) + str('_landed_') + str(price_index)
            except Exception as e: print('name fails') 
            try:
                temp[name] = df_prices[price_index].add(df_freight[k])
            except Exception as e: print('temp fails')    
    except Exception as e: print('check')
temp
temp.tail()
temp.tail(-20)
temp.tail(-10)
temp.head(-10)
temp
temp.resample('W')
temp
temp.resample('W').mean()
temp.resample('W', convention = 'end').mean()
temp.resample('W', convention = 'start').mean()
temp.resample('W-FRI').mean()
temp.resample('W-MON').mean()
temp.resample('W').mean()
temp.resample('W-FRI').mean()
temp
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill')
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
from pandas.tseries.offsets import BDay
BDay()
0*BDay()
crude_diffs.index.map(lambda x : x + 0*BDay())
crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')
crude_diffs
monthly_averages = total['PFAOH00'].resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]

"""Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]       
func_ma_on_days
monthly_averages
total['PFAOH00']
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean()
monthly_averages
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded2')
writer.save()
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]

"""Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)

if crude == 'Basrah Light':
    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
else:
    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               


df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
df_freight.drop(['Date'], axis = 1, inplace=True)
df_freight
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
from pandas.tseries.offsets import BDay
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    if assay[crude]['LoadPort'] == 'Basrah':
        df_freight['Basrah_Costs'] = 0.76
        df_freight['Costs'] += df_freight['Basrah_Costs']
    
    if assay[crude]['LoadPort'] == 'Ras Tanura':
        df_freight['Saudi_Costs'] = 0.66
        df_freight['Costs'] += df_freight['Saudi_Costs']
    
    return df_freight 

calculate_flat_rate()
calculate_port_costs()
crude = 'Basrah Light'
destination = 'Rotterdam'

#crude = 'Forties'
#destination = 'Houston'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight


def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    if assay[crude]['LoadPort'] == 'Basrah':
        df_freight['Basrah_Costs'] = 0.76
        df_freight['Costs'] += df_freight['Basrah_Costs']
    
    if assay[crude]['LoadPort'] == 'Ras Tanura':
        df_freight['Saudi_Costs'] = 0.66
        df_freight['Costs'] += df_freight['Saudi_Costs']
    
    return df_freight  

calculate_flat_rate()
calculate_port_costs()
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
monthly_averages
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)
df_freight
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
from pandas.tseries.offsets import BDay



def import_data():
    t2 = time.process_time()
    #data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
    #raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')
    
    data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
    raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')
    
    trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')
    
    
    assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
    ws = pd.read_excel(data, 'ws')
    expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
    ports = pd.read_excel(data, 'ports')
    sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
    sub_to_ws = sub_to_ws.set_index([0]).to_dict()
    
    """table containing the basrah base worldscale that they fix their freight against"""
    basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')
    
    """Take in the crude prices and codes and convert to a dataframe.
    We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
    Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
    #prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  
    
    """Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
    We don't drop rows now as dropping would be dependent on any nans in any column"""
    #total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
    #total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
    #total = total.iloc[total.index > dt(2015,12,31)]
    
    """this new total table generates all the prices in one place for us"""
    
    total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
    
    total.index = pd.to_datetime(total.index)
    total.sort_index(inplace=True)
    total.fillna(method='ffill', inplace=True)
    total = total[total.index > dt(2015,1,1)]
    
    
    """Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
    cleaned_column_headers = [i.strip() for i in total.columns.values]
    total.columns = cleaned_column_headers
    
    """The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
    crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
    crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
    crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
    crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
    crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')
crude_diffs
crude_diffs['Basrah Light']
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
codes_list
total.update(crude_diffs[codes_list])
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
total
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
from pandas.tseries.offsets import BDay
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
diff
crude = 'Basrah Light'
destination = 'Rotterdam'

#crude = 'Forties'
#destination = 'Houston'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs = crude_diffs.reindex(total.index).fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])

#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
crude = 'Basrah Light'
destination = 'Rotterdam'

#crude = 'Forties'
#destination = 'Houston'

"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = total['WTCLc1-LCOc1']
wtim1_m2 = total['CLc1-CLc2']
brentm1_m2 = total['LCOc1-LCOc2']
efpm2 = total['AAGVX00']
brentm2 = total['LCOc2']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }



if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']]
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

diff
total
total[exceptions[crude][discharge_price_region]['Code']]
diff
diff = total[exceptions[crude][discharge_price_region]['Code']]
diff
diff = total[exceptions[crude][discharge_price_region]['Code']].shift(1, freq='MS')
diff
diff = total[exceptions[crude][discharge_price_region]['Code']].shift(1, freq='M')
diff
diff = total[exceptions[crude][discharge_price_region]['Code']].shift(1, freq='D')
diff
total[exceptions[crude][discharge_price_region]['Code']]
total[exceptions[crude][discharge_price_region]['Code']].resample('BMS').mean()
total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean()
total[exceptions[crude][discharge_price_region]['Code']].resample('M').mean()
total[exceptions[crude][discharge_price_region]['Code']].resample('M').mean().shift(1, freq='MS')
otal[exceptions[crude][discharge_price_region]['Code']].resample('M').mean().shift(-1, freq='MS')
total[exceptions[crude][discharge_price_region]['Code']].resample('M').mean().shift(-1, freq='MS')
total[exceptions[crude][discharge_price_region]['Code']].resample('M').mean().shift(-2, freq='MS')
total[exceptions[crude][discharge_price_region]['Code']].resample('M').mean()
total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean()
total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-2, freq='MS')
total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS')
total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
df_freight[['WS for Esc','API Esc']]
df_freight
if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    if assay[crude]['LoadPort'] == 'Basrah':
        df_freight['Basrah_Costs'] = 0.76
        df_freight['Costs'] += df_freight['Basrah_Costs']
    
    if assay[crude]['LoadPort'] == 'Ras Tanura':
        df_freight['Saudi_Costs'] = 0.66
        df_freight['Costs'] += df_freight['Saudi_Costs']
    
    return df_freight 

monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]

"""Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25
df_freight['Flat Rate'] = df_freight['Date'].apply(func_fr)

if crude == 'Basrah Light':
    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
else:
    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               


df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Flat Rate'] / 7.3 / 100
df_freight.drop(['Date'], axis = 1, inplace=True)
df_freight
df_freight[['WS for Esc','API Esc']]
df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
df_freight
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]

"""Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)

if crude == 'Basrah Light':
    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
else:
    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               



df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
df_freight.drop(['Date'], axis = 1, inplace=True)
df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')

df_freight
(df_freight['WS Month Avg'] - df_freight['SOMO Base WS'])
(df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] 
(df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3
(df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
df_freight
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]

"""Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)

if crude == 'Basrah Light':
    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
else:
    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               



df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
df_freight.drop(['Date'], axis = 1, inplace=True)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)

#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill')
crude_diffs
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')
crude_diffs
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah2.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah3.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/GPW2504.py', wdir='C:/Users/mima/.spyder-py3')
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
# =============================================================================
#writer = pd.ExcelWriter('C://Users//mima//Documents//GlobalArbs20.xlsx')
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//Basrah3.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
# =============================================================================
# suez_margins.to_excel(writer, sheet_name='Suez Adv')
# afra_margins.to_excel(writer, sheet_name='Afra Adv')
# =============================================================================
writer.save()
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]


writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs.xlsx')
basrah_suez.to_excel(writer, sheet_name='BasrahLanded')
writer.save()
global_arbs
writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs.xlsx')
global_arbs.to_excel(writer, sheet_name='global_arbs')
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]


writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs2.xlsx')
global_arbs.to_excel(writer, sheet_name='global_arbs')
writer.save()
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
total['AALSM01'].loc[total['AALSM01'] < 25,:]
total['AALSM01'].loc[total['AALSM01'] < 25]
total['AALSM01']
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])
total['AALSM01'].loc[total['AALSM01'] < 25]
total['AALSM01']
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])




"""We have to convert the prices that are in absolutes into a diff vs a local index"""
total['AALSM01'].loc[total['AALSM01'] < 25]
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
codes_list = [x for x in crude_diffs.columns if x not in ('Basrah Light','Basrah Heavy','LOOP SOUR')]
codes_list
crude_diffs.columns
total.update(crude_diffs[codes_list])
total['AALSM01'].loc[total['AALSM01'] < 25]
total['AALSM01'].loc[total['AALSM01'] > 25]
total['AALSM01'].loc[total['AALSM01'] < 25]
total['AALSM01'].loc[total['AALSM01'] > 2]
total['AALSM01'].loc[total['AALSM01'] > 2]
total['AALSM01'].loc[total['AALSM01'] < 2]
total['AALSM01'].loc[total['AALSM01'] > 10]
total['AALSM01'].loc[total['AALSM01'] > 5]
total['AALSM01'].loc[total['AALSM01'] > 3]
total['AALSM01'].loc[total['AALSM01'] > 30] = total['AALSM01'].loc[total['AALSM01'] > 30] - total['CLc1']
total['AALSM01']
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
codes_list = [x for x in crude_diffs.columns if x not in crudes_diff_against_osp]
codes_list
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]


writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs3.xlsx')
global_arbs.to_excel(writer, sheet_name='global_arbs')
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)

#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]


writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs3.xlsx')
global_arbs.to_excel(writer, sheet_name='global_arbs')
writer.save()
""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes = ['Amna']
#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs.columns
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
codes_list = [x for x in crude_diffs.columns if x not in crudes_diff_against_osp]
crude_diffs
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)
crude_diffs
crude_diffs.columns
"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
codes_list = [x for x in crude_diffs.columns if x not in crudes_diff_against_osp]
codes_list
crude_diffs.columns
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes = ['Amna']
#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]


writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs3.xlsx')
global_arbs.to_excel(writer, sheet_name='global_arbs')
writer.save()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
freight_and_quality_exceptions()
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
codes_list = [x for x in crude_diffs.columns if x not in crudes_diff_against_osp]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])




"""We have to convert the prices that are in absolutes into a diff vs a local index, and if there are, set to zero"""
total['AALSM01'].loc[total['AALSM01'] > 30] = total['AALSM01'].loc[total['AALSM01'] > 30] - total['CLc1']
#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
crude = 'Forties'
destination = 'Houston'
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
brentm1 = total['LCOc1']
brentm2 = total['LCOc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = wtim1 - brentm1
wtim1_m2 = wtim1-wtim2
brentm1_m2 = brentm1 - brentm2
efpm2 = total['AAGVX00']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }


"""Need to handle the one month forward OSP concept, so here, take the dataframe for the exceptions above, condense to monhtly values which wont
change the value as same for each day, shift that forward then re-expand"""
if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    if assay[crude]['LoadPort'] == 'Basrah':
        df_freight['Basrah_Costs'] = 0.76
        df_freight['Costs'] += df_freight['Basrah_Costs']
    
    if assay[crude]['LoadPort'] == 'Ras Tanura':
        df_freight['Saudi_Costs'] = 0.66
        df_freight['Costs'] += df_freight['Saudi_Costs']
    
    return df_freight     

def freight_and_quality_exceptions():
    if crude in ('Forties'):
        df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
        df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
        df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
        df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
        df_freight['Platts_De_Esc'] = total['AAUXL00']
        df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
        df_freight['Costs'] += df_freight['Forties_Margin_Impact']
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
        func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
        df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
        
        if crude == 'Basrah Light':
            df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
        else:
            df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
        
        
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
        #df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
     
     # South Korean particulars
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        # Freight rebate on imported crudes
        df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
        df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
        df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
        df_freight['Costs'] -= df_freight['Freight_Rebate']
        
        # Tax rebate on crudes out of Europe
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
            df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Tax rebate on crudes out of the US
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
            df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Costs ascociated with lifting CPC based on delays
        if crude == 'CPC Blend':
            df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
            df_freight['TS_Demur'] = total['AAPED00']
            df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
            df_freight['Costs'] += df_freight['TS_Demur_Costs']
        
        # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
        if crude in (['Urals Nth', 'Urals Med']):
            df_freight['Urals_Cif_Rebate'] = 0.11
            df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
        
        if crude == 'Forties':
            df_freight['Forties_Mkt_Discount'] = 0.5
            df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
        else:
            pass
    
    
    
    return df_freight

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    vessel_size = []
    for i in list(ws_codes['Code']):
        #i = 'PFAGN10'
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        vessel_size.append(size)
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
    
    if 'WS for Esc' in df_freight.columns.values:
        for i in vessel_size:
            df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
    
    return df_freight

calculate_flat_rate()
calculate_port_costs()
freight_and_quality_exceptions()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
brentm1 = total['LCOc1']
brentm2 = total['LCOc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = wtim1 - brentm1
wtim1_m2 = wtim1-wtim2
brentm1_m2 = brentm1 - brentm2
efpm2 = total['AAGVX00']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }


"""Need to handle the one month forward OSP concept, so here, take the dataframe for the exceptions above, condense to monhtly values which wont
change the value as same for each day, shift that forward then re-expand"""
if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

calculate_flat_rate()
freight_and_quality_exceptions()
calculate_port_costs()
freight_and_quality_exceptions()
calculate_freight()
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes = ['Amna']
#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e) 
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
df = pd.DataFrame(index=total.index)
df['Date'] = df.index
data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
codes_list = [x for x in crude_diffs.columns if x not in crudes_diff_against_osp]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])




"""We have to convert the prices that are in absolutes into a diff vs a local index, and if there are, set to zero"""
total['AALSM01'].loc[total['AALSM01'] > 30] = total['AALSM01'].loc[total['AALSM01'] > 30] - total['CLc1']
#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
total
for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)
df
"""create the dataframes for use later"""
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
df_freight
index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
brentm1 = total['LCOc1']
brentm2 = total['LCOc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = wtim1 - brentm1
wtim1_m2 = wtim1-wtim2
brentm1_m2 = brentm1 - brentm2
efpm2 = total['AAGVX00']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }


"""Need to handle the one month forward OSP concept, so here, take the dataframe for the exceptions above, condense to monhtly values which wont
change the value as same for each day, shift that forward then re-expand"""
if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

    crude = 'Forties'
    destination = 'Houston'
    
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    brentm1 = total['LCOc1']
    brentm2 = total['LCOc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = wtim1 - brentm1
    wtim1_m2 = wtim1-wtim2
    brentm1_m2 = brentm1 - brentm2
    efpm2 = total['AAGVX00']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    
    """Need to handle the one month forward OSP concept, so here, take the dataframe for the exceptions above, condense to monhtly values which wont
    change the value as same for each day, shift that forward then re-expand"""
    if assay[crude]['Code'] == 'multiple':     
        diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            
            
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
                      (rate_data['DischargePort'] == 'Singapore')]
                v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
                df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            return df_freight
        
        def calculate_port_costs():
            """These are for the odd costs, tax rebates, etc"""
            df_freight['Costs'] = 0
            
            # This is the export cost out of Houston
            if sub_region in (['US GULF (padd 3)']):
                df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
                df_freight['Costs'] += df_freight['Houston_Load_Costs']
            
            # Port costs to discharge in Rotterdam               
            if destination == 'Rotterdam':
                df_freight['Rott_Discharge_Costs'] = 0.15
                df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
            
            # Port costs to discharge in Houston
            if destination == 'Houston':
                df_freight['Hous_Discharge_Costs'] = 0.25
                df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
            
            if assay[crude]['LoadPort'] == 'Basrah':
                df_freight['Basrah_Costs'] = 0.76
                df_freight['Costs'] += df_freight['Basrah_Costs']
            
            if assay[crude]['LoadPort'] == 'Ras Tanura':
                df_freight['Saudi_Costs'] = 0.66
                df_freight['Costs'] += df_freight['Saudi_Costs']
            
            return df_freight                
        
        
        
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
                df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
                df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
                df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
                df_freight['Platts_De_Esc'] = total['AAUXL00']
                df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
                df_freight['Costs'] += df_freight['Forties_Margin_Impact']
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
                df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
                #df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
             
             # South Korean particulars
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                # Freight rebate on imported crudes
                df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
                df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
                df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
                df_freight['Costs'] -= df_freight['Freight_Rebate']
                
                # Tax rebate on crudes out of Europe
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
                    df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Tax rebate on crudes out of the US
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
                    df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Costs ascociated with lifting CPC based on delays
                if crude == 'CPC Blend':
                    df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
                    df_freight['TS_Demur'] = total['AAPED00']
                    df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
                    df_freight['Costs'] += df_freight['TS_Demur_Costs']
                
                # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
                if crude in (['Urals Nth', 'Urals Med']):
                    df_freight['Urals_Cif_Rebate'] = 0.11
                    df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
                
                if crude == 'Forties':
                    df_freight['Forties_Mkt_Discount'] = 0.5
                    df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
            else:
                 pass
            
            
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                #i = 'PFAGN10'
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
            
            return df_freight
        
        
        calculate_flat_rate()
        
        
        
        calculate_port_costs()
        
        
        #try:
        freight_and_quality_exceptions()
# =============================================================================
#         except Exception as e: 
#             print('freight_and_quality_exceptions() fails') 
#             print(e.args)
#             print(e.message)
#             print(e)
# =============================================================================
        
        
        calculate_freight()
    crude = 'Forties'
    destination = 'Houston'
    
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    brentm1 = total['LCOc1']
    brentm2 = total['LCOc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = wtim1 - brentm1
    wtim1_m2 = wtim1-wtim2
    brentm1_m2 = brentm1 - brentm2
    efpm2 = total['AAGVX00']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    
    """Need to handle the one month forward OSP concept, so here, take the dataframe for the exceptions above, condense to monhtly values which wont
    change the value as same for each day, shift that forward then re-expand"""
    if assay[crude]['Code'] == 'multiple':     
        diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            
            
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
                      (rate_data['DischargePort'] == 'Singapore')]
                v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
                df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            return df_freight
        
        def calculate_port_costs():
            """These are for the odd costs, tax rebates, etc"""
            df_freight['Costs'] = 0
            
            # This is the export cost out of Houston
            if sub_region in (['US GULF (padd 3)']):
                df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
                df_freight['Costs'] += df_freight['Houston_Load_Costs']
            
            # Port costs to discharge in Rotterdam               
            if destination == 'Rotterdam':
                df_freight['Rott_Discharge_Costs'] = 0.15
                df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
            
            # Port costs to discharge in Houston
            if destination == 'Houston':
                df_freight['Hous_Discharge_Costs'] = 0.25
                df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
            
            if assay[crude]['LoadPort'] == 'Basrah':
                df_freight['Basrah_Costs'] = 0.76
                df_freight['Costs'] += df_freight['Basrah_Costs']
            
            if assay[crude]['LoadPort'] == 'Ras Tanura':
                df_freight['Saudi_Costs'] = 0.66
                df_freight['Costs'] += df_freight['Saudi_Costs']
            
            return df_freight                
        
        
        
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
                df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
                df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
                df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
                df_freight['Platts_De_Esc'] = total['AAUXL00']
                df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
                df_freight['Costs'] += df_freight['Forties_Margin_Impact']
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
                df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
                #df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
             
             # South Korean particulars
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                # Freight rebate on imported crudes
                df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
                df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
                df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
                df_freight['Costs'] -= df_freight['Freight_Rebate']
                
                # Tax rebate on crudes out of Europe
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
                    df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Tax rebate on crudes out of the US
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
                    df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Costs ascociated with lifting CPC based on delays
                if crude == 'CPC Blend':
                    df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
                    df_freight['TS_Demur'] = total['AAPED00']
                    df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
                    df_freight['Costs'] += df_freight['TS_Demur_Costs']
                
                # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
                if crude in (['Urals Nth', 'Urals Med']):
                    df_freight['Urals_Cif_Rebate'] = 0.11
                    df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
                
                if crude == 'Forties':
                    df_freight['Forties_Mkt_Discount'] = 0.5
                    df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
            else:
                 pass
            
            
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                #i = 'PFAGN10'
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
            
            return df_freight
        
        
        calculate_flat_rate()
        
        
        
        calculate_port_costs()
        
        
        #try:
        freight_and_quality_exceptions()
# =============================================================================
#         except Exception as e: 
#             print('freight_and_quality_exceptions() fails') 
#             print(e.args)
#             print(e.message)
#             print(e)
# =============================================================================
        
        
        calculate_freight()
        
        return df_freight
calculate_flat_rate()
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight


def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    if assay[crude]['LoadPort'] == 'Basrah':
        df_freight['Basrah_Costs'] = 0.76
        df_freight['Costs'] += df_freight['Basrah_Costs']
    
    if assay[crude]['LoadPort'] == 'Ras Tanura':
        df_freight['Saudi_Costs'] = 0.66
        df_freight['Costs'] += df_freight['Saudi_Costs']
    
    return df_freight                





def freight_and_quality_exceptions():
    if crude in ('Forties'):
        df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
        df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
        df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
        df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
        df_freight['Platts_De_Esc'] = total['AAUXL00']
        df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
        df_freight['Costs'] += df_freight['Forties_Margin_Impact']
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
        func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
        df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
        
        if crude == 'Basrah Light':
            df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
        else:
            df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
        
        
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
        #df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
     
     # South Korean particulars
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        # Freight rebate on imported crudes
        df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
        df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
        df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
        df_freight['Costs'] -= df_freight['Freight_Rebate']
        
        # Tax rebate on crudes out of Europe
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
            df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Tax rebate on crudes out of the US
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
            df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Costs ascociated with lifting CPC based on delays
        if crude == 'CPC Blend':
            df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
            df_freight['TS_Demur'] = total['AAPED00']
            df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
            df_freight['Costs'] += df_freight['TS_Demur_Costs']
        
        # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
        if crude in (['Urals Nth', 'Urals Med']):
            df_freight['Urals_Cif_Rebate'] = 0.11
            df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
        
        if crude == 'Forties':
            df_freight['Forties_Mkt_Discount'] = 0.5
            df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
    else:
         pass
    
    
    
    return df_freight


def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    vessel_size = []
    for i in list(ws_codes['Code']):
        #i = 'PFAGN10'
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        vessel_size.append(size)
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
    
    if 'WS for Esc' in df_freight.columns.values:
        for i in vessel_size:
            df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
    
    return df_freight

calculate_flat_rate()
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
from pandas.tseries.offsets import BDay

calculate_flat_rate()
calculate_port_costs()
freight_and_quality_exceptions()
calculate_freight()
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
construct_freight()
crude = 'Amna'
df_freight = construct_freight()
df_freight
df_freight = construct_freight()
try:
    df_freight = construct_freight()
except Exception as e: print(e), print('df_freight')    

df_freight
    crude = 'Amna'
    #destination = 'Houston'
    
    """create the dataframes for use later"""
    df_freight = pd.DataFrame(index=df.index)
    df_prices = pd.DataFrame(index=df.index)
    
    index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
    index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
    index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
    crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
    
    """Declare the main prices that will be used in order to use shorthand notation"""
    
    
    dtd = total['PCAAS00']
    dub = total['AAVMR00']
    wtim1 = total['CLc1'] 
    wtim2 = total['CLc2']
    brentm1 = total['LCOc1']
    brentm2 = total['LCOc2']
    wti_cma_m1 = total['AAVSN00']
    cfd1 = total['PCAKG00']
    cfd2 = total['AAGLV00']
    cfd3 = total['PCAKE00']
    cfd4 = total['PCAKG00']
    cfd5 = total['AAGLU00']
    cfd6 = total['AAGLV00']
    cfd7 = total['AALCZ00']
    cfd8 = total['AALDA00']
    wti_br_m1 = wtim1 - brentm1
    wtim1_m2 = wtim1-wtim2
    brentm1_m2 = brentm1 - brentm2
    efpm2 = total['AAGVX00']
    efs2 = total['AAEBS00']
    mars_wti2 = total['AAKTH00']
    dfl_m1 = total['AAEAA00']
    
    days = 5
    sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
    sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
    discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)
    
    expiry_condition = df.index < df.Expiry
    cfd_condition = cfd1 > cfd2
    
    
    exceptions = {
            'Arab Extra Light':
                {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
            'Arab Light':
                {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
                'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
            'Arab Medium':
                {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
            'Arab Heavy':
                {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
                 'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
            'Basrah Light':
                {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
                 'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
            'Basrah Heavy':
                {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
                 'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
                 'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
                 'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
            'Iranian Heavy':
                {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
                 #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
                'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
            'Iranian Light':
                {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
                 'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
            'Forozan':
                {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
                'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
                'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
            'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
            'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
                'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
                'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
                'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
            }
    
    
    """Need to handle the one month forward OSP concept, so here, take the dataframe for the exceptions above, condense to monhtly values which wont
    change the value as same for each day, shift that forward then re-expand"""
    if assay[crude]['Code'] == 'multiple':     
        diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
        crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
    else:    
        diff = total[assay[crude]['Code']]
        crude_vs = assay[crude]['Index'].lower().strip()
    
    
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            
            
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
                      (rate_data['DischargePort'] == 'Singapore')]
                v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
                df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            return df_freight
        
        def calculate_port_costs():
            """These are for the odd costs, tax rebates, etc"""
            df_freight['Costs'] = 0
            
            # This is the export cost out of Houston
            if sub_region in (['US GULF (padd 3)']):
                df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
                df_freight['Costs'] += df_freight['Houston_Load_Costs']
            
            # Port costs to discharge in Rotterdam               
            if destination == 'Rotterdam':
                df_freight['Rott_Discharge_Costs'] = 0.15
                df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
            
            # Port costs to discharge in Houston
            if destination == 'Houston':
                df_freight['Hous_Discharge_Costs'] = 0.25
                df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
            
            if assay[crude]['LoadPort'] == 'Basrah':
                df_freight['Basrah_Costs'] = 0.76
                df_freight['Costs'] += df_freight['Basrah_Costs']
            
            if assay[crude]['LoadPort'] == 'Ras Tanura':
                df_freight['Saudi_Costs'] = 0.66
                df_freight['Costs'] += df_freight['Saudi_Costs']
            
            return df_freight                
        
        
        
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
                df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
                df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
                df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
                df_freight['Platts_De_Esc'] = total['AAUXL00']
                df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
                df_freight['Costs'] += df_freight['Forties_Margin_Impact']
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
                df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
                #df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
             
             # South Korean particulars
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                # Freight rebate on imported crudes
                df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
                df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
                df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
                df_freight['Costs'] -= df_freight['Freight_Rebate']
                
                # Tax rebate on crudes out of Europe
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
                    df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Tax rebate on crudes out of the US
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
                    df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Costs ascociated with lifting CPC based on delays
                if crude == 'CPC Blend':
                    df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
                    df_freight['TS_Demur'] = total['AAPED00']
                    df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
                    df_freight['Costs'] += df_freight['TS_Demur_Costs']
                
                # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
                if crude in (['Urals Nth', 'Urals Med']):
                    df_freight['Urals_Cif_Rebate'] = 0.11
                    df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
                
                if crude == 'Forties':
                    df_freight['Forties_Mkt_Discount'] = 0.5
                    df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
            else:
                 pass
            
            
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                #i = 'PFAGN10'
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
            
            return df_freight
        
        
        calculate_flat_rate()
        
        
        
        calculate_port_costs()
        
        
        #try:
        freight_and_quality_exceptions()
# =============================================================================
#         except Exception as e: 
#             print('freight_and_quality_exceptions() fails') 
#             print(e.args)
#             print(e.message)
#             print(e)
# =============================================================================
        
        
        calculate_freight()
        
        return df_freight
    
    def convert_prices():
        if crude in crudes_diff_against_osp:
            df_prices['OSP'] = diff
            df_prices['Diff to OSP'] = crude_diffs[crude]
            df_prices['diff'] = diff + crude_diffs[crude]
        else:
            df_prices['diff'] = diff
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                if crude in ('Basrah Light','Basrah Heavy'): 
                    df_prices['cfd3'] = cfd3
                    df_prices['cfd4'] = cfd4
                    df_prices['structure'] = ((cfd3 - cfd4)/7) * 7
                    df_prices['vs_dtd'] = df_prices['diff'] + df_prices['structure']
                else:
                    df_prices['cfd3'] = cfd3
                    df_prices['cfd5'] = cfd5
                    if sub_region == sub_region_2:
                        df_prices['vs_dtd'] = diff
                    elif sub_region == 'WAF':
                        df_prices['structure'] = ((cfd3 - cfd5)/14) * days
                        df_prices['vs_dtd'] = diff + df_prices['structure']
                    else:
                        df_prices['structure'] = ((cfd3 - cfd5)/7) * days
                        df_prices['vs_dtd'] = diff + df_prices['structure']
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
df_freight = construct_freight()
df_freight
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
codes_list = [x for x in crude_diffs.columns if x not in crudes_diff_against_osp]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])




"""We have to convert the prices that are in absolutes into a diff vs a local index, and if there are, set to zero"""
total['AALSM01'].loc[total['AALSM01'] > 30] = total['AALSM01'].loc[total['AALSM01'] > 30] - total['CLc1']
#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
import pandas as pd
import numpy as np
from datetime import datetime as dt
import time
from pandas.tseries.offsets import BDay
t2 = time.process_time()
#data = pd.ExcelFile('C://Users//mike_//Downloads//toydata1004.xlsx')
#raw_rates = pd.ExcelFile('C://Users//mike_//Downloads//FlatRatesComplete.xlsx')

data = pd.ExcelFile('C://Users//mima//Documents//toydata1004.xlsx')
raw_rates = pd.ExcelFile('C://Users//mima//Documents//FlatRatesComplete.xlsx')

trader_assessed = pd.ExcelFile('L://TRADING//ANALYSIS//GLOBAL//Arb Models//Pecking Order 2018.xlsm')


assay = pd.read_excel(data, 'assay', index_col = 'Database_Name').to_dict('index')
ws = pd.read_excel(data, 'ws')
expiry_table = pd.read_excel(data, 'expiry', index_col = 'Month')
ports = pd.read_excel(data, 'ports')
sub_to_ws = pd.read_excel(data, 'sub_to_ws', header = None)
sub_to_ws = sub_to_ws.set_index([0]).to_dict()

"""table containing the basrah base worldscale that they fix their freight against"""
basrah_ws_base = pd.read_excel(data, 'basrah_ws_base', index_col = 'YEAR')

"""Take in the crude prices and codes and convert to a dataframe.
We need to take the first 2 rows of the prices with no headers as this will give us the cude name and the code ascociated
Then transpose from rows to columns and rename the columns. This will be for later when we determine crude prices basis desired comaprison"""
#prices_reference = (pd.read_excel(data, 'paper prices', header = None).iloc[0:2,1:]).transpose().rename(columns={0:'Name', 1: 'Code'})  

"""Merge the WS table with the prices table, slice df so 2016 onwards (Flat rates last date is 2015). 
We don't drop rows now as dropping would be dependent on any nans in any column"""
#total = prices.merge(ws_table, how = 'inner', left_index = True, right_index = True)
#total = total.merge(paper_prices, how = 'inner', left_index = True, right_index = True)
#total = total.iloc[total.index > dt(2015,12,31)]

"""this new total table generates all the prices in one place for us"""

total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]


"""Clean the column hedaers so no white spcaes - use simple list comprehension and set headers equal to cleaned"""
cleaned_column_headers = [i.strip() for i in total.columns.values]
total.columns = cleaned_column_headers

"""The below was get rid of the row in the index that hax NaT against it and then expand to daily and fill backwards"""
crude_diffs = pd.read_excel(trader_assessed, 'Crude Diffs Traders', header = 0)
crude_diffs = crude_diffs.loc[pd.notnull(crude_diffs.index)]
crude_diffs = crude_diffs.drop([name for name in crude_diffs.columns if 'Unnamed' in name], axis=1)


#crude_diffs.index = crude_diffs.index.map(lambda x : x + 1*BDay())
crude_diffs = crude_diffs.reindex(total.index).fillna(method='bfill').fillna(method='ffill')

"""Slice the crude diffs where the dates in the index are the same as the dates in the total dataframe"""
#crude_diffs = crude_diffs[crude_diffs.index.isin(total.index)]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']
codes_list = [x for x in crude_diffs.columns if x not in crudes_diff_against_osp]

"""Apply the values in crude diffs to the correct codes and dates in the total dataframe"""
total.update(crude_diffs[codes_list])




"""We have to convert the prices that are in absolutes into a diff vs a local index, and if there are, set to zero"""
total['AALSM01'].loc[total['AALSM01'] > 30] = total['AALSM01'].loc[total['AALSM01'] > 30] - total['CLc1']
#total.loc[total.index.isin(crude_diffs.index), codes_list] = crude_diffs[codes_list]
#total[codes_list]

#total.update(crude_diffs[codes_list])
""" Need this for the sulphur table"""
forties_sulphur = pd.read_excel(trader_assessed, 'Forties de-esc', header = [22], parse_cols="H:I").set_index('week ending')
forties_sulphur = forties_sulphur.loc[pd.notnull(forties_sulphur.index)]
forties_sulphur = forties_sulphur.reindex(total.index).fillna(method='ffill')

"""Also need to adjust the cfds to take into account the inter month BFOE spread"""   
cfd_list = ['PCAKA00','PCAKC00','PCAKE00','PCAKG00','AAGLU00','AAGLV00','AALCZ00','AALDA00']
temp = total[cfd_list].sub(pd.Series(total['PCAAQ00'] - total['PCAAR00']), axis=0)
temp = temp[temp.index > dt(2017,6,30)]
total.loc[total.index.isin(temp.index), list(temp.columns)] = temp[list(temp.columns)]

"""This turns the 5 years of rate matricies into a table for use to reference - 12/04/2018"""    
rates = []
for x,y in enumerate([name.split()[2] for name in raw_rates.sheet_names]):
    f  = pd.read_excel(raw_rates, sheetname = x, header = None).iloc[1:47,1:]
    lplen = len(f.iloc[:,1])
    dplen = len(f.iloc[1,:])
    for j in range(1, dplen):
        for i in range(1,lplen):
            LoadPort = f.iloc[i,0]
            DischargePort = f.iloc[0,j]
            Year = y
            Rate = f.iloc[i,j]
            rates.append({'LoadPort':LoadPort, 'DischargePort': DischargePort, 'Year':Year,'Rate':Rate})


rate_data = pd.DataFrame(rates)

"""Also initialise the temp df with index of total. Temp df is tol hold the dataseries needed to calculate the freight"""
df = pd.DataFrame(index=total.index)
df['Date'] = df.index

"""This function allows us to apply the expiration date for the wti futures used to determine what structure we apply to the CMA
Have tried timing and slight improvment with the blow of 0.2seconds...."""

t = time.process_time()

for_dates = lambda x: (expiry_table.loc[(expiry_table.index.month == x.month)&(expiry_table.index.year == x.year)]['Expiry']).iat[0]

df['Expiry'] = df['Date'].apply(for_dates)
df.drop(['Date'], inplace=True, axis=1)

print("df['Expiry'] created successfully: Time was {}".format(time.process_time() - t))
print("Temp DataFrame created successfully")
print("import_data() created successfully: Time was {}".format(time.process_time() - t2))
crude = 'Amna'
destination = 'Rotterdam'
df_freight = pd.DataFrame(index=df.index)
df_prices = pd.DataFrame(index=df.index)

index_wti = [x.lower().strip() for x in ['WTI F1','WTI CMA','1ST LINE WTI','2D LINE WTI','L.A WTI','FORWARD WTI','WTI']]
index_dtd = [x.lower().strip() for x in ['DATED BRENT', 'DATED','N.SEA DATED','BTC Dated', 'MED DATED','WAF DATED','CANADA DATED','CANADA BRENT DATED','ANGOLA DATED','    GHANA DATED']]
index_dub = [x.lower().strip() for x in ['DUBAI','DUBAI M2','OMAN/DUBAI']]
crudes_diff_against_osp = ['Basrah Light','Basrah Heavy','Amna','El Sharara','Zueitina','Mellitah']

"""Declare the main prices that will be used in order to use shorthand notation"""


dtd = total['PCAAS00']
dub = total['AAVMR00']
wtim1 = total['CLc1'] 
wtim2 = total['CLc2']
brentm1 = total['LCOc1']
brentm2 = total['LCOc2']
wti_cma_m1 = total['AAVSN00']
cfd1 = total['PCAKG00']
cfd2 = total['AAGLV00']
cfd3 = total['PCAKE00']
cfd4 = total['PCAKG00']
cfd5 = total['AAGLU00']
cfd6 = total['AAGLV00']
cfd7 = total['AALCZ00']
cfd8 = total['AALDA00']
wti_br_m1 = wtim1 - brentm1
wtim1_m2 = wtim1-wtim2
brentm1_m2 = brentm1 - brentm2
efpm2 = total['AAGVX00']
efs2 = total['AAEBS00']
mars_wti2 = total['AAKTH00']
dfl_m1 = total['AAEAA00']

days = 5
sub_region = ports[ports['Name'] == assay[crude]['LoadPort']]['Subregion'].map(sub_to_ws[1]).to_string(index = False) # NB the index = False is make sure we dont take in the index number given in the output
sub_region_2 = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[1]).to_string(index = False)
discharge_price_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[3]).to_string(index = False)

expiry_condition = df.index < df.Expiry
cfd_condition = cfd1 > cfd2


exceptions = {
        'Arab Extra Light':
            {'ROTTERDAM':{'Code':'AAIQQ00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQK00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIQZ00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQV00','Index':'OMAN/DUBAI'}},
        'Arab Light':
            {'ROTTERDAM':{'Code':'AAIQR00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAWQL00','Index':'BWAVE'},
            'HOUSTON':{'Code':'AAIRA00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQW00','Index':'OMAN/DUBAI'}},
        'Arab Medium':
            {'ROTTERDAM':{'Code':'AAIQS00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQM00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRB00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQX00','Index':'OMAN/DUBAI'}},
        'Arab Heavy':
            {'ROTTERDAM':{'Code':'AAIQT00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAWQN00','Index':'BWAVE'},
             'HOUSTON':{'Code':'AAIRC00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIQY00','Index':'OMAN/DUBAI'}},
        'Basrah Light':
            {'ROTTERDAM':{'Code':'AAIPH00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAIPH00','Index':'Dated'},
             'HOUSTON':{'Code':'AAIPG00','Index':'WTI'},
             'SINGAPORE':{'Code':'AAIPE00','Index':'OMAN/DUBAI'}},
        'Basrah Heavy':
            {'ROTTERDAM':{'Code':'AAXUC00','Index':'Dated'},
             'AUGUSTA':{'Code':'AAXUC00','Index':'Dated'},
             'HOUSTON':{'Code':'AAXUE00','Index':'Mars'},
             'SINGAPORE':{'Code':'AAXUA00','Index':'OMAN/DUBAI'}},
        'Iranian Heavy':
            {'ROTTERDAM':{'Code':'AAIPB00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCH00','Index':'BWAVE'},
             #'Iranian Heavy':{'HOUSTON':{'Code':abcde,'Index':'WTI'}},
            'SINGAPORE':{'Code':'AAIOY00','Index':'OMAN/DUBAI'}},
        'Iranian Light':
            {'ROTTERDAM':{'Code':'AAIPA00','Index':'BWAVE'},
             'AUGUSTA':{'Code':'AAUCJ00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOX00','Index':'OMAN/DUBAI'}},
        'Forozan':
            {'ROTTERDAM':{'Code':'AAIPC00','Index':'BWAVE'},
            'AUGUSTA':{'Code':'AAUCF00','Index':'BWAVE'},
            'SINGAPORE':{'Code':'AAIOZ00','Index':'OMAN/DUBAI'}},
        'Isthmus':{'ROTTERDAM':{'Code':'AAIQC00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQC00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPZ00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQE00','Index':'OMAN/DUBAI'}},
        'Maya':{'ROTTERDAM':{'Code':'AAIQB00','Index':'Dated'},
            'AUGUSTA':{'Code':'AAIQB00','Index':'Dated'},
            'HOUSTON':{'Code':'AAIPY00','Index':'WTI'},
            'SINGAPORE':{'Code':'AAIQD00','Index':'OMAN/DUBAI'}}
        }


"""Need to handle the one month forward OSP concept, so here, take the dataframe for the exceptions above, condense to monhtly values which wont
change the value as same for each day, shift that forward then re-expand"""
if assay[crude]['Code'] == 'multiple':     
    diff = total[exceptions[crude][discharge_price_region]['Code']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
    crude_vs = exceptions[crude][discharge_price_region]['Index'].lower().strip()
else:    
    diff = total[assay[crude]['Code']]
    crude_vs = assay[crude]['Index'].lower().strip()

def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

"""These are for the odd costs, tax rebates, etc"""
df_freight['Costs'] = 0

# This is the export cost out of Houston
if sub_region in (['US GULF (padd 3)']):
    df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
    df_freight['Costs'] += df_freight['Houston_Load_Costs']


# Port costs to discharge in Rotterdam               
if destination == 'Rotterdam':
    df_freight['Rott_Discharge_Costs'] = 0.15
    df_freight['Costs'] += df_freight['Rott_Discharge_Costs']


# Port costs to discharge in Houston
if destination == 'Houston':
    df_freight['Hous_Discharge_Costs'] = 0.25
    df_freight['Costs'] += df_freight['Hous_Discharge_Costs']


if assay[crude]['LoadPort'] == 'Basrah':
    df_freight['Basrah_Costs'] = 0.76
    df_freight['Costs'] += df_freight['Basrah_Costs']


if assay[crude]['LoadPort'] == 'Ras Tanura':
    df_freight['Saudi_Costs'] = 0.66
    df_freight['Costs'] += df_freight['Saudi_Costs']


return df_freight       
def calculate_flat_rate():
    """create the flat rates table for the rates calculations and column creation"""    
    flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
              (rate_data['DischargePort'] == destination)]
    
    def calculate_flat_rates(x):
        return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
    
    """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
    v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
    df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    
    
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
              (rate_data['DischargePort'] == 'Singapore')]
        v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
        df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
    
    return df_freight

def calculate_port_costs():
    """These are for the odd costs, tax rebates, etc"""
    df_freight['Costs'] = 0
    
    # This is the export cost out of Houston
    if sub_region in (['US GULF (padd 3)']):
        df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
        df_freight['Costs'] += df_freight['Houston_Load_Costs']
    
    # Port costs to discharge in Rotterdam               
    if destination == 'Rotterdam':
        df_freight['Rott_Discharge_Costs'] = 0.15
        df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
    
    # Port costs to discharge in Houston
    if destination == 'Houston':
        df_freight['Hous_Discharge_Costs'] = 0.25
        df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
    
    if assay[crude]['LoadPort'] == 'Basrah':
        df_freight['Basrah_Costs'] = 0.76
        df_freight['Costs'] += df_freight['Basrah_Costs']
    
    if assay[crude]['LoadPort'] == 'Ras Tanura':
        df_freight['Saudi_Costs'] = 0.66
        df_freight['Costs'] += df_freight['Saudi_Costs']
    
    return df_freight    

def freight_and_quality_exceptions():
    if crude in ('Forties'):
        df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
        df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
        df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
        df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
        df_freight['Platts_De_Esc'] = total['AAUXL00']
        df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
        df_freight['Costs'] += df_freight['Forties_Margin_Impact']
    
    if crude in ('Basrah Light','Basrah Heavy'):
        """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
        monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
        func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
        
        """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
        func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
        func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
        func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
        func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
        df_freight['Date'] = df_freight.index
        df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
        df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
        # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
        # only valid for 2018
        df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
        df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
        
        if crude == 'Basrah Light':
            df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
        else:
            df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
        
        
        df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
        df_freight.drop(['Date'], axis = 1, inplace=True)
        #df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
     
     # South Korean particulars
    if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
        # Freight rebate on imported crudes
        df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
        df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
        df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
        df_freight['Costs'] -= df_freight['Freight_Rebate']
        
        # Tax rebate on crudes out of Europe
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
            df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Tax rebate on crudes out of the US
        if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
            df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
            df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
        
        # Costs ascociated with lifting CPC based on delays
        if crude == 'CPC Blend':
            df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
            df_freight['TS_Demur'] = total['AAPED00']
            df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
            df_freight['Costs'] += df_freight['TS_Demur_Costs']
        
        # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
        if crude in (['Urals Nth', 'Urals Med']):
            df_freight['Urals_Cif_Rebate'] = 0.11
            df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
        
        if crude == 'Forties':
            df_freight['Forties_Mkt_Discount'] = 0.5
            df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
    else:
         pass
    
    
    
    return df_freight

def calculate_freight():
    
    """This finds the correct worldscale rate and adjusts if it is lumpsum"""
    ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
    
    vessel_size = []
    for i in list(ws_codes['Code']):
        #i = 'PFAGN10'
        size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
        vessel_size.append(size)
        name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
        if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
            df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
            df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
            df_freight.drop(['Rate'], axis=1)
        else:
            df_freight[name] = total[i]
            df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
    
    if 'WS for Esc' in df_freight.columns.values:
        for i in vessel_size:
            df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
    
    return df_freight

calculate_flat_rate()
calculate_port_costs()
freight_and_quality_exceptions()
calculate_freight()
    def convert_prices():
        if crude in crudes_diff_against_osp:
            df_prices['OSP'] = diff
            df_prices['Diff to OSP'] = crude_diffs[crude]
            df_prices['diff'] = diff + crude_diffs[crude]
        else:
            df_prices['diff'] = diff
        """depending on discharge, choose the appropriate index"""
        def convert_wti():
            df_prices['outright'] = wtim1
            if crude_vs in ['wti cma']:
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['vs_wti'] = diff + wti_cma_m1 - wtim1
            
            elif crude_vs in ['mars']:
                df_prices['mars_wti2'] = mars_wti2
                df_prices['vs_wti'] = diff + mars_wti2
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['vs_wti'] = np.where(expiry_condition,
                         diff + wtim1_m2,
                         diff)
            
            elif crude_vs in index_dtd:
                cfd_condition = cfd4 > cfd8
                df_prices['cfd4'] = cfd4
                df_prices['cfd8'] = cfd8
                df_prices['efpm2'] = efpm2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['vs_wti']  = np.where(cfd_condition,
                         diff + cfd8 +  efpm2 - (wti_br_m1),
                         diff + cfd4 +  efpm2 - (wti_br_m1))
            
            elif crude_vs in index_dub:
                """ This is because all the eastern crudes heading here have diffs against BWAVE"""
                pass
            else:
                df_prices['vs_wti'] = diff
            return df_prices
        
        def convert_dtd():
            df_prices['outright'] = dtd
# =============================================================================
#             conditions = [(expiry_condition & cfd_condition),
#                           (expiry_condition & np.invert(cfd_condition)),
#                           (np.invert(expiry_condition) & cfd_condition),
#                           (np.invert(expiry_condition) & np.invert(cfd_condition))]
#             choices = [(diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff +  wtim1_m2 - (brentm2 - wtim2) - efpm2 - cfd1),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd2),
#                        (diff - (brentm2 - wtim2) - efpm2 - cfd1)]
# =============================================================================
            
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efpm2'] = efpm2
                df_prices['cfd8'] = cfd8
                df_prices['vs_dtd'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 - efpm2 - cfd8 
            
            elif crude_vs in ['bwave']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['dfl_m1'] = dfl_m1
                df_prices['vs_dtd'] = diff - dfl_m1 
            
            elif crude_vs in index_wti:
                df_prices['cfd8'] = cfd8
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['efpm2'] = efpm2
                df_prices['vs_dtd'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 - efpm2 - cfd8,
                         diff + wti_br_m1 - efpm2 - cfd8)
            
            elif crude_vs in index_dtd:
                if crude in ('Basrah Light','Basrah Heavy'): 
                    df_prices['cfd3'] = cfd3
                    df_prices['cfd4'] = cfd4
                    df_prices['structure'] = ((cfd3 - cfd4)/7) * 7
                    df_prices['vs_dtd'] = df_prices['diff'] + df_prices['structure']
                else:
                    df_prices['cfd3'] = cfd3
                    df_prices['cfd5'] = cfd5
                    if sub_region == sub_region_2:
                        df_prices['vs_dtd'] = diff
                    elif sub_region == 'WAF':
                        df_prices['structure'] = ((cfd3 - cfd5)/14) * days
                        df_prices['vs_dtd'] = diff + df_prices['structure']
                    else:
                        df_prices['structure'] = ((cfd3 - cfd5)/7) * days
                        df_prices['vs_dtd'] = diff + df_prices['structure']
            
            #elif crude_vs in index_dub:
                #""" This is because all the eastern crudes heading here have diffs against BWAVE"""
                #pass
                #df_prices['diff'] = diff
                #if assay[crude]['LoadPort'] in ['Basrah']:
                    #df_prices['osp_vs_dtd']
                    #df_prices['freight esc/desc']
                    #df_prices['api esc/desc']
                   # df_prices['osp_vs_dtd']
            
            else:
                df_prices['vs_dtd'] = diff
            return df_prices
        
        def convert_dub():
            df_prices['outright'] = dub
            if crude_vs in ['wti cma']:
                """Here use cfd 8 on reccomendation of Andrea as by the time it loads only cfd wk 8 applicable"""
                df_prices['wti_cma_m1'] = wti_cma_m1
                df_prices['wtim1'] = wtim1
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + wti_cma_m1 - wtim1 + wtim1_m2 + wti_br_m1 + efs2 
            
            elif crude_vs in index_wti:
                df_prices['wtim1_m2'] = wtim1_m2
                df_prices['wti_br_m1'] = wti_br_m1
                df_prices['brentm2'] = brentm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = np.where(expiry_condition,
                         diff + wtim1_m2 + wti_br_m1 + efs2,
                         diff + wti_br_m1 + efs2)
            
            elif crude_vs in index_dtd:
                df_prices['cfd6'] = cfd6
                df_prices['efpm2'] = efpm2
                df_prices['efs2'] = efs2
                df_prices['vs_dub'] = diff + cfd6 +  efpm2 + efs2
            
            else:
                df_prices['vs_dub'] = diff
            
            return df_prices
        
        index_region = ports[ports['Name'] == destination]['Subregion'].map(sub_to_ws[2]).to_string(index = False)
        func_list = {'wti':convert_wti, 'dtd':convert_dtd, 'dub':convert_dub}
        [f() for index, f in func_list.items() if index == index_region][0]
        return df_prices
df_freight = construct_freight()
    def construct_freight():
        def calculate_flat_rate():
            """create the flat rates table for the rates calculations and column creation"""    
            flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == assay[crude]['LoadPort'])&
                      (rate_data['DischargePort'] == destination)]
            
            def calculate_flat_rates(x):
                return float(flat_rate_table.loc[flat_rate_table['Year'].astype(int) == x, 'Rate'])
            
            """Vectorising the function amkes it applicable over an array - before had to use pandas which was element wise application - i.e. SLOW"""
            v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
            df_freight['Rate'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            
            
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                flat_rate_table = rate_data.loc[(rate_data['LoadPort'] == 'Ruwais')&
                      (rate_data['DischargePort'] == 'Singapore')]
                v_calculate_flat_rates = np.vectorize(calculate_flat_rates)
                df_freight['Murban_Sing_Flat'] = np.apply_along_axis(v_calculate_flat_rates,0,np.array(df.index.year))
            
            return df_freight
        
        def calculate_port_costs():
            """These are for the odd costs, tax rebates, etc"""
            df_freight['Costs'] = 0
            
            # This is the export cost out of Houston
            if sub_region in (['US GULF (padd 3)']):
                df_freight['Houston_Load_Costs'] = np.where(df_freight.index > dt(2018,2,28),0.09,0)
                df_freight['Costs'] += df_freight['Houston_Load_Costs']
            
            # Port costs to discharge in Rotterdam               
            if destination == 'Rotterdam':
                df_freight['Rott_Discharge_Costs'] = 0.15
                df_freight['Costs'] += df_freight['Rott_Discharge_Costs']
            
            # Port costs to discharge in Houston
            if destination == 'Houston':
                df_freight['Hous_Discharge_Costs'] = 0.25
                df_freight['Costs'] += df_freight['Hous_Discharge_Costs']
            
            if assay[crude]['LoadPort'] == 'Basrah':
                df_freight['Basrah_Costs'] = 0.76
                df_freight['Costs'] += df_freight['Basrah_Costs']
            
            if assay[crude]['LoadPort'] == 'Ras Tanura':
                df_freight['Saudi_Costs'] = 0.66
                df_freight['Costs'] += df_freight['Saudi_Costs']
            
            return df_freight                
        
        
        
        
        def freight_and_quality_exceptions():
            if crude in ('Forties'):
                df_freight['Buzzard_Content'] = forties_sulphur['buzzard content']
                df_freight['Implied_Sulphur'] = df_freight['Buzzard_Content'] * 0.012 + 0.003
                df_freight['De-Escalator_Threshold'] = np.round(df_freight['Implied_Sulphur'], 3)
                df_freight['De-Escalator_Counts'] = np.minimum(0, 6-df_freight['Implied_Sulphur']*1000)
                df_freight['Platts_De_Esc'] = total['AAUXL00']
                df_freight['Forties_Margin_Impact'] = df_freight['Platts_De_Esc'] * df_freight['De-Escalator_Counts'] * -1
                df_freight['Costs'] += df_freight['Forties_Margin_Impact']
            
            if crude in ('Basrah Light','Basrah Heavy'):
                """This handles the freight escalation calculation from Iraq - the base is sent by SOMO, and table is in databse / excel wb"""
                monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
                func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
                
                """Create funcs to handle basrah base and flat rate values, apply over df and calc esclator"""
                func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
                func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
                func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
                func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
                df_freight['Date'] = df_freight.index
                df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
                df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
                # We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
                # only valid for 2018
                df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
                df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
                
                if crude == 'Basrah Light':
                    df_freight['API Esc'] = df_freight['Date'].apply(func_blapi)
                else:
                    df_freight['API Esc'] = df_freight['Date'].apply(func_bhapi)               
                
                
                df_freight['WS for Esc'] = (df_freight['WS Month Avg'] - df_freight['SOMO Base WS']) * df_freight['Base_FR_for_esc'] / 7.3 / 100
                df_freight.drop(['Date'], axis = 1, inplace=True)
                #df_freight[['WS for Esc','API Esc']] = df_freight[['WS for Esc','API Esc']].resample('MS').mean().shift(-1, freq='MS').reindex(total.index).fillna(method='ffill')
             
             # South Korean particulars
            if ports[ports['Name'] == destination]['Country'].iat[0] == 'South Korea':
                # Freight rebate on imported crudes
                df_freight['Murban_Freight_Comp'] = total['PFAOC00'] / 100 * df_freight['Murban_Sing_Flat'] / 7.66 #Murban density conversion
                df_freight['UKC-Yosu_VLCC'] = total['AASLA00'] * 1000000 / 2000000
                df_freight['Freight_Rebate'] = np.maximum(df_freight['UKC-Yosu_VLCC'] - df_freight['Murban_Freight_Comp'], 0.6)
                df_freight['Costs'] -= df_freight['Freight_Rebate']
                
                # Tax rebate on crudes out of Europe
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['NW EUROPE','MED']):
                    df_freight['FTA_Tax_Rebate'] = 0.006 * total['LCOc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Tax rebate on crudes out of the US
                if ports[ports['Name'] == assay[crude]['LoadPort']]['Region'].iat[0] in (['N AMERICA']):
                    df_freight['FTA_Tax_Rebate'] = 0.005 * total['CLc1']
                    df_freight['Costs'] -= df_freight['FTA_Tax_Rebate']
                
                # Costs ascociated with lifting CPC based on delays
                if crude == 'CPC Blend':
                    df_freight['TS_Delays'] = np.maximum(total['AAWIL00'] + total['AAWIK00'] - 2,0)
                    df_freight['TS_Demur'] = total['AAPED00']
                    df_freight['TS_Demur_Costs'] = df_freight['TS_Delays'].mul(df_freight['TS_Demur'])/130
                    df_freight['Costs'] += df_freight['TS_Demur_Costs']
                
                # Costs ascociated with lifting Urals, actually a rebate as giving back port costs that are included in CIF price
                if crude in (['Urals Nth', 'Urals Med']):
                    df_freight['Urals_Cif_Rebate'] = 0.11
                    df_freight['Costs'] -= df_freight['Urals_Discharge_Costs']
                
                if crude == 'Forties':
                    df_freight['Forties_Mkt_Discount'] = 0.5
                    df_freight['Costs'] -= df_freight['Forties_Mkt_Discount']
            else:
                 pass
            
            
            
            return df_freight
        
        def calculate_freight():
            
            """This finds the correct worldscale rate and adjusts if it is lumpsum"""
            ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
            
            vessel_size = []
            for i in list(ws_codes['Code']):
                #i = 'PFAGN10'
                size = ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
                vessel_size.append(size)
                name = ws_codes[ws_codes['Code'] == i]['Name'].iat[0]
                if ws_codes[ws_codes['Code'] == i]['Terms'].values == 'lumpsum':
                    df_freight[name] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000
                    df_freight[size] = total[ws_codes[ws_codes['Code'] == i]['Code'].values] * 1000000 / (ws_codes[ws_codes['Code'] == i]['bbls'].values * 1000) + df_freight['Costs']
                    df_freight.drop(['Rate'], axis=1)
                else:
                    df_freight[name] = total[i]
                    df_freight[size] = total[i] / 100 * df_freight['Rate'] / assay[crude]['Conversion'] + df_freight['Costs']
            
            if 'WS for Esc' in df_freight.columns.values:
                for i in vessel_size:
                    df_freight[i] = df_freight[i] - df_freight['WS for Esc'] + df_freight['API Esc']
            
            return df_freight
        
        
        calculate_flat_rate()
        
        
        
        calculate_port_costs()
        
        
        #try:
        freight_and_quality_exceptions()
# =============================================================================
#         except Exception as e: 
#             print('freight_and_quality_exceptions() fails') 
#             print(e.args)
#             print(e.message)
#             print(e)
# =============================================================================
        
        
        calculate_freight()
        
        return df_freight
df_freight = construct_freight()
df_freight
df_prices = convert_prices()
df_prices
temp = pd.concat([df_prices,df_freight], axis=1)
price_index = [price_index for price_index in df_prices.columns if 'vs_' in price_index][0]
freight_list = [freight for freight in df_freight.columns if 'max' in freight or 'VLCC' in freight]
try:
    
    for k in freight_list:
        try:
            name = str(k[:4]) + str('_landed_') + str(price_index)
        except Exception as e: print('name fails') 
        if destination == assay[crude]['LoadPort']:
            temp[name] = df_prices[price_index]
        else:
            try:
                temp[name] = df_prices[price_index].add(df_freight[k])
            except Exception as e: print('temp fails')    
except Exception as e: print('check')

temp
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes = ['Amna']
#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e)
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
crudes = ['Forties']
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes
ws_codes[ws_codes['Code'] == i]['Size'].iat[0]
ws_codes = ws[(ws['Origin'] == sub_region)&(ws['Destination'] == sub_region_2)]
ws_codes
list(ws_codes['Code'])
ws_codes[ws_codes['Code'] == i]['Size']
ws_codes['Code']
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
monthly_averages
func_ma_on_days = lambda x: (monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)]).iat[0]
func_ma_on_days
func_ws_base = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['SOMO']).iat[0]
func_fr = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['FR']).iat[0]
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_bhapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BHAPI']).iat[0]                
func_blapi = lambda x: (basrah_ws_base.loc[(basrah_ws_base.index.year == x.year)]['BLAPI']).iat[0]                
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['SOMO Base WS'] = df_freight['Date'].apply(func_ws_base)
# We have to apply the corrcetion here after SOMO dropped their base rate earlier this year - assumption
# only valid for 2018
df_freight['SOMO Base WS'].iloc[(df_freight.index >= dt(2018,4,1))&(df_freight.index <= dt(2018,12,31))] = 25               
df_freight['Base_FR_for_esc'] = df_freight['Date'].apply(func_fr)
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight
df_freight['Date'] = df_freight.index
df_freight['Date']
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['Date']
func_ma_on_days
monthly_averages
monthly_averages.index.month
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight['Date'] = df_freight.index
df_freight['WS Month Avg'] = df_freight['Date'].apply(func_ma_on_days)
df_freight
df_freight.index
(monthly_averages.loc[(monthly_averages.index.month == x.month)&(monthly_averages.index.year == x.year)])
total['PFAOH00']
total['PFAOH00'].asfreq(BDay()).resample('BMS').mean()
monthly_averages = total['PFAOH00'].asfreq(BDay()).resample('BMS').mean() # resampled so we have the business month start, corrects averaging error if cma
monthly_averages
total['PFAOH00'].asfreq(BDay())
total['PFAOH00']
total['PFAOH00'].resample('BMS').mean(
total['PFAOH00'].resample('BMS').mean()
total['PFAOH00'].resample('BMS').mean().asfreq(BDay())
total['PFAOH00']
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])
total
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total
total.index
total.index[-1]
total.index[-1] -total.index[-2]
total.index[-1] - total.index[-2] > 25
mike = total.index[-1] - total.index[-2]
mike
dt.timedelta(days=10)
timedelta(days=10)
pd.Timedelta(days=10) 
total.index[-1] - total.index[-2] > pd.Timedelta(days=10) 
if total.index[-1] - total.index[-2] > pd.Timedelta(days=10):
    total.index[-1] = total.index[-2] + pd.Timedelta(days=1)

total.index.to_series()
total.index[-1].values = total.index[-2] + pd.Timedelta(days=1)
total.index
total.index[-2] + pd.Timedelta(days=1)
total.index[-1].values
total.index.values 
total.index.values[-1]
total.index.values[-1] = total.index[-2] + pd.Timedelta(days=1)
total.index
total = pd.read_excel(data, 'price_warehouse', header = 4).drop(['Timestamp'])

total.index = pd.to_datetime(total.index)
total.sort_index(inplace=True)
total.fillna(method='ffill', inplace=True)
total = total[total.index > dt(2015,1,1)]

"""This will help with the date error. Turn the index into a numpy array and then assign the value"""
if total.index[-1] - total.index[-2] > pd.Timedelta(days=10):
    total.index.values[-1] = total.index[-2] + pd.Timedelta(days=1)

total
runfile('C:/Users/mima/.spyder-py3/ArbEcons2504.py', wdir='C:/Users/mima/.spyder-py3')
global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
crudes = ['Forties']
#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e)
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']
global_arbs
import pandas as pd
from ArbEcons2504 import import_data
from ArbEcons2504 import arb
from GPW2504 import gpw_calculation
import seaborn as sns
from datetime import datetime as dt

var = import_data()

""" This gives us the assyta from the 1st positional argument in var. Then transpose to get crudes as the index and bring back code and loadport"""
crude_check = pd.DataFrame(var[0]).transpose()[['Code','LoadPort']].dropna(axis=0)

"""Check that these ports all have corresponding flat rates otherwise they will fail at this stage.
Also check the crudes are also there. Finally need to check that the load port is in the overall list as known issues with novo etc"""
ports_with_rates = var[4]['LoadPort'].unique()
crudes_with_codes = var[3].columns.values
ports_in_targo = var[2]['Name'].unique()

"""Slice our data so only crudes with loadports that have flat rates are returned - this prevents an error"""
test_crudes = crude_check.loc[crude_check['LoadPort'].isin(ports_with_rates)]
no_flat_rates = crude_check.loc[~crude_check['LoadPort'].isin(ports_with_rates)]

"""Check to see if the loadports are in the targo extract"""
test_crudes = test_crudes.loc[test_crudes['LoadPort'].isin(ports_in_targo)]
not_in_targo = crude_check.loc[~crude_check['LoadPort'].isin(ports_in_targo)]

"""Check to see if the neccessary codes are in the total dataframe"""

test_crudes = test_crudes.loc[(test_crudes['Code'].isin(crudes_with_codes)) | (test_crudes['Code'] == 'multiple')].drop('Null')
#codes_missing = crude_check.loc[~crude_check['Code'].isin(crudes_with_codes)]


crudes = list(test_crudes.index.values)
#crudes = ['Forties']
#crudes = ['Basrah Light','Basrah Heavy']
#crudes = ['Basrah Light','Urals Nth','CPC Blend','Arab Medium','Maya','Mars']
#destinations = ['Houston']
destinations = ['Rotterdam','Augusta','Houston']
substitution_pct = 0.2

refinery_configurations = {'simple':{'refinery_volume':200,
                             'reformer_capacity':42,
                             'fcc_capacity':48,
                             'coker_capacity':0,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15},
                    'complex':{'refinery_volume':350,
                             'reformer_capacity':100,
                             'fcc_capacity':80,
                             'coker_capacity':48,
                             'lvn_gasolinepool':0.12,
                             'kero_gasolinepool':0.15}} 

def make_arbs(crudes,destinations, *var, **refinery_configurations): 
    counter = 0
    try:
        base_blends = {'Rotterdam':{'Urals Nth':0.5,'Ekofisk':0.2,'Forties':0.1,'Gullfaks':0.1,'Statfjord':0.1},
            'Augusta':{'Qua Iboe':0.1,'Azeri':0.3,'Saharan':0.2,'Urals Med':0.4},
            #'Houston':{'Mars':0.15,'Urals Med':0.35,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
            'Houston':{'Maya':0.15,'Basrah Light':0.175,'Arab Medium':0.175,'Eagleford 45':0.125,'Bakken':0.125,'WTI Midland':0.25}}
        
        """generate the landed base blends for each region"""
        base_costs = pd.DataFrame()
        for j in base_blends.keys():
            base_landed = pd.DataFrame()
            for i in base_blends[j].keys():
                #i = 'Basrah Light'
                #j = 'Houston'
                arb_frame = arb(i,j,*var)
                #arb_frame.head()
                freight_list = [column_header for column_header in arb_frame.columns if 'landed' in column_header]
                base_landed[i] = arb_frame[freight_list].mean(axis=1) * base_blends[j][i] * 1.01
            base_costs[j] = base_landed.sum(1)           
    except Exception as e: print(e)
    
    for m in refinery_configurations.keys():
        refinery_config = refinery_configurations[m]
        for i in crudes:
            for k in destinations:
                try:
                    if counter == 0:
                        #m = 'complex'
                        #i = 'Basrah Light'
                        #k = 'Rotterdam'
                        arb_values = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        arb_values[base] = base_costs[k]
                        gpw = gpw_calculation(i,k, *var, **refinery_config)
                        arb_values = pd.concat([arb_values,gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in arb_values.columns if '_landed' in index]:
                            name = j[:4]+"_margin"
                            margin_headers.append(name)
                            if 'base' in j:
                                arb_values[name] =  arb_values['Base_GPW'] - arb_values[j] - arb_values['outright']
                            else:
                                arb_values[name] =  arb_values['GPW'] - arb_values[j] - arb_values['outright']
                        for x in range(len(margin_headers)-1):
                            name = margin_headers[x]+'_advantage'
                            arb_values[name] = (arb_values[margin_headers[x]] - arb_values[margin_headers[-1]]) * substitution_pct
                        arb_values.columns = pd.MultiIndex.from_product([[m],[i],[k], arb_values.columns])
                        counter +=1
                    else:
                        next_arb = arb(i,k, *var)
                        base = k[:4]+'_base_landed'
                        next_arb[base] = base_costs[k]
                        next_gpw = gpw_calculation(i,k, *var, **refinery_config)
                        next_arb = pd.concat([next_arb,next_gpw], axis=1)
                        margin_headers = []
                        for j in [index for index in next_arb.columns if '_landed' in index]:
                           name = j[:4]+"_margin"
                           margin_headers.append(name)
                           if 'base' in j:
                               next_arb[name] =  next_arb['Base_GPW'] - next_arb[j] - next_arb['outright']
                           else:
                               next_arb[name] =  next_arb['GPW'] - next_arb[j] - next_arb['outright']
                        for x in range(len(margin_headers)-1):
                           name = margin_headers[x]+'_advantage'
                           next_arb[name] = (next_arb[margin_headers[x]] - next_arb[margin_headers[-1]]) * substitution_pct
                        next_arb.columns = pd.MultiIndex.from_product([[m],[i],[k], next_arb.columns])
                        arb_values = pd.concat([arb_values,next_arb], axis=1)
                
                except Exception as e:
                    print(e)
                    print("{} failed into {}".format(i,k))         
    
    return arb_values


global_arbs = make_arbs(crudes, destinations, *var, **refinery_configurations)

"""Assign the levels names"""
global_arbs.columns.names, global_arbs.index.names = ['RefineryConfig','Grade','Region','Series'], ['Date']

basrah_suez = global_arbs.iloc[global_arbs.index > dt(2017,9,1),(global_arbs.columns.get_level_values('RefineryConfig')=='simple')&
                 (global_arbs.columns.get_level_values('Grade').isin(['Basrah Light','Basrah Heavy']))&
                 (global_arbs.columns.get_level_values('Region').isin(['Rotterdam','Augusta']))]


writer = pd.ExcelWriter('L://TRADING//ANALYSIS//Python//GlobalArbs4.xlsx')
global_arbs.to_excel(writer, sheet_name='global_arbs')
writer.save()